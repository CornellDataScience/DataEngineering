{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import findspark  # this needs to be the first import                                                                        \n",
    "#findspark.init()\n",
    "\n",
    "# jupyter notebook &\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import scipy as sc\n",
    "\n",
    "from IPython.display import display\n",
    "# pip install graphlab-create\n",
    "#import graphlab as gl\n",
    "import json\n",
    "\n",
    "#gl.canvas.set_target('ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://128.84.48.178:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "try:\n",
    "    sc = SparkContext()     \n",
    "except:\n",
    "    pass\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://128.84.48.178:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del spark\n",
    "# spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .master(\"yarn\") \\\n",
    "#         .appName(\"testing\") \\\n",
    "#         .config(\"spark.executor.instances\", \"4\") \\\n",
    "#         .config(\"spark.executor.memory\",\"2g\") \\\n",
    "#         .config(\"spark.driver.memory\",\"4g\") \\\n",
    "#         .config(\"spark.executor.cores\",'1') \\\n",
    "#         .config(\"spark.scheduler.mode\",\"FIFO\") \\\n",
    "#        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.144.40.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1052c51d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actually, why should I partition?\n",
    "#dat = spark.read.json('checkin.json')\n",
    "dat = spark.read.json('hdfs://master:9000/user/serverteam_1/FinalFrontier/checkin.json').repartition(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 23.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120782"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# count num of unique rows in a column\n",
    "dat.select(col(\"time\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 8.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120782"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# count num of unique rows in a column\n",
    "dat.select(col(\"time\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus = spark.read.json('hdfs://master:9000/user/serverteam_1/FinalFrontier/business.json').repartition(150)\n",
    "# bus.printSchema()\n",
    "\n",
    "# phot = spark.read.json('hdfs://localhost:9000/yelp/photos.json').repartition(150)\n",
    "# phot.printScheme()\n",
    "\n",
    "review= spark.read.json('hdfs://master:9000/user/serverteam_1/FinalFrontier/review.json').repartition(150)\n",
    "# phot.printScheme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_yyy = review.join(bus, review.business_id == bus.business_id, 'inner').drop(bus.business_id).drop(bus.stars)\n",
    "# join based on user and business keys\n",
    "# other objects get garbage collected\n",
    "\n",
    "# _yyy.select(\"stars\")\n",
    "review_business = _yyy.withColumnRenamed(\"stars\", \"business_avg_stars\").withColumnRenamed(\"attributes\", \"business_type\").withColumnRenamed(\\\n",
    "                                         \"review_count\", \"business_review_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "getsizeof(review_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business_id',\n",
       " 'cool',\n",
       " 'date',\n",
       " 'funny',\n",
       " 'review_id',\n",
       " 'business_avg_stars',\n",
       " 'text',\n",
       " 'useful',\n",
       " 'user_id',\n",
       " 'address',\n",
       " 'business_type',\n",
       " 'categories',\n",
       " 'city',\n",
       " 'hours',\n",
       " 'is_open',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'name',\n",
       " 'neighborhood',\n",
       " 'postal_code',\n",
       " 'business_review_count',\n",
       " 'state']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_business.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='qj4KHXGgwyutHpvWRLI87g', business_review_count=5, count=1),\n",
       " Row(business_id='iK6JSngIDcToad8SBl04Fg', business_review_count=35, count=1),\n",
       " Row(business_id='UXJKNmBuo2_0egYuN8KxJg', business_review_count=3, count=2),\n",
       " Row(business_id='5a3ZWk3F9JgSPp97McF76Q', business_review_count=3, count=2),\n",
       " Row(business_id='RtlEHMt-96EROQ3du7B8dA', business_review_count=3, count=2),\n",
       " Row(business_id='IW1j1wqAdiBcj6fWGkyQCA', business_review_count=3, count=2),\n",
       " Row(business_id='akGAR7Kasfr0AuBBOWYV3g', business_review_count=3, count=2),\n",
       " Row(business_id='xqWx-D0sT5cevkULWJy6xA', business_review_count=3, count=2),\n",
       " Row(business_id='POcKs2JOkWdq0mm_vtIehw', business_review_count=3, count=2),\n",
       " Row(business_id='8gbJ_oKfKzR_GOOFIHIHFQ', business_review_count=3, count=2),\n",
       " Row(business_id='pmyHFrPvE2sPsbPFd4ckyQ', business_review_count=3, count=2),\n",
       " Row(business_id='BiQGSKfQJgmjx-HSgeO2Mg', business_review_count=3, count=2),\n",
       " Row(business_id='bmM39KLhkHfrm1FHTCCZkg', business_review_count=3, count=2),\n",
       " Row(business_id='Qb3699Z3jChkSmNhY9oqxg', business_review_count=3, count=2),\n",
       " Row(business_id='YHX61tYzUKlz9H4gdROlDA', business_review_count=3, count=2),\n",
       " Row(business_id='Gdbx1UW2Qr53yOanW0O5og', business_review_count=3, count=2),\n",
       " Row(business_id='A3H749M1AEggNylDQ5Fr-A', business_review_count=3, count=2),\n",
       " Row(business_id='FRqzMYKxASVDhn0W2limWg', business_review_count=3, count=2),\n",
       " Row(business_id='mhjSXQR9LvL10SSHH6aHIg', business_review_count=3, count=2),\n",
       " Row(business_id='h5csTpDRI7_oo1RUOeHOtQ', business_review_count=3, count=2),\n",
       " Row(business_id='mb6e5-4s6YIaQAlbPHuWyQ', business_review_count=3, count=2),\n",
       " Row(business_id='-RYgx5kQ91vsQQ7RNJ1DQQ', business_review_count=3, count=2),\n",
       " Row(business_id='sih1hTxuGP0vtTD3tciKKg', business_review_count=3, count=2),\n",
       " Row(business_id='_iTsyRdHmwDKT2pQLW3LaA', business_review_count=3, count=2),\n",
       " Row(business_id='OpoA5j5ohq8UqXpsyAXxpw', business_review_count=3, count=2),\n",
       " Row(business_id='ZS55tkaRnPR-QSi2i_871w', business_review_count=3, count=2),\n",
       " Row(business_id='eMlRICB4vFotYzHVgeNjWw', business_review_count=3, count=2),\n",
       " Row(business_id='JoZjCxjbS-01rjBz3KR8Nw', business_review_count=3, count=2),\n",
       " Row(business_id='ja1Svo3pX0Z_flliTrukHg', business_review_count=3, count=2),\n",
       " Row(business_id='KGoK1lpKmkTV6uMK2hrzfQ', business_review_count=3, count=2),\n",
       " Row(business_id='dAsnC9InN3ql_niiA6s8Tw', business_review_count=27, count=2),\n",
       " Row(business_id='Fs8u8swdxFOmZAZN2Djmag', business_review_count=3, count=2),\n",
       " Row(business_id='1J3eSZk7SoXef_cbJJ5jHQ', business_review_count=3, count=2),\n",
       " Row(business_id='Egu3Q_gHX2NmfGMi5voS8A', business_review_count=3, count=2),\n",
       " Row(business_id='cPDWvaxauKlokoV7MEVHxQ', business_review_count=3, count=2),\n",
       " Row(business_id='SpJYus_184M2TdT7t52rIA', business_review_count=3, count=2),\n",
       " Row(business_id='Ar7lMMYZIo1ND3AUzhW-Gw', business_review_count=3, count=2),\n",
       " Row(business_id='9FSnPeIIKpIxe7EUkd8JnA', business_review_count=3, count=2),\n",
       " Row(business_id='uPOCtXFMpkCjllwAscQBDg', business_review_count=3, count=2),\n",
       " Row(business_id='6WfesHZWlPY5c6RCadYGsg', business_review_count=3, count=2),\n",
       " Row(business_id='o5cjHbxPMcfqauzklOyPQw', business_review_count=3, count=2),\n",
       " Row(business_id='HHgHEtXyIO228Sybr6vdIg', business_review_count=3, count=2),\n",
       " Row(business_id='roxnZ0kodKkj5iR72UlQSw', business_review_count=3, count=2),\n",
       " Row(business_id='1z_l90NydQFJp9s5eYWkTw', business_review_count=3, count=2),\n",
       " Row(business_id='8KWc-2UW0EZrUgtuzLELNA', business_review_count=3, count=3),\n",
       " Row(business_id='BSPs1Z2sMBKErvOJIJMOEA', business_review_count=3, count=3),\n",
       " Row(business_id='hLMR8KpRT60aruFli-OtDQ', business_review_count=3, count=3),\n",
       " Row(business_id='qNqOVlOuCzcoG6b-LRhfKg', business_review_count=3, count=3),\n",
       " Row(business_id='rLBK2tT6651UA3jrJpOQUw', business_review_count=3, count=3),\n",
       " Row(business_id='iG0tDFIY2WmExsmq0gp80w', business_review_count=3, count=3),\n",
       " Row(business_id='ouqdDj6aw9rnCKjMCU3cWg', business_review_count=3, count=3),\n",
       " Row(business_id='K-AHrz65aeIIqd_kJqPzXA', business_review_count=3, count=3),\n",
       " Row(business_id='t1Azq5wUOYkmhAkyQNUkAw', business_review_count=3, count=3),\n",
       " Row(business_id='QtxCMUGahy0blSoTBGDClA', business_review_count=3, count=3),\n",
       " Row(business_id='gqHOgTCDmuw76tXAOF_giA', business_review_count=3, count=3),\n",
       " Row(business_id='tq1yTjjVDlLaodYO89zUdw', business_review_count=3, count=3),\n",
       " Row(business_id='IL3s95PpIboNRHh4rgV8XQ', business_review_count=3, count=3),\n",
       " Row(business_id='GJvh9J3Isdn2N4I_3j7OfQ', business_review_count=3, count=3),\n",
       " Row(business_id='w-VkZdwkdxta1puLyKLorA', business_review_count=3, count=3),\n",
       " Row(business_id='B0yRqjlDInEVtVr9aMKJaw', business_review_count=3, count=3),\n",
       " Row(business_id='KeS8tUgcRqmGJ7dAvbL-2Q', business_review_count=3, count=3),\n",
       " Row(business_id='sjXzaEF28BWywExGi_cuWg', business_review_count=3, count=3),\n",
       " Row(business_id='UPnidWBzdmMpPDR-wx_0LQ', business_review_count=3, count=3),\n",
       " Row(business_id='h4B1e0MUC1x6hDPDmozwWw', business_review_count=3, count=3),\n",
       " Row(business_id='kEC8phxfUK2PHHu2Z0Gj6w', business_review_count=3, count=3),\n",
       " Row(business_id='XDoN3mqtmG-j62v2VYOoaw', business_review_count=3, count=3),\n",
       " Row(business_id='ZIIHxgQ4uJSX8q9XqtwSFw', business_review_count=3, count=3),\n",
       " Row(business_id='04jtcZLxvrFOi_G9ElWYtQ', business_review_count=3, count=3),\n",
       " Row(business_id='bcxazL_npiO3Zyw1mbWzZg', business_review_count=3, count=3),\n",
       " Row(business_id='RHhio6zarLCeYrsyKna23w', business_review_count=3, count=3),\n",
       " Row(business_id='UonzheNs-TSXigHLnM0rYg', business_review_count=3, count=3),\n",
       " Row(business_id='AVxv-CWlrzkDREHnNyx9-g', business_review_count=3, count=3),\n",
       " Row(business_id='EvUguy_0FLqejAU6kgdziw', business_review_count=3, count=3),\n",
       " Row(business_id='2Q9VJZHgdkuLQeseX9N4-g', business_review_count=3, count=3),\n",
       " Row(business_id='nU-Dp5L6AnpE3VabaX9CQQ', business_review_count=3, count=3),\n",
       " Row(business_id='DuH4zyZkVcr6smVUvwrNZA', business_review_count=3, count=3),\n",
       " Row(business_id='lxSmRK319GawA7aQluNExQ', business_review_count=3, count=3),\n",
       " Row(business_id='w-bCLcuiZSHYq4QAtuiJiw', business_review_count=3, count=3),\n",
       " Row(business_id='o1zmKr00cIM5a1jLxCI21w', business_review_count=3, count=3),\n",
       " Row(business_id='eXGmTPLLc7BR2hGKtBqiZA', business_review_count=3, count=3),\n",
       " Row(business_id='0MKfNoq14qHQsppxWJzg1Q', business_review_count=3, count=3),\n",
       " Row(business_id='k6LAXUMqwvfy3g5Yin8fQQ', business_review_count=3, count=3),\n",
       " Row(business_id='NMqNtiEkiSCrZoq91b8LVg', business_review_count=3, count=3),\n",
       " Row(business_id='SFUs_wyUcF5tWpfYeOyYyQ', business_review_count=3, count=3),\n",
       " Row(business_id='-GkXrgdeRSn446BY8XrFYA', business_review_count=3, count=3),\n",
       " Row(business_id='ynN2PeYIpufc_eSjE2Rtbg', business_review_count=3, count=3),\n",
       " Row(business_id='nPPMqr6MKN6rdrBqU30gyw', business_review_count=3, count=3),\n",
       " Row(business_id='ZP9LEV9ylHlouE-ncHKowg', business_review_count=3, count=3),\n",
       " Row(business_id='KP24NhR-xnPbA6sEfQudLg', business_review_count=3, count=3),\n",
       " Row(business_id='XYXgAYal0NGDNAxrJSgGzg', business_review_count=3, count=3),\n",
       " Row(business_id='7ip3JWOAxL-FlTGQGaHLxQ', business_review_count=3, count=3),\n",
       " Row(business_id='IaL3NMcVUjwk29CnR7w4tw', business_review_count=3, count=3),\n",
       " Row(business_id='nYJFKWogKTuQysjjo6NZew', business_review_count=3, count=3),\n",
       " Row(business_id='FyoOUQEuhA2IGBBKoXelJw', business_review_count=3, count=3),\n",
       " Row(business_id='C2OrJwFnulmIRYuqvzBKdg', business_review_count=3, count=3),\n",
       " Row(business_id='HFAFnAC8lEWjz9xm1wrz6A', business_review_count=3, count=3),\n",
       " Row(business_id='ilELkaycQPNo0ikN3Ec5Rw', business_review_count=3, count=3),\n",
       " Row(business_id='yg--5IbP2PbBQlPlcLDSMQ', business_review_count=3, count=3),\n",
       " Row(business_id='QHesTH0rC20lRkx9upubVQ', business_review_count=3, count=3),\n",
       " Row(business_id='DIvqWWQr8yBi_s6f_lDURA', business_review_count=3, count=3),\n",
       " Row(business_id='RTCurn_NYMozOlvw8Mspmw', business_review_count=3, count=3),\n",
       " Row(business_id='OZL2XvGi5aXoX6Oynw7sJw', business_review_count=3, count=3),\n",
       " Row(business_id='9Z6WpoBnuBZBWr0ML6WRpQ', business_review_count=3, count=3),\n",
       " Row(business_id='jC7X3Vs5MsNffzt4C-59ag', business_review_count=3, count=3),\n",
       " Row(business_id='RbN_6T5Ara6NKSfPj2R-MA', business_review_count=3, count=3),\n",
       " Row(business_id='aPFiBLJOXqRd4Y6g4Ynx4Q', business_review_count=3, count=3),\n",
       " Row(business_id='xtzbpC8nvkPRnIKy7UTkig', business_review_count=3, count=3),\n",
       " Row(business_id='WQENq3d-59E96R9Cp7IHMg', business_review_count=3, count=3),\n",
       " Row(business_id='P26wevITtr_nn63X4qDBMQ', business_review_count=3, count=3),\n",
       " Row(business_id='JC2tSIdW8LDSOSmU7P6z2g', business_review_count=3, count=3),\n",
       " Row(business_id='17WMhO7Ygx-T9agjoD6Znw', business_review_count=3, count=3),\n",
       " Row(business_id='zrqeLHgpWke31dQZReit7w', business_review_count=3, count=3),\n",
       " Row(business_id='Z-6QSR2lC-opp9l6HLYG_w', business_review_count=3, count=3),\n",
       " Row(business_id='pvr3EqSH_uIC1oGm10KB_g', business_review_count=3, count=3),\n",
       " Row(business_id='loJTG5tXGDjcbUvGzg76Jg', business_review_count=3, count=3),\n",
       " Row(business_id='pzewHt4NnbyrGLqw_BUrWQ', business_review_count=3, count=3),\n",
       " Row(business_id='2FCgbiyufvkBdwKLH4FfHw', business_review_count=3, count=3),\n",
       " Row(business_id='XUjg4CEx_z51cIKGh_8MXQ', business_review_count=3, count=3),\n",
       " Row(business_id='qvR1DTJhYCwvmYRUccReSg', business_review_count=3, count=3),\n",
       " Row(business_id='qntawxFyIXM-BMJSJfcVYQ', business_review_count=3, count=3),\n",
       " Row(business_id='FB6fvsooMfMCwbiGBOefnA', business_review_count=3, count=3),\n",
       " Row(business_id='oWYzzx-hs4jVNHW0xLWUtA', business_review_count=3, count=3),\n",
       " Row(business_id='hO7kziifftiIViRK8dFoKA', business_review_count=3, count=3),\n",
       " Row(business_id='9qsezKsVNaNECLqVVGxJIg', business_review_count=3, count=3),\n",
       " Row(business_id='K1n4c1iwPP6iLIAR_o1D6g', business_review_count=3, count=3),\n",
       " Row(business_id='VMguQTq_vSncsqWnOHyQJA', business_review_count=3, count=3),\n",
       " Row(business_id='Q12cpzX-muy_N7_14KER6g', business_review_count=3, count=3),\n",
       " Row(business_id='he25lwYU_fofAiQqlPP5bQ', business_review_count=3, count=3),\n",
       " Row(business_id='0nyzmtzkjJm_AqkEMuKKlw', business_review_count=3, count=3),\n",
       " Row(business_id='sIG5CANiWws1fOGJRUDjAQ', business_review_count=3, count=3),\n",
       " Row(business_id='yhTRlXGTYgwlRGu7w4G4UA', business_review_count=3, count=3),\n",
       " Row(business_id='jbD7Bj3i0o94chP4PFAZkA', business_review_count=3, count=3),\n",
       " Row(business_id='l8U3A5dGtQUM9iubmaQlKw', business_review_count=3, count=3),\n",
       " Row(business_id='O0SituHw-BEVQro147pMeg', business_review_count=3, count=3),\n",
       " Row(business_id='cJw5Z6MGkqxEAFwf0CrQHA', business_review_count=3, count=3),\n",
       " Row(business_id='59qbNMYKia5w1LpSwdezPw', business_review_count=3, count=3),\n",
       " Row(business_id='XFwob57trBc9zaflcxa1bA', business_review_count=3, count=3),\n",
       " Row(business_id='x2xGInSOIZlT6Mc04wtBNg', business_review_count=3, count=3),\n",
       " Row(business_id='YY0zHGIqBIVcIpjVmOu54A', business_review_count=3, count=3),\n",
       " Row(business_id='PVveyZsT6nwhRZIncbrWww', business_review_count=3, count=3),\n",
       " Row(business_id='p_o19-gYFD1ewixxIV_HAw', business_review_count=3, count=3),\n",
       " Row(business_id='zVSGMN20-0sNL2dQss2pZw', business_review_count=3, count=3),\n",
       " Row(business_id='Hn7ZimIKh44w2POiRBgUFg', business_review_count=3, count=3),\n",
       " Row(business_id='vK7zRmGCYkCQJnqfajrTuA', business_review_count=3, count=3),\n",
       " Row(business_id='UrWRmtR96gk3yOEoMBJGcA', business_review_count=3, count=3),\n",
       " Row(business_id='4nOFzonin3g2kg9kph7aIQ', business_review_count=3, count=3),\n",
       " Row(business_id='RXhMtim4RXqUNbw9KeRMwQ', business_review_count=3, count=3),\n",
       " Row(business_id='i7OJk7zmkMMB2tQ_TLzPWg', business_review_count=3, count=3),\n",
       " Row(business_id='yDBYokzNTFH-_A1Mys4iDA', business_review_count=3, count=3),\n",
       " Row(business_id='VbYtdUn1_HM8Hv4vxTEGQQ', business_review_count=3, count=3),\n",
       " Row(business_id='0zJnUKQEurpIb_l4RDx3rw', business_review_count=3, count=3),\n",
       " Row(business_id='N9Sd6S_YH8WyPeZyAM0dnw', business_review_count=3, count=3),\n",
       " Row(business_id='8yjfHthp0TlZGYDVxFsIOA', business_review_count=3, count=3),\n",
       " Row(business_id='zKjL2-L2qT-4QDr7buj7bg', business_review_count=3, count=3),\n",
       " Row(business_id='1qLFgJzsSkpcbNznRiZxUg', business_review_count=3, count=3),\n",
       " Row(business_id='LpIzVxkFd09sEQ8TFMlrpg', business_review_count=3, count=3),\n",
       " Row(business_id='c-XMctLK6QAy1NjLSkcLRA', business_review_count=3, count=3),\n",
       " Row(business_id='L72WIRqdDFgNhJSfw7k_Ow', business_review_count=3, count=3),\n",
       " Row(business_id='cDoSFVJWZ-fRjN3ZFzuang', business_review_count=3, count=3),\n",
       " Row(business_id='rKcaF1oZLgClv0IPVIi3eg', business_review_count=3, count=3),\n",
       " Row(business_id='AElUmfwwN7PnYaxTHxVl6w', business_review_count=3, count=3),\n",
       " Row(business_id='Y0iufYcuEBl6DCOglptphw', business_review_count=3, count=3),\n",
       " Row(business_id='AcfhazpwycDN7uM87mcbgA', business_review_count=3, count=3),\n",
       " Row(business_id='cz_Ylt6F-yU_2uwAuDpO0g', business_review_count=3, count=3),\n",
       " Row(business_id='jvVUt2ph2VZG1cu3rR9Y5g', business_review_count=3, count=3),\n",
       " Row(business_id='dBAUTAHpsejsMybVyg9itw', business_review_count=3, count=3),\n",
       " Row(business_id='PaM8-tqEnxJMz995ImJ83w', business_review_count=3, count=3),\n",
       " Row(business_id='5O5nABu1OTOINGZnR2ytgw', business_review_count=3, count=3),\n",
       " Row(business_id='7Zpf05tANGgfDGC9hC3PtQ', business_review_count=3, count=3),\n",
       " Row(business_id='qhb0kYDqKxQaMqD0iec_Zg', business_review_count=3, count=3),\n",
       " Row(business_id='L3e0ZBj7zDSYMNaj0LATkA', business_review_count=3, count=3),\n",
       " Row(business_id='qWtgmc2s1sV00wxmX365Qw', business_review_count=3, count=3),\n",
       " Row(business_id='8PE1ChqEq8G0M3hcJoJHOA', business_review_count=3, count=3),\n",
       " Row(business_id='jk9MxXOAFjhHAKu8qWSbVg', business_review_count=3, count=3),\n",
       " Row(business_id='EbWr6MudSL2FSk_62i474g', business_review_count=3, count=3),\n",
       " Row(business_id='Lx3AlAQOYez45rNvUw2RQA', business_review_count=3, count=3),\n",
       " Row(business_id='DHP5gORcJBlpUacP33NjpA', business_review_count=3, count=3),\n",
       " Row(business_id='TCXmc8_3NHRj7XqvPsxR5Q', business_review_count=3, count=3),\n",
       " Row(business_id='UYjCyuRRhrGcKPBkP_wqOQ', business_review_count=3, count=3),\n",
       " Row(business_id='nE57nb6CkiejFpjbph9w-A', business_review_count=3, count=3),\n",
       " Row(business_id='INc0lk5heP6P76vf5Ly4fA', business_review_count=3, count=3),\n",
       " Row(business_id='fGNcUHaBey6T529oELbrvg', business_review_count=3, count=3),\n",
       " Row(business_id='5YleSU5qrntsnHAuYJd-rA', business_review_count=3, count=3),\n",
       " Row(business_id='_jxu6G83MHvA8ce96JJCGQ', business_review_count=3, count=3),\n",
       " Row(business_id='pv59B7swR4T6LFYK4aNwBA', business_review_count=3, count=3),\n",
       " Row(business_id='Qph2RWhxG-nu5ZkSAgKT0g', business_review_count=3, count=3),\n",
       " Row(business_id='TjdGsiKtOOzAmbqxtfHjhQ', business_review_count=3, count=3),\n",
       " Row(business_id='OjFhcNxN83qMqLMrQUxXag', business_review_count=3, count=3),\n",
       " Row(business_id='KQUBjSq7D8k7IFmfWdoKtw', business_review_count=3, count=3),\n",
       " Row(business_id='b78OMuOntgDZBYYuxSVhzg', business_review_count=3, count=3),\n",
       " Row(business_id='MxRNfjQywFs8sm5_VtuAng', business_review_count=3, count=3),\n",
       " Row(business_id='aQQ457hZMeYUxdDvyTZrHw', business_review_count=3, count=3),\n",
       " Row(business_id='4JawQ7HsD-xmFiXcSck1sA', business_review_count=3, count=3),\n",
       " Row(business_id='3Av-LKlSyqYyQoL533Yxsg', business_review_count=3, count=3),\n",
       " Row(business_id='TF579vsgOFuQ76sqyZRUOw', business_review_count=3, count=3),\n",
       " Row(business_id='6fsYPoLR8064d20UvTMLjw', business_review_count=3, count=3),\n",
       " Row(business_id='A014v-_dW6y0rGRxnHvZHA', business_review_count=3, count=3),\n",
       " Row(business_id='_ycbokaXvR0tHxaYaYwDcQ', business_review_count=3, count=3),\n",
       " Row(business_id='5F0TYbUhFl8c7fTwQzNtgw', business_review_count=3, count=3),\n",
       " Row(business_id='jGtX1-5zjlwyiILi0qJ1OA', business_review_count=3, count=3),\n",
       " Row(business_id='ISlHfcbqJ13m98_QREtp_w', business_review_count=3, count=3),\n",
       " Row(business_id='8i-QR-5hURXi8rAuo4kIVA', business_review_count=3, count=3),\n",
       " Row(business_id='p1TjlRsz7jHjsxLNIzAGhg', business_review_count=3, count=3),\n",
       " Row(business_id='HbiX90FP5uYNOwtCfsaAWg', business_review_count=3, count=3),\n",
       " Row(business_id='ivl7VOK6pWPx3n2Rj3Qjlw', business_review_count=3, count=3),\n",
       " Row(business_id='osVODz4p1KPslQU4cuU1sg', business_review_count=3, count=3),\n",
       " Row(business_id='jw7bfC2tVHL_6gkqg9DDPg', business_review_count=3, count=3),\n",
       " Row(business_id='qGXEuLH7Tx23bcAfY8i7rw', business_review_count=3, count=3),\n",
       " Row(business_id='i8QLvwrGRvUMQtIalnrL_Q', business_review_count=3, count=3),\n",
       " Row(business_id='vXf5a444ys2LRE3PWB6aUg', business_review_count=3, count=3),\n",
       " Row(business_id='jaW_S2fuJg3pf5JCQ5M_vg', business_review_count=3, count=3),\n",
       " Row(business_id='n8ZtUInemWvJzW2qzgIbOA', business_review_count=3, count=3),\n",
       " Row(business_id='KS7gQb0xi53LJsuBAefvmg', business_review_count=3, count=3),\n",
       " Row(business_id='RorYAXzgp4OtZUR1eD2iQw', business_review_count=3, count=3),\n",
       " Row(business_id='qdpub2kRBos-6TI9dUSOtw', business_review_count=3, count=3),\n",
       " Row(business_id='j3ZJmFCEnkPBddSqBjt_mg', business_review_count=3, count=3),\n",
       " Row(business_id='D7J9o6xO3V5DHLKt7a19Rw', business_review_count=3, count=3),\n",
       " Row(business_id='qxyPngFeUv4iSdOoXVFjkw', business_review_count=3, count=3),\n",
       " Row(business_id='jW5hwtR288HOjf9tdxCI-g', business_review_count=3, count=3),\n",
       " Row(business_id='mCy_GCe5bviiqmUZ7TXwdQ', business_review_count=3, count=3),\n",
       " Row(business_id='p66HDmRiwG0Te5zSj67Crg', business_review_count=3, count=3),\n",
       " Row(business_id='eJIYkn25Sm11KK0C0EqsZA', business_review_count=3, count=3),\n",
       " Row(business_id='7Ny6x4wX1JJLqdVeKQ1JYg', business_review_count=3, count=3),\n",
       " Row(business_id='4xoU4nfruODY0ODRrQB9Kg', business_review_count=3, count=3),\n",
       " Row(business_id='F8g0gnW3aZ96YlxuEMcKKQ', business_review_count=3, count=3),\n",
       " Row(business_id='3HwJ6-9KmYP1TammxZSO9g', business_review_count=3, count=3),\n",
       " Row(business_id='cdTlFbFNRukbm9VXjRtmWA', business_review_count=3, count=3),\n",
       " Row(business_id='r8nHNFUJkH9MzR6i7E0C8w', business_review_count=3, count=3),\n",
       " Row(business_id='ZQq4S-BcYyb6l7sd1fEwMw', business_review_count=3, count=3),\n",
       " Row(business_id='E5XjyUEqZNFWAnu5ze0NnA', business_review_count=3, count=3),\n",
       " Row(business_id='T5Q4HOAckxH98Zq4ESEyaQ', business_review_count=3, count=3),\n",
       " Row(business_id='t09msA0MsNsTZCn5eTPzLw', business_review_count=3, count=3),\n",
       " Row(business_id='AAIoOPPKUNL1-yLzOEv66Q', business_review_count=3, count=3),\n",
       " Row(business_id='wElA5yxklgd0xgEjCddMag', business_review_count=3, count=3),\n",
       " Row(business_id='x0kNZVWQ-1vxpxFhDdTOAQ', business_review_count=3, count=3),\n",
       " Row(business_id='CYD45HQ_cg2W2plhBvgYbw', business_review_count=3, count=3),\n",
       " Row(business_id='Bg4NKGDXCzDV46EXm3zLGQ', business_review_count=3, count=3),\n",
       " Row(business_id='JdbYyxufvaU6uwmNg0EqhQ', business_review_count=3, count=3),\n",
       " Row(business_id='D9-49m53GX41Quxd9OdSGQ', business_review_count=3, count=3),\n",
       " Row(business_id='YfvzKR0_JDBS0uf7iRkKmQ', business_review_count=3, count=3),\n",
       " Row(business_id='i_YGEsJNTvkHiCuKXLPCeQ', business_review_count=3, count=3),\n",
       " Row(business_id='_MEcyJ3Lscsd4F9ZKbmYNA', business_review_count=3, count=3),\n",
       " Row(business_id='hqRGL9vj3HDr9gH02Yj-vQ', business_review_count=3, count=3),\n",
       " Row(business_id='odynOoo6Ec7fLgZkXsQLdQ', business_review_count=3, count=3),\n",
       " Row(business_id='2Dy2WBU5Wj0dh3ntfwqRxw', business_review_count=3, count=3),\n",
       " Row(business_id='aFjl3aPOjM4yy1WNB9-FAA', business_review_count=3, count=3),\n",
       " Row(business_id='Hv15VTPEUKQCMjfHVN-HgA', business_review_count=3, count=3),\n",
       " Row(business_id='u3h41mmJ30NgURHf6ik_ag', business_review_count=3, count=3),\n",
       " Row(business_id='CY-hoqj0ib5e9ghUrCezKg', business_review_count=3, count=3),\n",
       " Row(business_id='YjfSIy1FhKRUGWZgGwj4tg', business_review_count=3, count=3),\n",
       " Row(business_id='kmtA5-Yz6sy5bbCdJ0T8-w', business_review_count=3, count=3),\n",
       " Row(business_id='KSIGq7FtrwsJ5IBmyGEUKA', business_review_count=3, count=3),\n",
       " Row(business_id='xinXcgLRuVjQ6W8ufxQxCw', business_review_count=3, count=3),\n",
       " Row(business_id='32MCxjAV0jwcD1n_-BQ_SA', business_review_count=3, count=3),\n",
       " Row(business_id='-FWhwGTP9YsV_5ONgdQ4hg', business_review_count=3, count=3),\n",
       " Row(business_id='rc-nBIOJ4onD1xIlogKmIw', business_review_count=3, count=3),\n",
       " Row(business_id='R8ior7Wkm44dBRlFIhUVsg', business_review_count=3, count=3),\n",
       " Row(business_id='e98gdPo99P0tMGjNpdB5nw', business_review_count=3, count=3),\n",
       " Row(business_id='yobej_WgXN2DzeNgNIR5UQ', business_review_count=3, count=3),\n",
       " Row(business_id='7MOYd-vtSgc5ZARyyqC4bA', business_review_count=3, count=3),\n",
       " Row(business_id='JTOCeakwYPcDIasUrngUbw', business_review_count=3, count=3),\n",
       " Row(business_id='G8SmiCze-N_62eGZ7U6drA', business_review_count=3, count=3),\n",
       " Row(business_id='NlpsCKu8FiGo2BvZSZdpFg', business_review_count=3, count=3),\n",
       " Row(business_id='jaIE-v1Y50Mkood66D50rA', business_review_count=3, count=3),\n",
       " Row(business_id='dgTvbf1lFSde5APkCK2nag', business_review_count=3, count=3),\n",
       " Row(business_id='iBxgN8ZBVcDzQ5HhvwuzhA', business_review_count=3, count=3),\n",
       " Row(business_id='tk-MgyEMz3kSPgYCgvC8DA', business_review_count=3, count=3),\n",
       " Row(business_id='L5cPTzfT8wRIZXRkmCljkg', business_review_count=3, count=3),\n",
       " Row(business_id='AJtRDRslOmLxSMWMcndVpg', business_review_count=3, count=3),\n",
       " Row(business_id='6SxrnI0BKc7bVj7Vcuu7Vw', business_review_count=3, count=3),\n",
       " Row(business_id='wneYX2CWQGFaboJp3qGdVw', business_review_count=3, count=3),\n",
       " Row(business_id='ecl4D6r9IiJ7xJnzWjQN-w', business_review_count=3, count=3),\n",
       " Row(business_id='PxQ_g5pfD1g2KTOO2bKWCA', business_review_count=3, count=3),\n",
       " Row(business_id='edFXHf-eCLhVA3AHbENOYg', business_review_count=3, count=3),\n",
       " Row(business_id='ytTFvHixT7hG0GVb2m8CBg', business_review_count=3, count=3),\n",
       " Row(business_id='BnNCSJO2NDvjqJVWp2EeBQ', business_review_count=3, count=3),\n",
       " Row(business_id='L9u8jZRq-kYKXmLdhPm6_A', business_review_count=3, count=3),\n",
       " Row(business_id='6yMyEVUemeN9aLz360x1Bg', business_review_count=3, count=3),\n",
       " Row(business_id='Z1etKi_kBsMr3oGolRrGaA', business_review_count=3, count=3),\n",
       " Row(business_id='29OFhmhw1c7s5F7DxmOEHw', business_review_count=3, count=3),\n",
       " Row(business_id='aaOBq1OE04KEYNC0kga7WA', business_review_count=3, count=3),\n",
       " Row(business_id='UyZqOcWxShRRtACCkZFkpQ', business_review_count=3, count=3),\n",
       " Row(business_id='tw1ure9rLoboPZ0h-ooyWw', business_review_count=3, count=3),\n",
       " Row(business_id='vK_UbrtYjWEXtXgFC7Ko_w', business_review_count=3, count=3),\n",
       " Row(business_id='FfQ_5hilPx25mj71N_cZYQ', business_review_count=3, count=3),\n",
       " Row(business_id='UfRT-t26YMMzUiOkTFOP4Q', business_review_count=3, count=3),\n",
       " Row(business_id='A0dOr_yTQO0PyKPNzfz52Q', business_review_count=3, count=3),\n",
       " Row(business_id='kXASVS10UcdxbtjCnprmbw', business_review_count=3, count=3),\n",
       " Row(business_id='PWjngofubkLClcJLt16L-Q', business_review_count=3, count=3),\n",
       " Row(business_id='PJXlSQDWXxXDecIKjwl7iA', business_review_count=3, count=3),\n",
       " Row(business_id='NEm4y5UghujKc9FkmPkb2Q', business_review_count=3, count=3),\n",
       " Row(business_id='JYWP4PR_9VDQEb0OqycoqQ', business_review_count=3, count=3),\n",
       " Row(business_id='_--o42LLuENj3JZobxykvQ', business_review_count=3, count=3),\n",
       " Row(business_id='bc4-cwjfQaCSDu3nh66Rkg', business_review_count=3, count=3),\n",
       " Row(business_id='gJ4J_i2YW7ffO5FcTqNI8w', business_review_count=3, count=3),\n",
       " Row(business_id='hYXLwY2_Qf7hCZVMSiBbAQ', business_review_count=3, count=3),\n",
       " Row(business_id='FjeYljvo8Af0iQZ9MkaaHg', business_review_count=3, count=3),\n",
       " Row(business_id='MLUIxvQA1k98KG-OtlGe9w', business_review_count=3, count=3),\n",
       " Row(business_id='xLDSUph_8z1NGxLSxfuUaA', business_review_count=3, count=3),\n",
       " Row(business_id='nF0WUm-rsDHRBpe2IYirWA', business_review_count=3, count=3),\n",
       " Row(business_id='sU9OwvFRMOsgQTwFVjU6lw', business_review_count=3, count=3),\n",
       " Row(business_id='XspSlamRfqU8blaT0rjPsA', business_review_count=3, count=3),\n",
       " Row(business_id='FAPncLWkZYxWVY_TSwQCjw', business_review_count=3, count=3),\n",
       " Row(business_id='8M17onEdv93jmP3rf0VM7A', business_review_count=3, count=3),\n",
       " Row(business_id='uWZwSMYw3QNtFWGjttw2MQ', business_review_count=3, count=3),\n",
       " Row(business_id='wx0lcNEvoMDSQ-of3TXVGA', business_review_count=3, count=3),\n",
       " Row(business_id='yS0wiHfV7Bdnx6z5wAQsQg', business_review_count=3, count=3),\n",
       " Row(business_id='UbxVaspYVhFQiO3eAkiD6w', business_review_count=3, count=3),\n",
       " Row(business_id='zQqPZfwy6IfekxfsdejzAA', business_review_count=3, count=3),\n",
       " Row(business_id='PFT5f10CRpVMwEmBLh7Dlw', business_review_count=3, count=3),\n",
       " Row(business_id='RWuk_fBzdWc__FBEsPozZw', business_review_count=3, count=3),\n",
       " Row(business_id='thS-Um7JyNrWDMt8IP_8MQ', business_review_count=3, count=3),\n",
       " Row(business_id='YRFDHOBOwt2WMR85r7Gzog', business_review_count=3, count=3),\n",
       " Row(business_id='L_nCjnJJcBdX07WjsQY-Bw', business_review_count=3, count=3),\n",
       " Row(business_id='60MxjRNI8jWNN-CGlnsbLw', business_review_count=3, count=3),\n",
       " Row(business_id='o1I9tQ37WWsPHTus5L82Ag', business_review_count=3, count=3),\n",
       " Row(business_id='hlkPqNb90hqZTgpnVDjsNg', business_review_count=3, count=3),\n",
       " Row(business_id='taC23FIDwT0EGDBzcEgtFw', business_review_count=3, count=3),\n",
       " Row(business_id='EsJzdxl7dmnoUF2zSGZ8QQ', business_review_count=3, count=3),\n",
       " Row(business_id='C5t3EwY7myqrOLVoh_fcCQ', business_review_count=3, count=3),\n",
       " Row(business_id='6t2yUxPPcZ33l1Vb_jJs2A', business_review_count=3, count=3),\n",
       " Row(business_id='n481ARQ6xZZtWkLchkFCXw', business_review_count=3, count=3),\n",
       " Row(business_id='neELaoG156-rZCKrSX2Q0Q', business_review_count=3, count=3),\n",
       " Row(business_id='O8ryiwZuocUK88k8FkJHAg', business_review_count=3, count=3),\n",
       " Row(business_id='EdbpFWsbBCVff8_jS5gS7w', business_review_count=3, count=3),\n",
       " Row(business_id='Z7uUe-lE3PaHwz7bh4XgVA', business_review_count=3, count=3),\n",
       " Row(business_id='B1vEIxf7_9RrrHz60X4-MQ', business_review_count=3, count=3),\n",
       " Row(business_id='7Xi2wL9IniTLJGuMifNnKA', business_review_count=3, count=3),\n",
       " Row(business_id='6N8X9OlhnF-5ZLFDnEF8NQ', business_review_count=3, count=3),\n",
       " Row(business_id='KO1i1hha2rzdbVqIe5U0vQ', business_review_count=3, count=3),\n",
       " Row(business_id='7M045BonrAsf89m8Zt_kEg', business_review_count=3, count=3),\n",
       " Row(business_id='zELRimyEh_AgD-lELdYm5Q', business_review_count=3, count=3),\n",
       " Row(business_id='QV2ynX_tKdZj35SVLLDebg', business_review_count=3, count=3),\n",
       " Row(business_id='91QvXL1NJvs_rZcTiN-D7Q', business_review_count=3, count=3),\n",
       " Row(business_id='dj2XkboYShGM9WvpkayJWA', business_review_count=3, count=3),\n",
       " Row(business_id='n8SkbCrvuwv1iHJyY4pnzw', business_review_count=3, count=3),\n",
       " Row(business_id='FW2Nr3w2DnFhtaITuTSCPw', business_review_count=3, count=3),\n",
       " Row(business_id='BGSHE8NbHEBFV4qCpDwcPw', business_review_count=3, count=3),\n",
       " Row(business_id='rrdsOP1fOAtywC-8wiFpTA', business_review_count=3, count=3),\n",
       " Row(business_id='UUCyYq9ghTQcCCs41cQfZQ', business_review_count=3, count=3),\n",
       " Row(business_id='EPOfP9T2XqH-wdtTlLgctQ', business_review_count=3, count=3),\n",
       " Row(business_id='MJH6UDLvXW8ySdp0pWiKQQ', business_review_count=3, count=3),\n",
       " Row(business_id='axVJwTg3rD85zqWnBVfIhA', business_review_count=3, count=3),\n",
       " Row(business_id='Dg0L3uU9BR1UgCaCG9Mo-Q', business_review_count=3, count=3),\n",
       " Row(business_id='a7C9J-SjyGGHXyTedk-ZVQ', business_review_count=3, count=3),\n",
       " Row(business_id='ERA5xF-Sr4v0hhG8eWwPnw', business_review_count=3, count=3),\n",
       " Row(business_id='Df4AyQtAHShAXE8X4BqTYA', business_review_count=3, count=3),\n",
       " Row(business_id='TUgkAtnqoiK4HymyGAvlyA', business_review_count=3, count=3),\n",
       " Row(business_id='ikf2NwCUMABjetNLe_XSCw', business_review_count=3, count=3),\n",
       " Row(business_id='QC1CKUF_Rbeo1-RckW1qbA', business_review_count=3, count=3),\n",
       " Row(business_id='HUk8iwcUWvUepkVDKL5RBw', business_review_count=3, count=3),\n",
       " Row(business_id='mzGrMnuHJpPJEF7E4esGqw', business_review_count=3, count=3),\n",
       " Row(business_id='0a1BBSewiusfCalA9UVEYA', business_review_count=3, count=3),\n",
       " Row(business_id='-TVac_C_sEBrKHkmZ8-guA', business_review_count=3, count=3),\n",
       " Row(business_id='SCEORWosUrYBSMzL8bfp8A', business_review_count=3, count=3),\n",
       " Row(business_id='5IIdYMexNto8sSqRluRJsQ', business_review_count=3, count=3),\n",
       " Row(business_id='Z3Uua6A3aeCsjLUX-xe7BA', business_review_count=3, count=3),\n",
       " Row(business_id='pbzWUJ58yodphrjdPNYnrw', business_review_count=3, count=3),\n",
       " Row(business_id='oZ6QpKWjKC1kOICc-my55Q', business_review_count=3, count=3),\n",
       " Row(business_id='oxq2jHZ4q5vbzYtBze01Zg', business_review_count=4, count=3),\n",
       " Row(business_id='i2QzZCNQQuMHyVORQkSLHg', business_review_count=3, count=3),\n",
       " Row(business_id='rf9BQgqS9veGoaND42eeHw', business_review_count=3, count=3),\n",
       " Row(business_id='ejc4W-dRVAKbaofVBADw0g', business_review_count=3, count=3),\n",
       " Row(business_id='RW0hFouTYdBCZdLECR8stw', business_review_count=3, count=3),\n",
       " Row(business_id='W9FhvXuPXOePP1DcGdpJTA', business_review_count=3, count=3),\n",
       " Row(business_id='bckM10enJclQn5wuokk_xw', business_review_count=3, count=3),\n",
       " Row(business_id='LUnlSLV2wMJM0-svcfHCTw', business_review_count=3, count=3),\n",
       " Row(business_id='Tg2zZ_cBsGxjHXCAGQvT0g', business_review_count=3, count=3),\n",
       " Row(business_id='oYLRQabCyjugA7AnbgchpQ', business_review_count=3, count=3),\n",
       " Row(business_id='Y2SDF5YF5Ly1oWIQcvWpCg', business_review_count=3, count=3),\n",
       " Row(business_id='tXx0AyBi4LJGBQdI-VjF6Q', business_review_count=3, count=3),\n",
       " Row(business_id='mSwr8_PzTtdTbk4g0PumFw', business_review_count=3, count=3),\n",
       " Row(business_id='3ajK8gw_jtnOxT8CuQKTtQ', business_review_count=3, count=3),\n",
       " Row(business_id='ebSUn2w_579yG6nrAn57Bw', business_review_count=3, count=3),\n",
       " Row(business_id='cC82b6PWlSaLYuW5Ui_isw', business_review_count=3, count=3),\n",
       " Row(business_id='505azllOke0Ob6oJ7kMuSg', business_review_count=3, count=3),\n",
       " Row(business_id='jOnABqtmSoieORiGXWUnew', business_review_count=3, count=3),\n",
       " Row(business_id='fNE8WzPxBONTfgMkD4t17Q', business_review_count=3, count=3),\n",
       " Row(business_id='KTY8-enKqXT4fqH6Wk4ucA', business_review_count=3, count=3),\n",
       " Row(business_id='L5lm3bZox2bEhMbm2sQN6Q', business_review_count=3, count=3),\n",
       " Row(business_id='PF7fVJtVaYsKy8xdrEzeHg', business_review_count=3, count=3),\n",
       " Row(business_id='HnGtPQ_jNQTnaIUmYEhX0g', business_review_count=3, count=3),\n",
       " Row(business_id='e4i45yu71otHCPVG4cSPvg', business_review_count=3, count=3),\n",
       " Row(business_id='CgJ18SuW03N0zUYtMd07DA', business_review_count=3, count=3),\n",
       " Row(business_id='pY_bfHdygNcWNa6H9VkTsw', business_review_count=3, count=3),\n",
       " Row(business_id='Wk-JJoXlmaozS0uqQf-Ngw', business_review_count=3, count=3),\n",
       " Row(business_id='i0DOikLpmkr2ycBomdu9Ig', business_review_count=3, count=3),\n",
       " Row(business_id='BzXhD2JpiDNAzPGYocP4hw', business_review_count=3, count=3),\n",
       " Row(business_id='zuExrx1--sXSbW8Iks7vTw', business_review_count=3, count=3),\n",
       " Row(business_id='MShj9LzV2MXaFs_sonkytg', business_review_count=3, count=3),\n",
       " Row(business_id='uhjVCnL-8T7mecvnhEq8xA', business_review_count=3, count=3),\n",
       " Row(business_id='qc764RiIDhpPB_fHROMjZw', business_review_count=3, count=3),\n",
       " Row(business_id='a6v7HilyCoZ8gTC-t8SBtw', business_review_count=3, count=3),\n",
       " Row(business_id='VjjP_YYJRLjmWd7s-0XgFA', business_review_count=3, count=3),\n",
       " Row(business_id='5UnvurKePI8papEL2DJt6A', business_review_count=3, count=3),\n",
       " Row(business_id='nL_IofJ0sQVigmj3HInPPg', business_review_count=3, count=3),\n",
       " Row(business_id='ZFd1nyu1jB7bpZun-KHErA', business_review_count=3, count=3),\n",
       " Row(business_id='vsP4VgobH_8wQSAMpqPXYA', business_review_count=3, count=3),\n",
       " Row(business_id='LlZFDFboGwf4uERysmGS1Q', business_review_count=3, count=3),\n",
       " Row(business_id='pecik4BXo_xGrL5zwH4noQ', business_review_count=3, count=3),\n",
       " Row(business_id='ZgGQZxCclFiPN-WllPLcwg', business_review_count=3, count=3),\n",
       " Row(business_id='ZCOlXSjVXtwDF4RdflA7Lg', business_review_count=3, count=3),\n",
       " Row(business_id='RNG5IjVp9gU1ok8OhE-1wQ', business_review_count=3, count=3),\n",
       " Row(business_id='O8fYdL0vOwuCbQ9cwAaHnA', business_review_count=3, count=3),\n",
       " Row(business_id='g7ACscfaKSTKmIuNRFDiqw', business_review_count=3, count=3),\n",
       " Row(business_id='pOfHQyxbrq9K5pykTJwf2g', business_review_count=3, count=3),\n",
       " Row(business_id='eghBLhn7q36minDrpUZ-bw', business_review_count=3, count=3),\n",
       " Row(business_id='trQP7DtlSRp6uwXuwLzbqw', business_review_count=3, count=3),\n",
       " Row(business_id='N9k36i5Ku_uAdJ9udMWKFw', business_review_count=3, count=3),\n",
       " Row(business_id='SEoKgkmX67uBNBO2g6cCFw', business_review_count=3, count=3),\n",
       " Row(business_id='w34N2j8lHIkjQ2RKsiplNw', business_review_count=3, count=3),\n",
       " Row(business_id='823B1Vl5vpg3Oz32HzDzCw', business_review_count=3, count=3),\n",
       " Row(business_id='ujHyL38EI-OxAyN4_En3wA', business_review_count=3, count=3),\n",
       " Row(business_id='7Mmaw96-qmDtFW-rakXoVQ', business_review_count=3, count=3),\n",
       " Row(business_id='n1OJrLF-iuOi3w6btTYW4g', business_review_count=3, count=3),\n",
       " Row(business_id='MOUWErK6NEf08EF9khIOSg', business_review_count=3, count=3),\n",
       " Row(business_id='t97JdgEBy7aOpx2J5cPObA', business_review_count=3, count=3),\n",
       " Row(business_id='FyDP0xY4nY9u3LrMRyPhVw', business_review_count=3, count=3),\n",
       " Row(business_id='cDhsN_381RDh1wZrfsJ75Q', business_review_count=3, count=3),\n",
       " Row(business_id='qhlEzXyOfph858J9dw32LA', business_review_count=3, count=3),\n",
       " Row(business_id='uyEfNDZgI-RwwYgXbiTZjg', business_review_count=4, count=3),\n",
       " Row(business_id='PfQcKt0r3WmVb2dFOd5udA', business_review_count=3, count=3),\n",
       " Row(business_id='xeQHAf3Gfa-6M7lrNhyAyg', business_review_count=3, count=3),\n",
       " Row(business_id='cBZJFFNU8tRk5ciqB7n0ig', business_review_count=3, count=3),\n",
       " Row(business_id='MG0S1VL45OCbqgAsAUmX_A', business_review_count=3, count=3),\n",
       " Row(business_id='nUqwTRbsUYObaPCM535aGQ', business_review_count=3, count=3),\n",
       " Row(business_id='8XJhsXJ4c5z8Qm_XLFI4Ng', business_review_count=3, count=3),\n",
       " Row(business_id='NvuDGsric-OFWsC7BzD6ug', business_review_count=3, count=3),\n",
       " Row(business_id='tuhhALFFsPePkpaFDaoNOg', business_review_count=3, count=3),\n",
       " Row(business_id='LdRM-ZiOjlgEc2U2nOMCbA', business_review_count=3, count=3),\n",
       " Row(business_id='hKeUxD6hmx2gtW5TYXp1cQ', business_review_count=3, count=3),\n",
       " Row(business_id='J8PbfZbU7XTs-h55ezuQcQ', business_review_count=3, count=3),\n",
       " Row(business_id='tRbFTTYAiPMuAIifuNpa5A', business_review_count=3, count=3),\n",
       " Row(business_id='TGXXZe4zuQk5CZbZrtXYUA', business_review_count=3, count=3),\n",
       " Row(business_id='DF3fGWKavF6E0VSeawkR5A', business_review_count=3, count=3),\n",
       " Row(business_id='fp_yeL2o-MkaI_TLD-nTVw', business_review_count=3, count=3),\n",
       " Row(business_id='i0c6YF9f--FhJPgJyeY3wg', business_review_count=3, count=3),\n",
       " Row(business_id='fJMMDHixNolirYi1Zp_A_Q', business_review_count=3, count=3),\n",
       " Row(business_id='RWXu8u_jFR43hiLpXeQh1A', business_review_count=3, count=3),\n",
       " Row(business_id='UKVxg3eFNvdKuPDSRT_yxA', business_review_count=3, count=3),\n",
       " Row(business_id='jCacle289y_krOVMwTnEVQ', business_review_count=3, count=3),\n",
       " Row(business_id='RhyvfzdlH7k-2KU4ADXv7Q', business_review_count=3, count=3),\n",
       " Row(business_id='njXqwVhEvEDNEMu3U8jS9g', business_review_count=3, count=3),\n",
       " Row(business_id='15to24Q-otAHmto7FzsWRg', business_review_count=3, count=3),\n",
       " Row(business_id='UWp92Ie7MhyjhDHObeTscQ', business_review_count=3, count=3),\n",
       " Row(business_id='FhJfFYmbPD9NIlojI21ZoQ', business_review_count=3, count=3),\n",
       " Row(business_id='wq09eyI_SNIEni4ALRX5kQ', business_review_count=3, count=3),\n",
       " Row(business_id='N4Dd4BjbV7U1ZAb0sDCHWQ', business_review_count=3, count=3),\n",
       " Row(business_id='ll9PWPSzuHudE8J1BT1G2A', business_review_count=3, count=3),\n",
       " Row(business_id='PEVEN4McCgs4z8SndFbbYg', business_review_count=3, count=3),\n",
       " Row(business_id='o45CYsY8jG2lKKI9ZBuBxQ', business_review_count=3, count=3),\n",
       " Row(business_id='Piw03nxAXZF86UYaVrDn1g', business_review_count=3, count=3),\n",
       " Row(business_id='BfcWfaiRa0irr-M53E8OIQ', business_review_count=3, count=3),\n",
       " Row(business_id='TFsHZsmzvxF1a43WPY073w', business_review_count=3, count=3),\n",
       " Row(business_id='xNPwH9FynmMoWUWRnk0sPQ', business_review_count=3, count=3),\n",
       " Row(business_id='6iZhmEXeGf4Hc9PF1a9pTQ', business_review_count=3, count=3),\n",
       " Row(business_id='LW0lEYD2Edd_J5JapoiRcA', business_review_count=3, count=3),\n",
       " Row(business_id='ZTMy_2ZlRlN7PLGVQ_KUKA', business_review_count=3, count=3),\n",
       " Row(business_id='tvg0EUF6Mjq9MzS8lXnQ9A', business_review_count=3, count=3),\n",
       " Row(business_id='pbKBYXjPmg47yQjzNW4LJw', business_review_count=3, count=3),\n",
       " Row(business_id='joFhoQkuHvhjNOlz4xuOpA', business_review_count=3, count=3),\n",
       " Row(business_id='FzKLECPZaIzvcrGJw4bS3A', business_review_count=3, count=3),\n",
       " Row(business_id='Dp0m7Hpgk17S26ypi-BoEw', business_review_count=3, count=3),\n",
       " Row(business_id='7qdb7MIMzXYg2L2_bHg3_Q', business_review_count=3, count=3),\n",
       " Row(business_id='CTQM1RHSh6fZQPf7bRHObA', business_review_count=3, count=3),\n",
       " Row(business_id='kR47yJ8_HiEkFpXEHPD2sA', business_review_count=3, count=3),\n",
       " Row(business_id='ygiL30hEoRHRFyA2K_TpsQ', business_review_count=3, count=3),\n",
       " Row(business_id='0WM0ok-5LZaAv5xlvqCVCg', business_review_count=3, count=3),\n",
       " Row(business_id='yI_iiFKg-h3UmiJX0v0nJg', business_review_count=3, count=3),\n",
       " Row(business_id='Q6qsuT8GH2MMwjLdkvBpbw', business_review_count=3, count=3),\n",
       " Row(business_id='_qKiVzRgBBwvjj6okikAqQ', business_review_count=3, count=3),\n",
       " Row(business_id='dsXwluYKt6W8sQ3P1VonQw', business_review_count=3, count=3),\n",
       " Row(business_id='uffe_f_EefpHxHLn1JUA8w', business_review_count=3, count=3),\n",
       " Row(business_id='UmrnroZ5iauPIsAD1Bs_Rw', business_review_count=3, count=3),\n",
       " Row(business_id='2TCEy0enoE4OM1RaiQX3Kw', business_review_count=3, count=3),\n",
       " Row(business_id='b-3pdX16fN0S33i0oza0PA', business_review_count=3, count=3),\n",
       " Row(business_id='1ha3iYlapOvpOhy_zMO6Tw', business_review_count=3, count=3),\n",
       " Row(business_id='dP7ivVZp0qjFowxa3IoDAw', business_review_count=3, count=3),\n",
       " Row(business_id='iK8sHemyJ2hSDlDGBp5JAA', business_review_count=3, count=3),\n",
       " Row(business_id='0nAFblolSC3TfbHDq8yjGQ', business_review_count=3, count=3),\n",
       " Row(business_id='e06WCye4cqWuA0ow_006FQ', business_review_count=3, count=3),\n",
       " Row(business_id='OCfGq5JcfJoj6XpJqNKQKQ', business_review_count=3, count=3),\n",
       " Row(business_id='7Wj3lyrLaMXde6G-v4yRKg', business_review_count=3, count=3),\n",
       " Row(business_id='_CO1fjU5-d0r0AazUaFkoQ', business_review_count=3, count=3),\n",
       " Row(business_id='-Sovk0Q8s-d7yDlk5Dh30w', business_review_count=3, count=3),\n",
       " Row(business_id='rLpZRRjhpUOJVOJnU3KeUw', business_review_count=3, count=3),\n",
       " Row(business_id='M6r6jKPr5tcpdatDn9K5lA', business_review_count=3, count=3),\n",
       " Row(business_id='ihZIqOdjjSvNzjtCszsnww', business_review_count=3, count=3),\n",
       " Row(business_id='aQKix0iMREiJsFGr7mH1mA', business_review_count=3, count=3),\n",
       " Row(business_id='jU_G3GHDlu2cDMTqIV0inQ', business_review_count=3, count=3),\n",
       " Row(business_id='bSVV7hJEJIJ5at2gW_d55w', business_review_count=3, count=3),\n",
       " Row(business_id='Zgx6fhvAydAWX8VunnJRCA', business_review_count=3, count=3),\n",
       " Row(business_id='Se8GzSpwTpSqfq7ER_BnPA', business_review_count=3, count=3),\n",
       " Row(business_id='LAZV00op8Np3JiItqC0S_g', business_review_count=3, count=3),\n",
       " Row(business_id='9HnIGXJcz_5dPP5Mh2IJ4Q', business_review_count=3, count=3),\n",
       " Row(business_id='AULNidnXWwWPVf8Ag0m2Rw', business_review_count=3, count=3),\n",
       " Row(business_id='9-eMqJiY9YW_IVTIandMHw', business_review_count=3, count=3),\n",
       " Row(business_id='BsxitzNoAiiTpOuXRTXIwQ', business_review_count=3, count=3),\n",
       " Row(business_id='OjDCdpuJ3CUonATh7SDi4A', business_review_count=3, count=3),\n",
       " Row(business_id='okZ6lbMVxkmIgn9Y6Qk2TQ', business_review_count=3, count=3),\n",
       " Row(business_id='3XDj7eLxpvvIhneUi5iT0Q', business_review_count=3, count=3),\n",
       " Row(business_id='A5yVFUy0FxwJwVup93tXZA', business_review_count=3, count=3),\n",
       " Row(business_id='yU3OrrCmDDEqidyIrrcDbw', business_review_count=3, count=3),\n",
       " Row(business_id='W0Gqu1L55LX5VvaP8MI8hA', business_review_count=3, count=3),\n",
       " Row(business_id='WPISzUkdgXQPFANj-coznQ', business_review_count=3, count=3),\n",
       " Row(business_id='R2VvmzgxCVk6WQS8qpzWqg', business_review_count=3, count=3),\n",
       " Row(business_id='4L6LGSO4wcGEHcIlswed1A', business_review_count=3, count=3),\n",
       " Row(business_id='9nkl_Z2_ztPV8_uGVL6gdw', business_review_count=3, count=3),\n",
       " Row(business_id='VXeRcpsxF-I0GXPv2wJcQw', business_review_count=3, count=3),\n",
       " Row(business_id='PdYkiviahdou7IBiahl9lA', business_review_count=3, count=3),\n",
       " Row(business_id='wLBzrVX0hGyowIYBAM41VA', business_review_count=3, count=3),\n",
       " Row(business_id='OdvWiu42pI8lL4DJ-eRFGQ', business_review_count=3, count=3),\n",
       " Row(business_id='fPgnWucfcEOzOhEfmoG9SQ', business_review_count=3, count=3),\n",
       " Row(business_id='d6qz12oUrdIxMJmg_T0XzQ', business_review_count=3, count=3),\n",
       " Row(business_id='OeJU1URt8oyhGfTNc290DA', business_review_count=3, count=3),\n",
       " Row(business_id='bPDfUbhr5Jab_fPTNTn9oQ', business_review_count=3, count=3),\n",
       " Row(business_id='L3ib0jiEGTXToq1Puc7Vqw', business_review_count=3, count=3),\n",
       " Row(business_id='lbvxrX4nTVpv_fHu9WNvVA', business_review_count=3, count=3),\n",
       " Row(business_id='2DCSbqKBoD9OwjsZ1uFuog', business_review_count=3, count=3),\n",
       " Row(business_id='uDAy7GdWSZJS6qQoxR0_HQ', business_review_count=3, count=3),\n",
       " Row(business_id='JdeAQHYWKHFJ9Jav5OepZA', business_review_count=3, count=3),\n",
       " Row(business_id='VoHZ4U8gvY33BxXtcsy5Rw', business_review_count=3, count=3),\n",
       " Row(business_id='Ws9xaO5DpAjLKJq6AImuhQ', business_review_count=3, count=3),\n",
       " Row(business_id='liXGTKOZ3ngHd5IZHAQ41A', business_review_count=3, count=3),\n",
       " Row(business_id='OAflyXeQ59YMV9_f2HO2DQ', business_review_count=3, count=3),\n",
       " Row(business_id='FylCLxLIUbrBM16OVLfB_Q', business_review_count=3, count=3),\n",
       " Row(business_id='LtxLc6tuiVgs4Jvro-wbww', business_review_count=3, count=3),\n",
       " Row(business_id='vtio1TgA1nvLgqd0UcotYg', business_review_count=3, count=3),\n",
       " Row(business_id='JlnMgV5JKnQU7QLBjsFf0w', business_review_count=3, count=3),\n",
       " Row(business_id='8gYwyKBo4DrmmFZxfL8_2g', business_review_count=3, count=3),\n",
       " Row(business_id='RAkXl2GwAaNVvS8-BCy59Q', business_review_count=3, count=3),\n",
       " Row(business_id='XPjlZl2HOeNR7JzuXEOegQ', business_review_count=3, count=3),\n",
       " Row(business_id='V2TaLceKiGwy-y7bAGzPuQ', business_review_count=3, count=3),\n",
       " Row(business_id='luZkpCTe1uV2rqj8cmur4A', business_review_count=3, count=3),\n",
       " Row(business_id='UkfAqnFkOaFEOhNxhCREbQ', business_review_count=3, count=3),\n",
       " Row(business_id='Ltznorfn-4xzwC1_IFludg', business_review_count=3, count=3),\n",
       " Row(business_id='XmlwwkE1KSzVrNdzwko5mA', business_review_count=3, count=3),\n",
       " Row(business_id='SQj4aQTJ2b3tbiK0kHNtNQ', business_review_count=3, count=3),\n",
       " Row(business_id='NjtSuYBNG-DEGNQ6koCWFg', business_review_count=3, count=3),\n",
       " Row(business_id='Ro9zMHw8vxQzc4D69qAcew', business_review_count=3, count=3),\n",
       " Row(business_id='GaEXBK59KJCO0qyap1Ye9A', business_review_count=3, count=3),\n",
       " Row(business_id='qWhf94ZFuA11p0j1LrGlhA', business_review_count=3, count=3),\n",
       " Row(business_id='yCbGr7qQvr9Qnoo2d1WdhA', business_review_count=3, count=3),\n",
       " Row(business_id='1LdvubOnn7Uf_xDpc5JcMA', business_review_count=3, count=3),\n",
       " Row(business_id='mR0qq-Xbiwu__UJjNkDorg', business_review_count=3, count=3),\n",
       " Row(business_id='Z0eJo_ZMufDJ4KcLHPVddw', business_review_count=3, count=3),\n",
       " Row(business_id='qVxQOd2ApDKEE3kcgereMw', business_review_count=3, count=3),\n",
       " Row(business_id='ba5jCpBlDpJCnq7tV9WeKg', business_review_count=3, count=3),\n",
       " Row(business_id='dXjA9dYG09-iYtxxu_jksg', business_review_count=3, count=3),\n",
       " Row(business_id='ptO-2UMzCfzBqHcV0Fwf8g', business_review_count=3, count=3),\n",
       " Row(business_id='Qmdt_r2e9z5HtmPEolLvWg', business_review_count=3, count=3),\n",
       " Row(business_id='7aEDbL2uuh2Gwfl8-aTFdA', business_review_count=3, count=3),\n",
       " Row(business_id='f7GSz0r0mGtv8GRBJq7n9A', business_review_count=3, count=3),\n",
       " Row(business_id='WO4oYsQsq4rJHoy-g2Lutw', business_review_count=3, count=3),\n",
       " Row(business_id='_awxyIPCSl7FwXIMHJ6IKw', business_review_count=3, count=3),\n",
       " Row(business_id='bKBhbF76GSTsfBQO8b5mSA', business_review_count=3, count=3),\n",
       " Row(business_id='4Nn9QoRDkOr-DWMF7KPqOQ', business_review_count=3, count=3),\n",
       " Row(business_id='MMM52bcIRTqyzMdidY-aMQ', business_review_count=3, count=3),\n",
       " Row(business_id='urtt8vB8ILa5On4D09fOXw', business_review_count=3, count=3),\n",
       " Row(business_id='JuBBXxdzgMF9s7bEwKr7dQ', business_review_count=3, count=3),\n",
       " Row(business_id='j8UUwx7bkPzO3Yi3SCA3PQ', business_review_count=3, count=3),\n",
       " Row(business_id='poVI2RjCF7RozmDfx1qafw', business_review_count=3, count=3),\n",
       " Row(business_id='Cswsfb0_uSgwHxyHMqFKwg', business_review_count=3, count=3),\n",
       " Row(business_id='7T1TYnAQYq_jwY_PuPRi0A', business_review_count=3, count=3),\n",
       " Row(business_id='zfn2YAYzU3y5SsJqRLJfMQ', business_review_count=3, count=3),\n",
       " Row(business_id='EUuwygxZYQlMlKmmrwPGZA', business_review_count=3, count=3),\n",
       " Row(business_id='O7uzVmRB8GDMU8XaX5l7Eg', business_review_count=3, count=3),\n",
       " Row(business_id='SYSHQAyEr2CFKvxPZuixuQ', business_review_count=3, count=3),\n",
       " Row(business_id='8_AmYY6baBQVqgW_VeXMXA', business_review_count=3, count=3),\n",
       " Row(business_id='TsVF3CByEzog2WqMLT-xxA', business_review_count=3, count=3),\n",
       " Row(business_id='7OmQshEGWcHgUGrhjcj2BQ', business_review_count=3, count=3),\n",
       " Row(business_id='UhHuqyWJ5YP9CC8Y-m808g', business_review_count=3, count=3),\n",
       " Row(business_id='olmAEE81O2vQsqJdD20YWw', business_review_count=3, count=3),\n",
       " Row(business_id='tpZfJdRi64OTBN4g7lRM3Q', business_review_count=3, count=3),\n",
       " Row(business_id='kHKGaaK4yAOT1End-XS65g', business_review_count=3, count=3),\n",
       " Row(business_id='ipTZ74Jf1OykhVfkY-4R_g', business_review_count=3, count=3),\n",
       " Row(business_id='JyrJkXRJ0N-ZX7pzZyrUMQ', business_review_count=3, count=3),\n",
       " Row(business_id='Dkpq1c1UndQIRIpqUBSwCw', business_review_count=3, count=3),\n",
       " Row(business_id='wBsZShKsemJ9B2Fxm9ntAw', business_review_count=3, count=3),\n",
       " Row(business_id='MGS723NaPVM-wu6XBVA0VA', business_review_count=3, count=3),\n",
       " Row(business_id='0P80meqrUpsJlHty1iuxVg', business_review_count=3, count=3),\n",
       " Row(business_id='OLfhthpdOZ-urMjOyEdi9g', business_review_count=3, count=3),\n",
       " Row(business_id='Yx22tJ_qBBHUGjbRgghhGQ', business_review_count=3, count=3),\n",
       " Row(business_id='sqj_mT_qGeqXU1wJASwNqA', business_review_count=3, count=3),\n",
       " Row(business_id='_4Or_PJSm9hXK12GA-vYfQ', business_review_count=3, count=3),\n",
       " Row(business_id='cPcUqzaNRtTLIq0E6LRqfg', business_review_count=3, count=3),\n",
       " Row(business_id='1JpEKidDBIYGXL_MwTPaoA', business_review_count=3, count=3),\n",
       " Row(business_id='yLGx6qqYVSjZ99pOJAPdXg', business_review_count=3, count=3),\n",
       " Row(business_id='xg7DgICgg4xqeLlF4zdLVw', business_review_count=3, count=3),\n",
       " Row(business_id='ysAq-mQmH017uM--rQml3A', business_review_count=3, count=3),\n",
       " Row(business_id='uFR3usTqHIyzw4kxU7rjOA', business_review_count=3, count=3),\n",
       " Row(business_id='HtV_94xg0bUSqi13T2xAng', business_review_count=3, count=3),\n",
       " Row(business_id='YyQ8WvwwxvsV6Nh5tlsdfw', business_review_count=3, count=3),\n",
       " Row(business_id='ylhmmzzyl-211VDiPaNDqw', business_review_count=3, count=3),\n",
       " Row(business_id='UyjF0jo9NgLkW5146WJoZg', business_review_count=3, count=3),\n",
       " Row(business_id='DZEQ3Ttn7JLPOMPWo2mwpg', business_review_count=3, count=3),\n",
       " Row(business_id='nEmmosWbGAs9HLMw9Gqxtg', business_review_count=3, count=3),\n",
       " Row(business_id='VXU6HNn-rpT3X5s_nDlTLg', business_review_count=3, count=3),\n",
       " Row(business_id='W7lhg5OonEgzEtiee2C1RA', business_review_count=3, count=3),\n",
       " Row(business_id='jl10tcPbqndF3lKQbO0psQ', business_review_count=3, count=3),\n",
       " Row(business_id='xk_Bl9x1JXGOj_L2UUM3rQ', business_review_count=3, count=3),\n",
       " Row(business_id='DdH6h7CyVRoEPYgj7KLMmQ', business_review_count=3, count=3),\n",
       " Row(business_id='5vPf9ymJuN3TSDCC8qLcPA', business_review_count=3, count=3),\n",
       " Row(business_id='qG8kBgRqS_pp0ndYpdL6ew', business_review_count=3, count=3),\n",
       " Row(business_id='2v8w3gU6ko8r9PQqpKDznA', business_review_count=3, count=3),\n",
       " Row(business_id='1y-KLm911xn8jZBCJfwfhQ', business_review_count=3, count=3),\n",
       " Row(business_id='mxsgQr4mugtFCsu8ZqzFLA', business_review_count=3, count=3),\n",
       " Row(business_id='aq9HL9R_vr3d0Jvh_vP_HQ', business_review_count=3, count=3),\n",
       " Row(business_id='r69l8JjGBCn5_RCJN9gh1Q', business_review_count=3, count=3),\n",
       " Row(business_id='nnFaG2FtH4FSyn6TfnASqA', business_review_count=3, count=3),\n",
       " Row(business_id='6ra1wTOYlQ4XjzVrBuE4Ig', business_review_count=3, count=3),\n",
       " Row(business_id='rPRhaoOS7MgpWkG9yzBCjQ', business_review_count=3, count=3),\n",
       " Row(business_id='lCiLB93bFqGIY_09lc_RRA', business_review_count=3, count=3),\n",
       " Row(business_id='WVEv6RpYjaDUEFLkQDYjgw', business_review_count=3, count=3),\n",
       " Row(business_id='HSFVbyVeZl7RSjMcgfNLBA', business_review_count=3, count=3),\n",
       " Row(business_id='-IvATB9k2qNz19Gy0Q2NQw', business_review_count=3, count=3),\n",
       " Row(business_id='6vsvM-v9pNRygE1Z1UMWtg', business_review_count=3, count=3),\n",
       " Row(business_id='DR8ZNcvzCv7J961kTzIqXg', business_review_count=3, count=3),\n",
       " Row(business_id='6h5UmbIV4HPFj5bklXj75w', business_review_count=3, count=3),\n",
       " Row(business_id='zEBredwSc3VcTu9Cb95-bg', business_review_count=3, count=3),\n",
       " Row(business_id='ypk64AtPFw8dY9HM-cQzsQ', business_review_count=3, count=3),\n",
       " Row(business_id='9hnyuencQIbpw8La9Fv0JQ', business_review_count=3, count=3),\n",
       " Row(business_id='d8EMQU_gXJx2St21TiYHsg', business_review_count=3, count=3),\n",
       " Row(business_id='Px6jHbn82gk05QvazuXnJg', business_review_count=3, count=3),\n",
       " Row(business_id='z8vgLc2ypHQAlz5qcTCnUg', business_review_count=3, count=3),\n",
       " Row(business_id='mphvfq5T8gM5cJ0eccwe_g', business_review_count=3, count=3),\n",
       " Row(business_id='_BBOK7MztBYzAexoFzTgEw', business_review_count=3, count=3),\n",
       " Row(business_id='sGX1RkfTVz4UE6dW0ahYBg', business_review_count=3, count=3),\n",
       " Row(business_id='3VhU5FQPYfHtYzxlwjKEAw', business_review_count=3, count=3),\n",
       " Row(business_id='OcXcAVjHoX0tTuzT5A8oww', business_review_count=3, count=3),\n",
       " Row(business_id='yh-aIIvVa7Gha_JhkTpd8g', business_review_count=3, count=3),\n",
       " Row(business_id='HMja0LgKco-PcSAdWhyRPA', business_review_count=3, count=3),\n",
       " Row(business_id='RIHydChuDTrfuuvr8TzH4g', business_review_count=3, count=3),\n",
       " Row(business_id='pN-qHhdu5ll3e6Wn1yaMkQ', business_review_count=3, count=3),\n",
       " Row(business_id='bo6V2g5XsTHRki3Rx9p4_Q', business_review_count=3, count=3),\n",
       " Row(business_id='QYIne4ghfzXpQrN1bzor8g', business_review_count=3, count=3),\n",
       " Row(business_id='3z8HM287uDsUTFTZAyXQRg', business_review_count=3, count=3),\n",
       " Row(business_id='_UV_1mPTjEQhvKX9PE2BlQ', business_review_count=3, count=3),\n",
       " Row(business_id='xhOTMvzYDXN5cv-FxAZFZA', business_review_count=3, count=3),\n",
       " Row(business_id='cdMVwA8h8M6pMs_Esxth-A', business_review_count=3, count=3),\n",
       " Row(business_id='pazOmG0hn5Hy6s4wPNS3ow', business_review_count=3, count=3),\n",
       " Row(business_id='Yfv6gPNdLomtKqFaiM3yFQ', business_review_count=3, count=3),\n",
       " Row(business_id='4Sw949z60DupwinXF14IpA', business_review_count=3, count=3),\n",
       " Row(business_id='yGuz3w_gXFyn9odKkn5DDg', business_review_count=3, count=3),\n",
       " Row(business_id='yFKXkUvUAJ2htaSg3JdxPA', business_review_count=3, count=3),\n",
       " Row(business_id='RxrTvWN2Fn9LRkFxD3LIXQ', business_review_count=3, count=3),\n",
       " Row(business_id='slvxNyRMwY52Xd9Cd5ElEA', business_review_count=3, count=3),\n",
       " Row(business_id='wZEK7GmLJ3hmkYBiRCo4Qw', business_review_count=3, count=3),\n",
       " Row(business_id='5z1tzT8C7H1l7onTttRvnw', business_review_count=3, count=3),\n",
       " Row(business_id='YpXKwCODp0LMlvtYVlnxQQ', business_review_count=3, count=3),\n",
       " Row(business_id='uB2Eyyr51fdBCQmpylKjxQ', business_review_count=3, count=3),\n",
       " Row(business_id='Q6Ad0-wNUu8tf_-OCnWT9w', business_review_count=3, count=3),\n",
       " Row(business_id='1p4B2K4gi5nqR6C9x9AhxA', business_review_count=3, count=3),\n",
       " Row(business_id='bEDXAOVuwgGJ5n_Z7iuPjQ', business_review_count=3, count=3),\n",
       " Row(business_id='K_p4VMcJ64GsDg_jmfqOPw', business_review_count=3, count=3),\n",
       " Row(business_id='lbTwhLVVWrdZz0DtrO208A', business_review_count=3, count=3),\n",
       " Row(business_id='SHOeL3VN_hIL9RGgqDhLgA', business_review_count=3, count=3),\n",
       " Row(business_id='yzlROKrfPWg_POdnw0gICg', business_review_count=3, count=3),\n",
       " Row(business_id='PJoFqIDhuKNYjAOmXnVbPA', business_review_count=3, count=3),\n",
       " Row(business_id='y6t8Fy5rccRuB9z8odYNuQ', business_review_count=3, count=3),\n",
       " Row(business_id='WRbqYb4DEfG0s5Ppr1sI9g', business_review_count=3, count=3),\n",
       " Row(business_id='obXqyHC8OBniyTRWfBQo2A', business_review_count=3, count=3),\n",
       " Row(business_id='KbVJ0FC1l61GVSniCNhP1g', business_review_count=3, count=3),\n",
       " Row(business_id='6l8WydUS1faDhOZWIeILbw', business_review_count=3, count=3),\n",
       " Row(business_id='qRzYUw0wEfITRDv9cGQVYQ', business_review_count=3, count=3),\n",
       " Row(business_id='JFPmKPRjoBukISOLxS0UCA', business_review_count=3, count=3),\n",
       " Row(business_id='DN3c3ANXoQ8E1tfrrDQSBg', business_review_count=3, count=3),\n",
       " Row(business_id='X20OIKcQnESnExiqD9_t9g', business_review_count=3, count=3),\n",
       " Row(business_id='YReWPXJZ7jOTE-FB2wYPvw', business_review_count=3, count=3),\n",
       " Row(business_id='HUZovVc8erN08q8-Bt7Szw', business_review_count=3, count=3),\n",
       " Row(business_id='8c_e0TwxSDnRSBYnw8j2Bw', business_review_count=3, count=3),\n",
       " Row(business_id='J-YcXd_F5w2bxOJJojwClQ', business_review_count=3, count=3),\n",
       " Row(business_id='ocxgCD2LukKZqMrsm-Ov7g', business_review_count=3, count=3),\n",
       " Row(business_id='_8aBU2J1MZR1Sms9RqLmYA', business_review_count=3, count=3),\n",
       " Row(business_id='WzFGD2v5ge7BOLDmKk-kEw', business_review_count=3, count=3),\n",
       " Row(business_id='Egc1zxFSKGwmje0gXi95-w', business_review_count=3, count=3),\n",
       " Row(business_id='ohAU8bISMlhqda3qqE1iLQ', business_review_count=3, count=3),\n",
       " Row(business_id='ddzASqd4YSbsiRttlZ1syQ', business_review_count=3, count=3),\n",
       " Row(business_id='pgK07kPLiGXhq2FWARyN6A', business_review_count=3, count=3),\n",
       " Row(business_id='MsKeshCDxuFda6Pl-yUVBQ', business_review_count=3, count=3),\n",
       " Row(business_id='l8yj8Z69Kun2FriFPcnViQ', business_review_count=3, count=3),\n",
       " Row(business_id='wKm7tCyMsehWgphTrdJsfA', business_review_count=3, count=3),\n",
       " Row(business_id='tmH-ARxe5rmtyJ4hEXN-LA', business_review_count=3, count=3),\n",
       " Row(business_id='j1fyIuQwrJWHJwR38UMysQ', business_review_count=3, count=3),\n",
       " Row(business_id='HvfB4kZf_XNp35uKCzzxwA', business_review_count=3, count=3),\n",
       " Row(business_id='nOPZjXboGsyPHn3B8IOFfg', business_review_count=3, count=3),\n",
       " Row(business_id='B4TY_GCJl5z4b42fJrkmqA', business_review_count=3, count=3),\n",
       " Row(business_id='HFmOJM_sBPb4AZTzzUU4AQ', business_review_count=3, count=3),\n",
       " Row(business_id='ZmKEOQDkA0qYcPlrgLezmg', business_review_count=3, count=3),\n",
       " Row(business_id='OWw5_o8eLkWs5pJLCfjWeQ', business_review_count=3, count=3),\n",
       " Row(business_id='Q81Q8YE8GqmLqG4-jouVZA', business_review_count=3, count=3),\n",
       " Row(business_id='aDNUS6Z-hDRL6D_lhWlFtw', business_review_count=3, count=3),\n",
       " Row(business_id='z3reQo0VWJtAg29xKp1wHA', business_review_count=3, count=3),\n",
       " Row(business_id='1A3uYPr5QrPZmBEy4e1UxQ', business_review_count=3, count=3),\n",
       " Row(business_id='8wDBrUPxtr94ThJQ6WjD3A', business_review_count=3, count=3),\n",
       " Row(business_id='hkoqV51IV875Jdzspa7lkA', business_review_count=3, count=3),\n",
       " Row(business_id='TWUJTrNfobnbzQiEfZOytA', business_review_count=3, count=3),\n",
       " Row(business_id='J4bGFF3EM-Sf00fPKxEpSg', business_review_count=3, count=3),\n",
       " Row(business_id='xZyltUBb0qla-MMQLhrVIg', business_review_count=3, count=3),\n",
       " Row(business_id='Zfu8iPneHkcLFqk1xAGAMg', business_review_count=3, count=3),\n",
       " Row(business_id='LKYrQ1UZ18x70YgEfR8s5g', business_review_count=3, count=3),\n",
       " Row(business_id='T3e_-jJtyZhjZQC5SOHbXQ', business_review_count=3, count=3),\n",
       " Row(business_id='I_3WnGBtij0BNzpFZgVZkw', business_review_count=3, count=3),\n",
       " Row(business_id='y5wfzC0d-LOXVSAH88CSBA', business_review_count=3, count=3),\n",
       " Row(business_id='tAtIQGO0JbJFBuM_zcQ_0w', business_review_count=3, count=3),\n",
       " Row(business_id='Va0hedZQH9aoMDxuDCZ2qg', business_review_count=3, count=3),\n",
       " Row(business_id='2Smvl8BWpiY6MDxpjGRTxQ', business_review_count=3, count=3),\n",
       " Row(business_id='bukZAbCoHgCi-IN91r04bQ', business_review_count=3, count=3),\n",
       " Row(business_id='pCktHkZpZ96TJPbXdl7uAw', business_review_count=3, count=3),\n",
       " Row(business_id='nQGp1dmlbGarKVSB0AWxIw', business_review_count=3, count=3),\n",
       " Row(business_id='1qp0uAaTQtveTxInTZ1ZLA', business_review_count=3, count=3),\n",
       " Row(business_id='TOtR5q1YzrIw5SlX1xzl9w', business_review_count=3, count=3),\n",
       " Row(business_id='qmctaQ2PqZRBdrg18TOU9w', business_review_count=3, count=3),\n",
       " Row(business_id='MZGpgB8-zBMN62p0iNKsKw', business_review_count=3, count=3),\n",
       " Row(business_id='4V1bOWHLpepjn126M6fHrg', business_review_count=3, count=3),\n",
       " Row(business_id='5CNZG9BRzI3IsSmPjpg2Xw', business_review_count=3, count=3),\n",
       " Row(business_id='juzbhH2ecsJvWwp712fY9g', business_review_count=3, count=3),\n",
       " Row(business_id='l-cqWTPX1KZPIjjkB4arvQ', business_review_count=3, count=3),\n",
       " Row(business_id='KKlKPqsL_EzySjEaDJjWIA', business_review_count=3, count=3),\n",
       " Row(business_id='clU62-alZxOzCyONoiCpYw', business_review_count=3, count=3),\n",
       " Row(business_id='EQ_xWS55psSTwWwaagVTDQ', business_review_count=3, count=3),\n",
       " Row(business_id='w8PBJExdvnPM_C0Wq_hL0g', business_review_count=3, count=3),\n",
       " Row(business_id='YYCmBcK3zgBakjMpggyorA', business_review_count=3, count=3),\n",
       " Row(business_id='XDNL2h_nZb5GbGoB0HWmKg', business_review_count=3, count=3),\n",
       " Row(business_id='4rPiZbqKZtTa2sc8atKs2A', business_review_count=3, count=3),\n",
       " Row(business_id='Ul8XmXst70Z6nUoLIbfgbg', business_review_count=3, count=3),\n",
       " Row(business_id='zA-5qI5HVh-_CaY886SrLA', business_review_count=3, count=3),\n",
       " Row(business_id='6GQVNKVhFjkPQ7Yrqjcp3g', business_review_count=3, count=3),\n",
       " Row(business_id='5qGL-boVb9RuABKm8SE0IA', business_review_count=3, count=3),\n",
       " Row(business_id='Qb6ehMYZbSmgIca_XTiPEQ', business_review_count=3, count=3),\n",
       " Row(business_id='4SPreBzBMbTA1PbvUGU1vw', business_review_count=3, count=3),\n",
       " Row(business_id='TMG3g1KyviRzhQbABgXN-w', business_review_count=3, count=3),\n",
       " Row(business_id='OIzj9gWbdUzWZo8Dssg5gQ', business_review_count=3, count=3),\n",
       " Row(business_id='GKnYP4axN4f2gjYm_MgY6g', business_review_count=3, count=3),\n",
       " Row(business_id='rpg3Zyr-YUhEvnvcK4cGuw', business_review_count=3, count=3),\n",
       " Row(business_id='FpxFNBdL9V8Y7xN6fFW7kA', business_review_count=3, count=3),\n",
       " Row(business_id='WxGM94Zwf4DNCTaxKxax7w', business_review_count=3, count=3),\n",
       " Row(business_id='MrrE1Ut6Mzi9Vyo0ULH9ZA', business_review_count=3, count=3),\n",
       " Row(business_id='MlqmLzt01Fs5EJE4TzkrrQ', business_review_count=3, count=3),\n",
       " Row(business_id='dtW8dDAh_KD3LAFAd_y8lw', business_review_count=3, count=3),\n",
       " Row(business_id='KSGgGyX2WehScEs3xqjAzg', business_review_count=3, count=3),\n",
       " Row(business_id='ZZn2sUiOBJZqPZyD8Gkf_g', business_review_count=3, count=3),\n",
       " Row(business_id='fcGCuWpXCh2eLZoIpU1AzQ', business_review_count=3, count=3),\n",
       " Row(business_id='B3fn9Ytm8GEEivWq6KpktA', business_review_count=3, count=3),\n",
       " Row(business_id='cMKaFzoWF4CI0KFAs3cI7w', business_review_count=3, count=3),\n",
       " Row(business_id='c8k7foiWwVbxVZEpVQdLXw', business_review_count=3, count=3),\n",
       " Row(business_id='LizO_aJOIgIKHYHCAs4Q6Q', business_review_count=3, count=3),\n",
       " Row(business_id='E9vwNT6rvoZk7AXwlgFQxQ', business_review_count=3, count=3),\n",
       " Row(business_id='uN48M69lTtcqQo-TrKoIOA', business_review_count=3, count=3),\n",
       " Row(business_id='1kMmw4scjJ8IFC7JzNlJeg', business_review_count=3, count=3),\n",
       " Row(business_id='Z9ewhosJ8ts7ZcIoOGdUvA', business_review_count=3, count=3),\n",
       " Row(business_id='e5HHNtD7gvZ4WM037zYuKA', business_review_count=3, count=3),\n",
       " Row(business_id='RUhr5Sy0xPQV1RVI5OpHRA', business_review_count=3, count=3),\n",
       " Row(business_id='5qZjwJTX3Flms0rw2G3oEA', business_review_count=3, count=3),\n",
       " Row(business_id='7zSVeeWX-8uVN5LHSzRQtA', business_review_count=3, count=3),\n",
       " Row(business_id='vjAZfDIMsuuJufYolPsW6A', business_review_count=3, count=3),\n",
       " Row(business_id='WJDOhlp18w3B5qjfSKM6Nw', business_review_count=3, count=3),\n",
       " Row(business_id='arCvIfuSjFqqe7-wbo96nQ', business_review_count=3, count=3),\n",
       " Row(business_id='0OGqZp8bSrXtLcLRPd5XfA', business_review_count=3, count=3),\n",
       " Row(business_id='4SPd65eJ1K0KiBPvXpo50A', business_review_count=3, count=3),\n",
       " Row(business_id='hsyxHwVCHBM4__-1StjfAg', business_review_count=3, count=3),\n",
       " Row(business_id='O8hnVYSGeMjXhuc_c-FZbQ', business_review_count=3, count=3),\n",
       " Row(business_id='ZM6AnLb5UIWj17AdyKTGgg', business_review_count=3, count=3),\n",
       " Row(business_id='mhI-cjcjtzq0SLYCGO29sQ', business_review_count=3, count=3),\n",
       " Row(business_id='5I1-Ck4Qgxii6sV73IPa_w', business_review_count=3, count=3),\n",
       " Row(business_id='4AA6Rx8ILRd4BMAOtEhGHQ', business_review_count=3, count=3),\n",
       " Row(business_id='aWqwrT1_BtkeILlE39szcQ', business_review_count=3, count=3),\n",
       " Row(business_id='nzN_92LD8uClqajFORnepg', business_review_count=3, count=3),\n",
       " Row(business_id='8aQ2FkMF2fDVeHAYCcgqSg', business_review_count=3, count=3),\n",
       " Row(business_id='zEeMhibCS7e4YbrJtRcClw', business_review_count=3, count=3),\n",
       " Row(business_id='p1zbmLpQ3bn2hCAxNV-FOw', business_review_count=3, count=3),\n",
       " Row(business_id='VigfQ7xC_f4kRAhxyBhLJA', business_review_count=3, count=3),\n",
       " Row(business_id='ErOrvk6Uu9myOb3AMQ218w', business_review_count=3, count=3),\n",
       " Row(business_id='W35qu-T2Fdj-TNuDAd1lGA', business_review_count=3, count=3),\n",
       " Row(business_id='_qVH7N2Ven3BuJkrvw6ORA', business_review_count=3, count=3),\n",
       " Row(business_id='AHmCsUfOiUvi38Cp8y2t-w', business_review_count=3, count=3),\n",
       " Row(business_id='D_zYqX5T2YckgqZuonXbxQ', business_review_count=3, count=3),\n",
       " Row(business_id='02AC1JvNgfmm4zdagzZevw', business_review_count=3, count=3),\n",
       " Row(business_id='WbKvE6n46a1lynr56Uq3OA', business_review_count=3, count=3),\n",
       " Row(business_id='hA5d0x7yJwxZkEj8LT59Kw', business_review_count=3, count=3),\n",
       " Row(business_id='-3wWjEQfVxRaCQG-PuKRMw', business_review_count=3, count=3),\n",
       " Row(business_id='xo4GKjNEK1R3l66f6OVIgA', business_review_count=3, count=3),\n",
       " Row(business_id='8b4amK9y5miP6-UzfH5K0A', business_review_count=3, count=3),\n",
       " Row(business_id='B5OXS0beUt4lNxrV6uxaJw', business_review_count=3, count=3),\n",
       " Row(business_id='Na8TXwmGdqR4CaQINi2ogg', business_review_count=3, count=3),\n",
       " Row(business_id='xMRNp1DRtwNabYxoY4MD5Q', business_review_count=3, count=3),\n",
       " Row(business_id='mTTlEmVnnewox833o0hNJA', business_review_count=3, count=3),\n",
       " Row(business_id='vDGhmuUREbzSG9nojWIYVA', business_review_count=3, count=3),\n",
       " Row(business_id='WCjDihJH7VP5jXX-b-2pNw', business_review_count=3, count=3),\n",
       " Row(business_id='ee_HzDGUJSVmJs9sqal7yQ', business_review_count=3, count=3),\n",
       " Row(business_id='lZyfUz4DZaUA9Y4ps0C_Cw', business_review_count=3, count=3),\n",
       " Row(business_id='Y5JED9wsVQwk-Fl6LUQY8Q', business_review_count=3, count=3),\n",
       " Row(business_id='6Th0vVjtlAlTE0cu-xrCng', business_review_count=3, count=3),\n",
       " Row(business_id='4SQpnL7Wk0nxF3eG9y4ptg', business_review_count=3, count=3),\n",
       " Row(business_id='vHaXbmEl6K_Lf-w5PFGBSA', business_review_count=3, count=3),\n",
       " Row(business_id='OUK0TD3_DQ196kUMvF_PKg', business_review_count=3, count=3),\n",
       " Row(business_id='5HS0n9IIS54LauV6AilwSQ', business_review_count=3, count=3),\n",
       " Row(business_id='G08jiG6D-CJ7cf5dDw6drA', business_review_count=3, count=3),\n",
       " Row(business_id='qLSOstsEk4edZdbdkLEnLg', business_review_count=3, count=3),\n",
       " Row(business_id='rm1RtdDdyYighdxyFz09zg', business_review_count=3, count=3),\n",
       " Row(business_id='sJF_sk61XGqyP_CkuyOldg', business_review_count=3, count=3),\n",
       " Row(business_id='F53MSa5SYzO9BG8c_JhskQ', business_review_count=3, count=3),\n",
       " Row(business_id='hI04LLEWSUdvdBDVvao9Nw', business_review_count=3, count=3),\n",
       " Row(business_id='LXLtlZzdJfT6iBH6OV9Pww', business_review_count=3, count=3),\n",
       " Row(business_id='2bvoK4d8f2iKdrie94UUCQ', business_review_count=3, count=3),\n",
       " Row(business_id='E8RmViOIGWUBeGgdz40URw', business_review_count=3, count=3),\n",
       " Row(business_id='O_MvJAHkwF1YtXDsnnlJnw', business_review_count=3, count=3),\n",
       " Row(business_id='fgDp7gfEcWlTeci8E_Ihlg', business_review_count=3, count=3),\n",
       " Row(business_id='K3YIi3wn-lTBcMBzu7rw1w', business_review_count=3, count=3),\n",
       " Row(business_id='ZtlvMed86vuWfpCaI7oxpg', business_review_count=3, count=3),\n",
       " Row(business_id='QGhV2GPpBD_nWs1IwquueQ', business_review_count=3, count=3),\n",
       " Row(business_id='1U7ngrf1mIM97VSBqfZlPQ', business_review_count=3, count=3),\n",
       " Row(business_id='lzsEOElAeLxUGtjYOnPoUQ', business_review_count=3, count=3),\n",
       " Row(business_id='Mn-LKQhgW_pSkphFYrkKng', business_review_count=3, count=3),\n",
       " Row(business_id='pkxQ91fZxbIkDPfXkV4phQ', business_review_count=3, count=3),\n",
       " Row(business_id='yWmGplCGCnQBy9LJFaGa6A', business_review_count=3, count=3),\n",
       " Row(business_id='8gzRJEmYJGTCdahRKcafXw', business_review_count=3, count=3),\n",
       " Row(business_id='z5hh524-H3q-65CfVXIi4Q', business_review_count=3, count=3),\n",
       " Row(business_id='RuUoIs0tdxv2fV2vGDPanw', business_review_count=3, count=3),\n",
       " Row(business_id='ZCaKwZDJ0vGssT7j7tCfQg', business_review_count=3, count=3),\n",
       " Row(business_id='4zXJtz1kFaU7BJ_bOqqfzQ', business_review_count=3, count=3),\n",
       " Row(business_id='rC7bKjfa5qH8iznBQ3G-8A', business_review_count=3, count=3),\n",
       " Row(business_id='mB3iWJvo7l3ZVwgHpyD78A', business_review_count=3, count=3),\n",
       " Row(business_id='0YNie6WxpqG7N4P1qgWuqQ', business_review_count=3, count=3),\n",
       " Row(business_id='gZVQ9CqyWaX4YxFv7ew9_Q', business_review_count=3, count=3),\n",
       " Row(business_id='qo5zH2vfCduPSZzmZfJzEg', business_review_count=3, count=3),\n",
       " Row(business_id='Ft2atJZShrTKeia1NfRxLw', business_review_count=3, count=3),\n",
       " Row(business_id='YrAcPbca-spJe8VKwVJyig', business_review_count=3, count=3),\n",
       " Row(business_id='L-EopaWNLjs2KrEgNNXyYw', business_review_count=3, count=3),\n",
       " Row(business_id='BK_wYsFjvPlb8kdzC2wlpQ', business_review_count=3, count=3),\n",
       " Row(business_id='JPBHpiGDsdaXcfgtEcG1OQ', business_review_count=3, count=3),\n",
       " Row(business_id='i5VtK6SC_qg1zldRzJOpTw', business_review_count=3, count=3),\n",
       " Row(business_id='e7oR6WPnt7252rvwtxj5og', business_review_count=3, count=3),\n",
       " Row(business_id='Ex9CAr9HRaT1KYf2Ekf58g', business_review_count=3, count=3),\n",
       " Row(business_id='ciDaLZY95c3XHYjB-YA04A', business_review_count=3, count=3),\n",
       " Row(business_id='tPi6NHPas-9t5pJOipTTWQ', business_review_count=3, count=3),\n",
       " Row(business_id='oP7S5UToeXGVpzlg6dbFKA', business_review_count=3, count=3),\n",
       " Row(business_id='xXv7KTFCDaMgkfF7NwT-Zw', business_review_count=3, count=3),\n",
       " Row(business_id='u0bVesp8ZqWUvTYyg81o4w', business_review_count=3, count=3),\n",
       " Row(business_id='Y9ACnc4JpVyw5tvm8rDbSQ', business_review_count=3, count=3),\n",
       " Row(business_id='qkjOhzGvUPSdKX7ssnRoVA', business_review_count=3, count=3),\n",
       " Row(business_id='ZRrkDsZicUfuX4BnjCkUAw', business_review_count=3, count=3),\n",
       " Row(business_id='AFblc5Ke0ZK_kxxy0OXsxw', business_review_count=3, count=3),\n",
       " Row(business_id='ZZVfFppAwOUBv7f-bNNvJg', business_review_count=3, count=3),\n",
       " Row(business_id='Wz2K5ff8Un-LNGGfOMPseg', business_review_count=3, count=3),\n",
       " Row(business_id='cYT0Th1LYSyAxrz0-ZbuhQ', business_review_count=3, count=3),\n",
       " Row(business_id='g31vJtmktkM6WQghOD0VIA', business_review_count=3, count=3),\n",
       " Row(business_id='NDZ1R1aq-hQFqinP-udHxQ', business_review_count=3, count=3),\n",
       " Row(business_id='vbWQvKr3fl-RvogVeLXk3g', business_review_count=3, count=3),\n",
       " Row(business_id='douyZ0DDIprgc7S5_Z7JkA', business_review_count=3, count=3),\n",
       " Row(business_id='Kpmy4Pld2FbJixlJtEnP4w', business_review_count=3, count=3),\n",
       " Row(business_id='gMkk3KGaZ5goUnY5J3PKug', business_review_count=3, count=3),\n",
       " Row(business_id='N7k5P9vNaMlmvig4zJ_H4Q', business_review_count=3, count=3),\n",
       " Row(business_id='xp3l3oEzfdCKutyjOnS-pA', business_review_count=3, count=3),\n",
       " Row(business_id='q1IxHtEyuYLk7hNnMPOm6g', business_review_count=3, count=3),\n",
       " Row(business_id='wbVVlOlq9XcWDFkJRyR_EQ', business_review_count=3, count=3),\n",
       " Row(business_id='6gZ4Jl65H56sMCfRBe4BiQ', business_review_count=3, count=3),\n",
       " Row(business_id='UqR2_ckoS7wIG7CDIMWMWw', business_review_count=3, count=3),\n",
       " Row(business_id='qm1Ki-FbRA48giTbmIv1_Q', business_review_count=3, count=3),\n",
       " Row(business_id='NmiX_7talK5QTgMKvHPUaQ', business_review_count=3, count=3),\n",
       " Row(business_id='CrnkfKMBrhs9S-2xqGKlgw', business_review_count=3, count=3),\n",
       " Row(business_id='Zua91nLoti4rx1VCkuIe6g', business_review_count=3, count=3),\n",
       " Row(business_id='K0MfmK34JuCO0kn9tdeSpA', business_review_count=3, count=3),\n",
       " Row(business_id='oobv4CHcrM3m3wxTvoiQJw', business_review_count=3, count=3),\n",
       " Row(business_id='Q-fNyVi4ZDz5jJkps8xYRw', business_review_count=3, count=3),\n",
       " Row(business_id='DrfQcrx-1Y1YBHUS06y9pw', business_review_count=3, count=3),\n",
       " Row(business_id='aVSETfnXg2ztYRn_q2hAug', business_review_count=3, count=3),\n",
       " Row(business_id='XwGSlQhDF-SxhCAUjlBPLw', business_review_count=3, count=3),\n",
       " Row(business_id='y1HEJrHSfYaeeIG_6ItMAw', business_review_count=3, count=3),\n",
       " Row(business_id='jOEcpy_XHmVR6c9tRcntXQ', business_review_count=3, count=3),\n",
       " Row(business_id='4CkAvwCy5265YKAurY1ZKg', business_review_count=3, count=3),\n",
       " Row(business_id='_s-7pGcbRtcV7r0Hpi7oNA', business_review_count=3, count=3),\n",
       " Row(business_id='HEaZ-hnwK-FWvz5MklT-fg', business_review_count=3, count=3),\n",
       " Row(business_id='lcKC_DKBJn0dRBG-EIw6ug', business_review_count=3, count=3),\n",
       " Row(business_id='_dh6uB299dJFPEwBp-aftA', business_review_count=3, count=3),\n",
       " Row(business_id='r3Px2dU0JatrrBBV66bBnA', business_review_count=3, count=3),\n",
       " Row(business_id='07MpHT8TrGDI_UVktfKMPA', business_review_count=3, count=3),\n",
       " Row(business_id='MHoRFW0k2yPKlv8Ti0Ri9A', business_review_count=3, count=3),\n",
       " Row(business_id='K7iDRCWPTbcLuHQoPR0rIA', business_review_count=3, count=3),\n",
       " Row(business_id='dNdmgR8nr7yK8F5d0o8avA', business_review_count=3, count=3),\n",
       " Row(business_id='So1fzEDZ_qcFDhyYd6XG7w', business_review_count=3, count=3),\n",
       " Row(business_id='H8VawNDJVWbSuqs2kKgDMw', business_review_count=3, count=3),\n",
       " Row(business_id='xrAJYKqINEcto7Q2iftKow', business_review_count=3, count=3),\n",
       " Row(business_id='_dtAA_i4P_sr0Z44YSudIQ', business_review_count=3, count=3),\n",
       " Row(business_id='mf1SXS4Nwdhso3eW6D54iw', business_review_count=3, count=3),\n",
       " Row(business_id='5Q1aXEKK8KxsX65QwTYLDA', business_review_count=3, count=3),\n",
       " Row(business_id='UZNkvZ7zhnobmO7K9Vmcsg', business_review_count=3, count=3),\n",
       " Row(business_id='LnDL3ariIUl69h-gLenm4w', business_review_count=3, count=3),\n",
       " Row(business_id='10d6PDujQkQmrvQMK6V0xA', business_review_count=3, count=3),\n",
       " Row(business_id='_TrkM4jGtGtd2MIydT2FDA', business_review_count=3, count=3),\n",
       " Row(business_id='6PsLzotx8s4lwCby430HBQ', business_review_count=3, count=3),\n",
       " Row(business_id='yIBqBGLKAxPdsuH2tUfFQg', business_review_count=3, count=3),\n",
       " Row(business_id='rjUuCbH7q-bSw5iWMwMHhQ', business_review_count=3, count=3),\n",
       " Row(business_id='TawJNwtAG8jT6TUc3L_dPw', business_review_count=3, count=3),\n",
       " Row(business_id='bmqqQPsRtxjuPt1Ovxp54A', business_review_count=3, count=3),\n",
       " Row(business_id='R_wQkUJHfAMxCSqgW3qXKQ', business_review_count=3, count=3),\n",
       " Row(business_id='--sdH6tFAdEs7j4Msr7nPA', business_review_count=3, count=3),\n",
       " Row(business_id='hSfsnEa9q1Xx-0lAYDTLGw', business_review_count=3, count=3),\n",
       " Row(business_id='Fhu2zDauHkdFT-uBQGLKjw', business_review_count=3, count=3),\n",
       " Row(business_id='e647NFPwpdWFEElbNowA9w', business_review_count=3, count=3),\n",
       " Row(business_id='nFSPJL7bQCJf_68in5hSEg', business_review_count=3, count=3),\n",
       " Row(business_id='v1TLi2f3VIHzvXUBGO7uIw', business_review_count=3, count=3),\n",
       " Row(business_id='qSwzJb2mmi_KbHdZDfiB-w', business_review_count=3, count=3),\n",
       " Row(business_id='aj7CS2CAgEoDv86UPFmoag', business_review_count=3, count=3),\n",
       " Row(business_id='6ztJ5JUDt6qxEuPegkNcvA', business_review_count=3, count=3),\n",
       " Row(business_id='sfLKlCSniopU8ml12lFb-Q', business_review_count=3, count=3),\n",
       " Row(business_id='Tg8tWvujg9ZoZRbq6jktEg', business_review_count=3, count=3),\n",
       " Row(business_id='uI_JANZUD9Wr9jDgqTbaaQ', business_review_count=3, count=3),\n",
       " Row(business_id='fGU0Eyoyq8fY787BD1PM4w', business_review_count=3, count=3),\n",
       " Row(business_id='1G7beFPMgIAs4SbBOP3qGA', business_review_count=3, count=3),\n",
       " Row(business_id='Xvjy6opnK7UTy9yRuskcRw', business_review_count=3, count=3),\n",
       " Row(business_id='ePBGDKArVr5YImZu1QP9rg', business_review_count=3, count=3),\n",
       " Row(business_id='EYXI0t6waW4twrAZwCJdsw', business_review_count=3, count=3),\n",
       " Row(business_id='Q5fzK17mAGIj4VC9QlaB1Q', business_review_count=3, count=3),\n",
       " Row(business_id='gMwqeg2s0a8JAC5DqSyM2A', business_review_count=3, count=3),\n",
       " Row(business_id='HmF0AvgKSELlDpyLR-gJNQ', business_review_count=3, count=3),\n",
       " Row(business_id='CH4my_JJ7PJ2T_4uQX4LwQ', business_review_count=3, count=3),\n",
       " Row(business_id='dkP0Zrg_q1x-95sXxg-vQA', business_review_count=3, count=3),\n",
       " Row(business_id='KVHDStrweYbWeEI9vEQ8jg', business_review_count=3, count=3),\n",
       " Row(business_id='bE9soe2Rbs_IZKGH530uTw', business_review_count=3, count=3),\n",
       " Row(business_id='2vF7oK2oV4ZW5hh_sKhpUQ', business_review_count=3, count=3),\n",
       " Row(business_id='2I9D9-1ZSYgQF42Azx1khg', business_review_count=3, count=3),\n",
       " Row(business_id='EB96OunllegghDDRR9NiMA', business_review_count=3, count=3),\n",
       " Row(business_id='pSWrT0wl-zuhC6DdBhfQ9A', business_review_count=3, count=3),\n",
       " Row(business_id='cuuCX5GzqwI2cLsmz8EttA', business_review_count=3, count=3),\n",
       " Row(business_id='GFefOKzqRcUs_HzgpZbdAA', business_review_count=3, count=3),\n",
       " Row(business_id='P6RrFk4uVjIXm6vmM7ifFA', business_review_count=3, count=3),\n",
       " Row(business_id='PRap3K4D3Rxu1O9Y6RrmUQ', business_review_count=3, count=3),\n",
       " Row(business_id='jmGjW4SRfEk9Tgf9GvjZnQ', business_review_count=3, count=3),\n",
       " Row(business_id='z1OhXPMw8CVdfq3BBGkQNQ', business_review_count=3, count=3),\n",
       " Row(business_id='fi30kfHmJr6XPq4T-Yqxjg', business_review_count=3, count=3),\n",
       " Row(business_id='o9PN5VhdfH7RViCBejG5sw', business_review_count=3, count=3),\n",
       " Row(business_id='aSh-9cBwcDu38sQpmc6c1A', business_review_count=3, count=3),\n",
       " Row(business_id='j21uFKk6f6VvvPwbMexTqA', business_review_count=3, count=3),\n",
       " Row(business_id='FLzj03p-thfx4IwgkFPrPA', business_review_count=3, count=3),\n",
       " Row(business_id='9v07yHsbrHzvjhH_UAE6Ww', business_review_count=3, count=3),\n",
       " Row(business_id='j0us8FRA_3MGmTwi8WhwyQ', business_review_count=3, count=3),\n",
       " Row(business_id='pCBKmWciHJkBAos6WD51Vg', business_review_count=3, count=3),\n",
       " Row(business_id='Z5_3PzGwUn8NXHPDZgfqyw', business_review_count=3, count=3),\n",
       " Row(business_id='O5-oPtM3iM6BlEQYx-Ljew', business_review_count=3, count=3),\n",
       " Row(business_id='QlEm8Es0kpEWhh9Ql1gcXw', business_review_count=3, count=3),\n",
       " Row(business_id='00RuEliQR5HBP3m7LpZ4yw', business_review_count=3, count=3),\n",
       " Row(business_id='N7d6SVzwZxCsAVs3yupa5A', business_review_count=3, count=3),\n",
       " Row(business_id='DS3-yphtWDHAdXLtIoqdAA', business_review_count=3, count=3),\n",
       " Row(business_id='V34Ej8-dXN4pI6gacG8Xtw', business_review_count=3, count=3),\n",
       " Row(business_id='--W4kqPWwXFycuqejFANmw', business_review_count=3, count=3),\n",
       " Row(business_id='NYwkawC7I1wEb476NYUr5A', business_review_count=3, count=3),\n",
       " Row(business_id='lSXaWcsBoNVyoOJ51gR0aA', business_review_count=3, count=3),\n",
       " Row(business_id='si4zOcTqHm_i0d-uwIvGjQ', business_review_count=3, count=3),\n",
       " Row(business_id='UYDNch9dPfhhpKNzJ7jikg', business_review_count=3, count=3),\n",
       " Row(business_id='G6l5x_JxwvE9hh1_Hk2Yxw', business_review_count=3, count=3),\n",
       " Row(business_id='uzUm1VihdgpNVd8pQnMbnQ', business_review_count=3, count=3),\n",
       " Row(business_id='a3SsmMOt_9KtDtLD0ubfBw', business_review_count=3, count=3),\n",
       " Row(business_id='dMyfwqK6CuLN-1zXJ__cdw', business_review_count=3, count=3),\n",
       " Row(business_id='1AfQ4DiYPseqXSEYRGqWew', business_review_count=3, count=3),\n",
       " Row(business_id='Olk72fpw3tt7bwFD16eZ9A', business_review_count=3, count=3),\n",
       " Row(business_id='DiZ2rabncBaCrNG3OB-CBA', business_review_count=3, count=3),\n",
       " Row(business_id='vBIugbXbkxNb5tCtck1tSg', business_review_count=3, count=3),\n",
       " Row(business_id='-6e0liTvH5EoB4HuncuQgA', business_review_count=3, count=3),\n",
       " Row(business_id='GrptlHG4Y-3mmUz25RARbQ', business_review_count=3, count=3),\n",
       " Row(business_id='qeBJlOz0-Bm6clrfR3hdOQ', business_review_count=3, count=3),\n",
       " Row(business_id='nfq6pZPg70BFQltyilKN_w', business_review_count=3, count=3),\n",
       " Row(business_id='BUZYzozo0t07YO6XrzjtvQ', business_review_count=3, count=3),\n",
       " Row(business_id='5HC43YJ4Sls422r-W2hkuQ', business_review_count=3, count=3),\n",
       " Row(business_id='VeSxVAhdyYWel3w00SPG8g', business_review_count=3, count=3),\n",
       " Row(business_id='P8qtxHvbn-PGgd8Rv5Nchg', business_review_count=3, count=3),\n",
       " Row(business_id='ubP4xKtsJx2eW2cKAcy9gA', business_review_count=3, count=3),\n",
       " Row(business_id='Vogx8EtS5zlmkcFIzscBYg', business_review_count=3, count=3),\n",
       " Row(business_id='hTNGDKDazrf5bWnMCXa9cA', business_review_count=3, count=3),\n",
       " Row(business_id='zuDz5HQ3-l5eeQdtob43Gw', business_review_count=3, count=3),\n",
       " Row(business_id='8M66zLhCWIqtZOdHU60arQ', business_review_count=3, count=3),\n",
       " Row(business_id='ms8aZyc8EIj1mg62CYFFCA', business_review_count=3, count=3),\n",
       " Row(business_id='o09XwaPavEJJzwiJnUZkYQ', business_review_count=3, count=3),\n",
       " Row(business_id='3PfrzQ0jv2EAOQwyTHGIcA', business_review_count=3, count=3),\n",
       " Row(business_id='ie9zlEaas3eAwGyXwda5HA', business_review_count=3, count=3),\n",
       " Row(business_id='VFbTlmwzpjkPg17LQQ5wEA', business_review_count=3, count=3),\n",
       " Row(business_id='vfWnUk30RI-C5WqENWUJeQ', business_review_count=3, count=3),\n",
       " Row(business_id='SY-tkMJOgHqgbUd1tD-yBQ', business_review_count=3, count=3),\n",
       " Row(business_id='A3enZ6j1HoBzhoaXuNPSeQ', business_review_count=3, count=3),\n",
       " Row(business_id='ugysvQDxFj5AZZYzVxgPDQ', business_review_count=3, count=3),\n",
       " Row(business_id='GSlkylmUYPw02VUQ6vAUGg', business_review_count=3, count=3),\n",
       " Row(business_id='s-tHt7jkILO3i3euUFGVjg', business_review_count=3, count=3),\n",
       " Row(business_id='Jh64txApxd4jz_ZzyXMZ3g', business_review_count=3, count=3),\n",
       " Row(business_id='nDJsRWlUrDfbvUbgZP0BcA', business_review_count=3, count=3),\n",
       " Row(business_id='E1WG9lbypCMKlKhAjzWRsQ', business_review_count=3, count=3),\n",
       " Row(business_id='rRRrEU2LxIHwTX30e0pTPw', business_review_count=3, count=3),\n",
       " Row(business_id='XlhbtfxLzthUy52FwO01Bw', business_review_count=3, count=3),\n",
       " Row(business_id='veECj5v2bnR9NF2NzJ32vQ', business_review_count=3, count=3),\n",
       " Row(business_id='QHksxVx4t6fp6sQxHBR23g', business_review_count=3, count=3),\n",
       " Row(business_id='EKc1TGru3LM19xSyNpWH_Q', business_review_count=3, count=3),\n",
       " Row(business_id='ngDI7XtTpP-E-Ttom3Gq1w', business_review_count=3, count=3),\n",
       " Row(business_id='Je5fSHkENFdmuQL9f2b_xg', business_review_count=3, count=3),\n",
       " Row(business_id='rSwGkWTJkKfwjfOWHYr3mA', business_review_count=3, count=3),\n",
       " Row(business_id='rink3RE3A9je8j1MXgXd5g', business_review_count=3, count=3),\n",
       " Row(business_id='wrZOLGBk5kLs4mnEijSEZw', business_review_count=3, count=3),\n",
       " Row(business_id='9T9bwdOdIqdQcIYc8i-8fA', business_review_count=3, count=3),\n",
       " Row(business_id='CxYg_k5plp_QHSAXhPYkLg', business_review_count=3, count=3),\n",
       " Row(business_id='elLfsO2mwKqMGQ0WNrc4KQ', business_review_count=3, count=3),\n",
       " Row(business_id='8eHUD8OBe2nXaLKV68K0Rw', business_review_count=3, count=3),\n",
       " Row(business_id='V0G7iGxA_SK6HmN8suyCoA', business_review_count=3, count=3),\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate maximum number of \"reviews\" per venue for the whole dataset\n",
    "review_business.groupBy(review_business.business_id, review_business.business_review_count).count().orderBy('count').collect()\n",
    "#caused spark to crash the first time- was it this cell, or another? spoopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1035, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:42378)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/sql/dataframe.py\", line 438, in collect\n",
      "    port = self._jdf.collectToPython()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 327, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o62.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:42378)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/sql/dataframe.py\", line 438, in collect\n",
      "    port = self._jdf.collectToPython()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 327, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o62.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serverteam_1/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1215, in time\n",
      "    exec(code, glob, local_ns)\n",
      "  File \"<timed exec>\", line 5, in <module>\n",
      "  File \"/usr/local/spark/python/pyspark/sql/dataframe.py\", line 1703, in toPandas\n",
      "    return pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/dataframe.py\", line 438, in collect\n",
      "    port = self._jdf.collectToPython()\n",
      "  File \"/usr/local/spark/python/pyspark/traceback_utils.py\", line 78, in __exit__\n",
      "    self._context._jsc.setCallSite(None)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:42378)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serverteam_1/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:42378)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o62.collectToPython",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \"\"\"\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    879\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:42378)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark import StorageLevel\n",
    "review_business.persist(StorageLevel.MEMORY_ONLY)\n",
    "import time\n",
    "start_time = time.time()\n",
    "_review_business = review_business.toPandas()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#first error was overhead\n",
    "#running on the server: spark.driver.maxResultSize error, changed temporarily below\n",
    "#caused spark to crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error msg from Terminal\n",
    "#### Exception in thread \"refresh progress\" Exception in thread \"dispatcher-event-loop-3\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
    "\tat org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:69)\n",
    "\tat org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55)\n",
    "\tat java.util.TimerThread.mainLoop(Timer.java:555)\n",
    "\tat java.util.TimerThread.run(Timer.java:505)\n",
    "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
    "\tat java.util.concurrent.Executors$DefaultThreadFactory.newThread(Executors.java:612)\n",
    "\tat org.spark_project.guava.util.concurrent.ThreadFactoryBuilder$1.newThread(ThreadFactoryBuilder.java:162)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
    "\tat java.lang.Thread.run(Thread.java:748)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving big data from RDD to local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abuse RDD.Collect() \n",
    "Collect (Action) - Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o75.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-644bbad1298f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'review_business.collect()    '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o75.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review_business.collect()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error before imcreasing driver maxResultSize \n",
    "####  size serialized results (1035.2 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n",
    "Problem stems from RDD sends/collects (big chunk of) all the data (from different nodes) to the driver node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy temp fix is changing driver memory in spark instance. \n",
    "change spark.driver.maxResultSize by \n",
    "1. start pyspark w/ --conf spark.driver.maxResultSize=2g or\n",
    "2. see next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## In Jupyter you have to stop the current context first\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o429.sessionState.\n: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1053)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:129)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:126)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:193)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1050)\n\t... 16 more\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\t... 25 more\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\t... 39 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\t... 45 more\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:193)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1050)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:129)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:126)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 107 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /home/serverteam_1/data/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 104 more\n------\r\n\nNestedThrowables:\njava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:193)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1050)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:129)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:126)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 107 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /home/serverteam_1/data/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 104 more\n------\r\n\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\t... 50 more\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:193)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1050)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:130)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:129)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:126)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 107 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /home/serverteam_1/data/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 104 more\n------\r\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\t... 79 more\nCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\t... 91 more\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2a3a9430, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 107 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /home/serverteam_1/data/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 104 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-938d25a024e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## restart w/ driver.maxResultSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yarn\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.instances\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"70\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.memory\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"4g\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.memory\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"30g\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.cores\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.scheduler.mode\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"FIFO\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.maxResultSize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2g\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#java.lang.IllegalArgumentException: Error while instantiating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#'org.apache.spark.sql.hive.HiveSessionStateBuilder': (looks like a permissions conflict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#looks like we can't configure from jupyter notebook on server, restarting from terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: \"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\""
     ]
    }
   ],
   "source": [
    "## restart w/ driver.maxResultSize\n",
    "#spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .master(\"yarn\") \\\n",
    "#         .appName(\"testing\") \\\n",
    "#         .config(\"spark.executor.instances\", \"70\") \\\n",
    "#         .config(\"spark.executor.memory\",\"4g\") \\\n",
    "#         .config(\"spark.driver.memory\",\"30g\") \\\n",
    "#         .config(\"spark.executor.cores\",'1') \\\n",
    "#         .config(\"spark.scheduler.mode\",\"FIFO\") \\\n",
    "#         .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "#         .getOrCreate()\n",
    "#java.lang.IllegalArgumentException: Error while instantiating \n",
    "#'org.apache.spark.sql.hive.HiveSessionStateBuilder': (looks like a permissions conflict)\n",
    "#looks like we can't configure from jupyter notebook on server, restarting from terminal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ok well.. encountered some error while re-importing SparkSeesion even tho already \"del spark\" ... \n",
    "### will restart pyspark and specific that in terminal\n",
    "\n",
    "pyspark --master yarn --deploy-mode client --num-executors 4 --executor-memory 2g --driver-memory 4g --conf spark.driver.maxResultSize=3g\n",
    "\n",
    "Error message fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 14.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# review_business.collect()     <<already executed above after we restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error after increasing driver maxResultSize:\n",
    "Py4JJavaError: An error occurred while calling o75.collectToPython.\n",
    ": java.lang.OutOfMemoryError: GC overhead limit exceeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alternatives?\n",
    "1. review_business.rdd      # and do all the magic stuff on the cluster, not in the driver\n",
    "2. Use spark.rdd.compress to compress RDDs when you collect them        << configuration\n",
    "3. Try to collect it using pagination. \n",
    "\n",
    "see https://stackoverflow.com/questions/31058504/spark-1-4-increase-maxresultsize-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recall RDD\n",
    "\n",
    "Collection of data partitions called RDD. These RDD must follow few properties:\n",
    "\n",
    "- Immutable,\n",
    "- Fault Tolerant,\n",
    "- Distributed,\n",
    "- +...\n",
    "\n",
    "Here RDD is either structured or unstructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_rb = review_business.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(yelp_rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2014)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-dec58e69a88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myelp_rb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2014)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "yelp_rb.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error this time, after running review_business.collect(), which was executed after we have increased the maxResultSize() --> \n",
    "\"Py4JJavaError: An error occurred while calling o75.collectToPython. : java.lang.OutOfMemoryError: GC overhead limit exceeded\"\n",
    "### Error following last cell: java.lang.IllegalStateException: SparkContext has been shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.144.40.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1052c71d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /Users/admin/anaconda/lib/python2.7/site-packages/IPython/utils/py3compat.py:288 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-76dd08d2c718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /Users/admin/anaconda/lib/python2.7/site-packages/IPython/utils/py3compat.py:288 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great.. can't even start it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method stop() must be called with SparkContext instance as first argument (got SparkSession instance instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2c20bec95eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# spark.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unbound method stop() must be called with SparkContext instance as first argument (got SparkSession instance instead)"
     ]
    }
   ],
   "source": [
    "SparkContext.stop(spark)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### also can't stop.... will restart pyspark.... \n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after restart pyspark .............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 ms, sys: 5.49 ms, total: 25.6 ms\n",
      "Wall time: 50.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.memory', u'2g'),\n",
       " (u'spark.executor.instances', u'4'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'http://dhcp-ccc-10261.redrover.cornell.edu:8088/proxy/application_1509845828116_0007'),\n",
       " (u'spark.driver.memory', u'4g'),\n",
       " (u'spark.driver.port', u'58591'),\n",
       " (u'spark.driver.host', u'10.144.40.21'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'spark.ui.proxyBase', u'/proxy/application_1509845828116_0007'),\n",
       " (u'spark.app.id', u'application_1509845828116_0007'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.driver.appUIAddress', u'http://10.144.40.21:4040'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip:/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/:/Users/admin/gildas-exe-sep15a/x86_64-darwin-gfortran/python:/Users/admin/anaconda/lib/python2.7/site-packages:/Users/admin/DNest3:/Users/admin/Lensingmodel/uvmcmcfit:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.4-src.zip'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'dhcp-ccc-10261.redrover.cornell.edu'),\n",
       " (u'spark.driver.maxResultSize', u'3g'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 ms, sys: 13.2 ms, total: 31.4 ms\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(business_id=u'--9e1ONYQuAa-CB_Rrw7Tw', cool=0, date=u'2012-03-29', funny=0, review_id=u'OJTjgyV6HYRPo7ZO1KlRsQ', business_avg_stars=5, text=u'For me the bar has been raised! \\n\\nI thought the Conch Republic in Key West was my best meal ever. It has been topped. The best Ribeye I have ever eaten. Everything about this place was top shelf. I had the escargot, Ribeye, loaded baked potato, and finished it off with the best pecan pie ever.', useful=0, user_id=u'poUDFITGDHUoaiAvwFzXEw', address=u'3355 Las Vegas Blvd S', business_type=Row(AcceptsInsurance=None, AgesAllowed=None, Alcohol=u'full_bar', Ambience=Row(casual=False, classy=True, divey=False, hipster=False, intimate=False, romantic=False, touristy=False, trendy=False, upscale=True), BYOB=False, BYOBCorkage=u'yes_corkage', BestNights=None, BikeParking=False, BusinessAcceptsBitcoin=None, BusinessAcceptsCreditCards=True, BusinessParking=Row(garage=True, lot=False, street=False, valet=True, validated=False), ByAppointmentOnly=None, Caters=False, CoatCheck=None, Corkage=True, DietaryRestrictions=None, DogsAllowed=None, DriveThru=None, GoodForDancing=None, GoodForKids=False, GoodForMeal=Row(breakfast=False, brunch=False, dessert=False, dinner=True, latenight=False, lunch=False), HairSpecializesIn=None, HappyHour=None, HasTV=False, Music=None, NoiseLevel=u'average', Open24Hours=False, OutdoorSeating=False, RestaurantsAttire=u'dressy', RestaurantsCounterService=None, RestaurantsDelivery=False, RestaurantsGoodForGroups=True, RestaurantsPriceRange2=4, RestaurantsReservations=True, RestaurantsTableService=True, RestaurantsTakeOut=False, Smoking=None, WheelchairAccessible=True, WiFi=u'no'), categories=[u'Steakhouses', u'Cajun/Creole', u'Restaurants'], city=u'Las Vegas', hours=Row(Friday=u'11:30-14:00', Monday=u'11:30-14:00', Saturday=u'11:30-14:00', Sunday=u'11:30-14:00', Thursday=u'11:30-14:00', Tuesday=u'11:30-14:00', Wednesday=u'11:30-14:00'), is_open=1, latitude=36.123183, longitude=-115.16919, name=u'Delmonico Steakhouse', neighborhood=u'The Strip', postal_code=u'89109', business_review_count=1389, state=u'NV')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "yelp_rb.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id=u'--9e1ONYQuAa-CB_Rrw7Tw', cool=0, date=u'2012-03-29', funny=0, review_id=u'OJTjgyV6HYRPo7ZO1KlRsQ', business_avg_stars=5, text=u'For me the bar has been raised! \\n\\nI thought the Conch Republic in Key West was my best meal ever. It has been topped. The best Ribeye I have ever eaten. Everything about this place was top shelf. I had the escargot, Ribeye, loaded baked potato, and finished it off with the best pecan pie ever.', useful=0, user_id=u'poUDFITGDHUoaiAvwFzXEw', address=u'3355 Las Vegas Blvd S', business_type=Row(AcceptsInsurance=None, AgesAllowed=None, Alcohol=u'full_bar', Ambience=Row(casual=False, classy=True, divey=False, hipster=False, intimate=False, romantic=False, touristy=False, trendy=False, upscale=True), BYOB=False, BYOBCorkage=u'yes_corkage', BestNights=None, BikeParking=False, BusinessAcceptsBitcoin=None, BusinessAcceptsCreditCards=True, BusinessParking=Row(garage=True, lot=False, street=False, valet=True, validated=False), ByAppointmentOnly=None, Caters=False, CoatCheck=None, Corkage=True, DietaryRestrictions=None, DogsAllowed=None, DriveThru=None, GoodForDancing=None, GoodForKids=False, GoodForMeal=Row(breakfast=False, brunch=False, dessert=False, dinner=True, latenight=False, lunch=False), HairSpecializesIn=None, HappyHour=None, HasTV=False, Music=None, NoiseLevel=u'average', Open24Hours=False, OutdoorSeating=False, RestaurantsAttire=u'dressy', RestaurantsCounterService=None, RestaurantsDelivery=False, RestaurantsGoodForGroups=True, RestaurantsPriceRange2=4, RestaurantsReservations=True, RestaurantsTableService=True, RestaurantsTakeOut=False, Smoking=None, WheelchairAccessible=True, WiFi=u'no'), categories=[u'Steakhouses', u'Cajun/Creole', u'Restaurants'], city=u'Las Vegas', hours=Row(Friday=u'11:30-14:00', Monday=u'11:30-14:00', Saturday=u'11:30-14:00', Sunday=u'11:30-14:00', Thursday=u'11:30-14:00', Tuesday=u'11:30-14:00', Wednesday=u'11:30-14:00'), is_open=1, latitude=36.123183, longitude=-115.16919, name=u'Delmonico Steakhouse', neighborhood=u'The Strip', postal_code=u'89109', business_review_count=1389, state=u'NV'),\n",
       " Row(business_id=u'--9e1ONYQuAa-CB_Rrw7Tw', cool=0, date=u'2013-12-30', funny=0, review_id=u'DEuf8OY5hQX-xugpb88nlg', business_avg_stars=5, text=u'I am in love. In love with the ceasar salad prepared at our table by our very likable  waiter, in love with the steak medallions topped with lobster and drizzled with a wonderful bearnaise sauce... in fact I would be hard pressed to find anything to criticize about the experience we had here last week. Finally, the banana cream pie was nothing short of the totally perfect end to our evening. Fine wine, fine company, fine service and more than fine food. Dare I say it one more time??? In LOVE haha', useful=0, user_id=u'-XzAzGKAGCLn-sxy0_3dDw', address=u'3355 Las Vegas Blvd S', business_type=Row(AcceptsInsurance=None, AgesAllowed=None, Alcohol=u'full_bar', Ambience=Row(casual=False, classy=True, divey=False, hipster=False, intimate=False, romantic=False, touristy=False, trendy=False, upscale=True), BYOB=False, BYOBCorkage=u'yes_corkage', BestNights=None, BikeParking=False, BusinessAcceptsBitcoin=None, BusinessAcceptsCreditCards=True, BusinessParking=Row(garage=True, lot=False, street=False, valet=True, validated=False), ByAppointmentOnly=None, Caters=False, CoatCheck=None, Corkage=True, DietaryRestrictions=None, DogsAllowed=None, DriveThru=None, GoodForDancing=None, GoodForKids=False, GoodForMeal=Row(breakfast=False, brunch=False, dessert=False, dinner=True, latenight=False, lunch=False), HairSpecializesIn=None, HappyHour=None, HasTV=False, Music=None, NoiseLevel=u'average', Open24Hours=False, OutdoorSeating=False, RestaurantsAttire=u'dressy', RestaurantsCounterService=None, RestaurantsDelivery=False, RestaurantsGoodForGroups=True, RestaurantsPriceRange2=4, RestaurantsReservations=True, RestaurantsTableService=True, RestaurantsTakeOut=False, Smoking=None, WheelchairAccessible=True, WiFi=u'no'), categories=[u'Steakhouses', u'Cajun/Creole', u'Restaurants'], city=u'Las Vegas', hours=Row(Friday=u'11:30-14:00', Monday=u'11:30-14:00', Saturday=u'11:30-14:00', Sunday=u'11:30-14:00', Thursday=u'11:30-14:00', Tuesday=u'11:30-14:00', Wednesday=u'11:30-14:00'), is_open=1, latitude=36.123183, longitude=-115.16919, name=u'Delmonico Steakhouse', neighborhood=u'The Strip', postal_code=u'89109', business_review_count=1389, state=u'NV')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_rb.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 579, slave-2, executor 25): java.io.IOException: Cannot run program \"/home/serverteam_1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Cannot run program \"/home/serverteam_1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 579, slave-2, executor 25): java.io.IOException: Cannot run program \"/home/serverteam_1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Cannot run program \"/home/serverteam_1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "yelp_rb.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## don't seem to have a straight way out, can only do:\n",
    "- buy more computing power\n",
    "- filter the data to smaller size before doing actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge, join, union transformation\n",
    "\n",
    "e.g., Union()\n",
    "- Union is basically used to merge two RDDs together if they have the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Union can only be performed on tables with the same number of columns, but the first table has 9 columns and the second table has 15 columns;;\\n'Union\\n:- Repartition 150, true\\n:  +- Relation[business_id#86,cool#87L,date#88,funny#89L,review_id#90,stars#91L,text#92,useful#93L,user_id#94] json\\n+- Repartition 150, true\\n   +- Relation[address#46,attributes#47,business_id#48,categories#49,city#50,hours#51,is_open#52L,latitude#53,longitude#54,name#55,neighborhood#56,postal_code#57,review_count#58L,stars#59,state#60] json\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d8d94761b94e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'review.union(bus).collect()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \"\"\"\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Union can only be performed on tables with the same number of columns, but the first table has 9 columns and the second table has 15 columns;;\\n'Union\\n:- Repartition 150, true\\n:  +- Relation[business_id#86,cool#87L,date#88,funny#89L,review_id#90,stars#91L,text#92,useful#93L,user_id#94] json\\n+- Repartition 150, true\\n   +- Relation[address#46,attributes#47,business_id#48,categories#49,city#50,hours#51,is_open#52L,latitude#53,longitude#54,name#55,neighborhood#56,postal_code#57,review_count#58L,stars#59,state#60] json\\n\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review.union(bus).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glom - return an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "http://parrotprediction.com/partitioning-in-apache-spark/\n",
    "\n",
    "Spark uses different partitioning schemes for various types of RDDs and operations. In a case of using parallelize(), data is evenly distributed between partitions using their indices (no partitioning scheme is used). << how could I get sc.parallelize() with my current setup tho...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(yelp_rb.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioner: None\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 106 tasks (3.0 GB) is bigger than spark.driver.maxResultSize (3.0 GB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d2723a5bdf55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Partitioner: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_rb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Partitions structure: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_rb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 106 tasks (3.0 GB) is bigger than spark.driver.maxResultSize (3.0 GB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitioner: {}\".format(yelp_rb.partitioner))\n",
    "print(\"Partitions structure: {}\".format(yelp_rb.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### again .... error \n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 106 tasks (3.0 GB) is bigger than spark.driver.maxResultSize (3.0 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
      "CPU times: user 20 ms, sys: 9.7 ms, total: 29.7 ms\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# in the following, let's do simple data instead of hauling the big yelp data\n",
    "nums = range(0, 10)\n",
    "\n",
    "with SparkContext(\"local\") as sc:\n",
    "    rdd = sc.parallelize(nums)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism: 2\n",
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n",
      "CPU times: user 16.3 ms, sys: 6.02 ms, total: 22.3 ms\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# allow driver to use two local cores.\n",
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(nums)\n",
    "    \n",
    "    print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 15\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]\n",
      "CPU times: user 20.9 ms, sys: 7.46 ms, total: 28.3 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# what happens if number of partitions exceeds number of data records?\n",
    "with SparkContext(\"local\") as sc:\n",
    "    rdd = sc.parallelize(nums, 15)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Spark created requested a number of partitions but most of them are empty. This is bad because the time needed to prepare a new thread for processing data (one element) is significantly greater than processing time itself (you can analyze it in Spark UI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom partitions with partitionBy()\n",
    "\n",
    "partitionBy() *transformation* allows applying custom partitioning logic over the RDD, requires data to be in key/value format we will need to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x116ac4590>\n",
      "Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\n"
     ]
    }
   ],
   "source": [
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(nums) \\     # put into RDD w/o partitioning schmee\n",
    "        .map(lambda el: (el, el)) \\ \n",
    "        .partitionBy(2) \\           # split into 2 chunks using the default hash partitioner\n",
    "        .persist()                  # persist storage level, default = memory\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_rb.persist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: [0]: 0 % 2 = partition 0\n",
      "Element: [1]: 1 % 2 = partition 1\n",
      "Element: [2]: 2 % 2 = partition 0\n",
      "Element: [3]: 3 % 2 = partition 1\n",
      "Element: [4]: 4 % 2 = partition 0\n",
      "Element: [5]: 5 % 2 = partition 1\n",
      "Element: [6]: 6 % 2 = partition 0\n",
      "Element: [7]: 7 % 2 = partition 1\n",
      "Element: [8]: 8 % 2 = partition 0\n",
      "Element: [9]: 9 % 2 = partition 1\n",
      "CPU times: user 236 µs, sys: 53 µs, total: 289 µs\n",
      "Wall time: 249 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# santity\n",
    "from pyspark.rdd import portable_hash\n",
    " \n",
    "num_partitions = 2\n",
    " \n",
    "for el in nums:\n",
    "    # partition number is evaluated using: partition = partitionFunc(key) % num_partitions.\n",
    "    # default partition func in pyspark = hash\n",
    "    print(\"Element: [{}]: {} % {} = partition {}\".format(\n",
    "        el, portable_hash(el), num_partitions, portable_hash(el) % num_partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### slightly more realistic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data consist of various dummy transactions made across different countries.\n",
    "\n",
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To optimize network traffic for further analysis, put records from one country in one node.\n",
    "Will use custom_partitioner(), which returns an integer for given object (tuple key).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f4c79c669619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_partitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Poland\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Germany\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"United Kingdom\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-f4c79c669619>\u001b[0m in \u001b[0;36mcountry_partitioner\u001b[0;34m(country)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ensure that data for each country is in one partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcountry_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Validate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/functions.pyc\u001b[0m in \u001b[0;36mhash\u001b[0;34m(*cols)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \"\"\"\n\u001b[1;32m   1250\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
     ]
    }
   ],
   "source": [
    "# ensure that data for each country is in one partition\n",
    "def country_partitioner(country):\n",
    "    return hash(country)\n",
    " \n",
    "# Validate results to see what partitions are assigned for each country.\n",
    "num_partitions = 5\n",
    " \n",
    "print(country_partitioner(\"Poland\") % num_partitions)\n",
    "print(country_partitioner(\"Germany\") % num_partitions)\n",
    "print(country_partitioner(\"United Kingdom\") % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x116cbf810>\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.py\", line 1753, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))\n  File \"<ipython-input-38-f4c79c669619>\", line 3, in country_partitioner\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1251, in hash\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.py\", line 1753, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))\n  File \"<ipython-input-38-f4c79c669619>\", line 3, in country_partitioner\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1251, in hash\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8feded77fc63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of partitions: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Partitioner: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Partitions structure: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.py\", line 1753, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))\n  File \"<ipython-input-38-f4c79c669619>\", line 3, in country_partitioner\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1251, in hash\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/rdd.py\", line 1753, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))\n  File \"<ipython-input-38-f4c79c669619>\", line 3, in country_partitioner\n  File \"/Users/admin/anaconda/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1251, in hash\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with SparkContext(\"local[2]\") as sc:\n",
    "    rdd = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(4, country_partitioner)\n",
    "    \n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some errors to look into later.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------------------------------------------------\n",
    " *** side question *** \n",
    " \n",
    "While running stuff in notebook, the following appears in the terminal:   \n",
    "[W 22:25:09.065 NotebookApp] Timeout waiting for kernel_info reply from 5e476d4b-3c33-4eee-839b-e3173fda7970\n",
    "\n",
    "17/11/04 22:25:39 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
    "\n",
    "[Stage 6:=================>                                      (47 + 2) / 150][I 22:26:45.828 NotebookApp] Saving file at /CostlyFunctions.ipynb\n",
    "\n",
    "[Stage 9:>                                                          (0 + 2) / 2][I 22:27:00.840 NotebookApp] Saving file at /CostlyFunctions.ipynb\n",
    "\n",
    "17/11/04 22:27:03 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
    "\n",
    "**what are the stages???? and what do they mean??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
