{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.feature import Imputer\n",
    "import time\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = spark.read.csv('/test.csv', inferSchema=True, header=True)\n",
    "dfbig= spark.read.json('/review.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Persist vs. cache](https://stackoverflow.com/questions/26870537/what-is-the-difference-between-cache-and-persist)\n",
    "- Persist: can store in other areas (ie. disk) \n",
    "    - On-heap: subject to garbage collection\n",
    "    - Off-heap: serialized data not subject to garbage collection\n",
    "        Slightly slower than memory-based (have to be deserialized still)\n",
    "        Still faster than disk-based\n",
    "        Don’t waste GC’s time\n",
    "        Java and Scala only!\n",
    "- Cache: store in memory only\n",
    "- Storage Levels: \n",
    "    - MEMORY_ONLY: \n",
    "        - Pro: Definitely the fastest, if you have the resources availabe\n",
    "        - Con: Based on the eviction policy, could use frames if you start running out of memroy\n",
    "    - MEMORY_AND_DISK:\n",
    "        - Pro: Best if you're going to run out of memory\n",
    "        - Pro: Still faster than re-evaluating the RDD\n",
    "        - Con: Slower than if you could just cache to memory\n",
    "    - Appending a number (ie. DISK_ONLY_2) adds that number of replicas to other nodes\n",
    "        - Pro: Keeps speed up if there's a node failure\n",
    "        - Con: Sucks up extra memory\n",
    "    - Appending SER (ie MEMORY_ONLY_SER) will serialize the data --> only in Java/Scala\n",
    "        - Pro: Relieves some strain on memory\n",
    "        - Con: Instead increases processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.456082105636597\n"
     ]
    }
   ],
   "source": [
    "#first try without caching --> 17 seconds\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "dur1 = time.time() - start_time\n",
    "print(dur1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting vs. caching: \n",
    "- persist(MEMORY_ONLY) and cache() should be the same\n",
    "- That being said, cacheing the dataframe to memory at best takes a minute longer, and usually just causes the program to crash. \n",
    "\n",
    "Times: \n",
    "- cache: 117\n",
    "    1. 83.89\n",
    "- memory_only: 73.37\n",
    "    1. 42.1\n",
    "- memory_and_disk: 42.47\n",
    "    1. 38.50\n",
    "- memory and disk serialized: n/a\n",
    "    1. 88.7\n",
    "- disk only: 44.11\n",
    "    1. 38.15\n",
    "- memory serialized: 41.84\n",
    "    1. 36.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \"spark.sql.inMemoryColumnarStorage.compressed\" to True in spark-defaults.conf. \n",
    "    - This enables Spark to automatically choose the optimal compression codec for the data. \n",
    "    - Moral of the story: have this enabled! Easy boost \n",
    "2. Tuning \"spark.sql.inMemoryColumnarStorage.batchSize\"\n",
    "    - \"Larger batch sizes may improve utilization but also result in out of memory errors\" [source](https://qubole.zendesk.com/hc/en-us/articles/216920846-How-To-Spark-SQL-Tuning)\n",
    "    - Probably best to tune once have acccess to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.8015570640564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: bigint, text: string, useful: bigint, user_id: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try caching the dataframe, then run the groupBy --> 111 seconds\n",
    "dfbig.cache()\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "cache_time = time.time() - start_time\n",
    "print(cache_time)\n",
    "dfbig.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.10225009918213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: bigint, text: string, useful: bigint, user_id: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try persisting the dataframe to memory, then run the groupBy --> 111 seconds\n",
    "dfbig.persist(StorageLevel.MEMORY_ONLY)\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "mem_time = time.time() - start_time\n",
    "print(mem_time)\n",
    "dfbig.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting at different storage levels\n",
    "Memory only and serialized seems to work best, and doesn't cause any crashes, unlike directly storing it in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.492923974990845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: bigint, text: string, useful: bigint, user_id: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#persist to disk as opposed to memory\n",
    "dfbig.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "mem_disk_time = time.time()-start_time\n",
    "print(mem_disk_time)\n",
    "dfbig.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.67262363433838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: bigint, text: string, useful: bigint, user_id: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#persist to disk as opposed to memory\n",
    "#be warned! another crash-inducing storage level\n",
    "dfbig.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "mem_disk_ser_time = time.time()-start_time\n",
    "print(mem_disk_ser_time)\n",
    "dfbig.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.15035128593445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: bigint, text: string, useful: bigint, user_id: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbig.persist(StorageLevel.DISK_ONLY)\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "disk_time = time.time()-start_time\n",
    "print(disk_time)\n",
    "dfbig.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.89974522590637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: bigint, text: string, useful: bigint, user_id: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbig.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "start_time = time.time()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()\n",
    "mem_ser_time = time.time()-start_time\n",
    "print(mem_ser_time)\n",
    "dfbig.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [When to cache/persist](https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd)\t\n",
    "- Nothing happens to data until actually perform an operation on it\n",
    "    - Only references\n",
    "    - Both RDDs and Dataframes are computed lazily\n",
    "- Linear situation: ie load file into rdd, perform a basic transformation, then count\n",
    "    - Cache not needed\n",
    "    - Data loaded to executors, transform, and count computed all in memory\n",
    "- Non-linear: ie. load file, want to create a filtered dataset to work with\n",
    "    - Cache before begin to branch\n",
    "- Rule of thumb: branching out\n",
    "    - Iterating (so very important in any ML)\n",
    "    - Reusing the same dataframe a lot in the program\n",
    "    - Cost to generate the dataframe is high (ie applied a complex map function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Serializers\n",
    "- Best practice to cache serialized objects\n",
    "- Pickle Serializer \n",
    "    - More universal\n",
    "- Marshall Serializer\n",
    "    - Faster\n",
    "- [TBD whether either of these actually offer an advantage over Kyros](https://stackoverflow.com/questions/36278574/do-you-benefit-from-the-kryo-serializer-when-you-use-pyspark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = dfbig.rdd\n",
    "rdd.cache()\n",
    "start_time = time.time()\n",
    "rdd.count()\n",
    "dur5 = time.time()-start_time #38 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Other Resources Used: \n",
    "- https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\n",
    "- https://unraveldata.com/to-cache-or-not-to-cache/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
