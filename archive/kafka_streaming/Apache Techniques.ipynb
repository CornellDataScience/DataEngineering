{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the start of a compilation of techniques to tune spark code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Helpful Resources:\n",
    "\n",
    "Tips and Tricks: https://gist.github.com/dusenberrymw/30cebf98263fae206ea0ffd2cb155813\n",
    "    \n",
    "Cloudera Tuning Part 1:\n",
    "https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/\n",
    "   \n",
    "Cloudera Tuning Part 2:\n",
    "https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\n",
    "\n",
    "Spark Documentation: https://spark.apache.org/docs/latest/tuning.html\n",
    "\n",
    "Really Good Series of Courses: https://www.coursera.org/specializations/scala\n",
    "\n",
    "This is super good: https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Serialization\n",
    "\n",
    "Spark's default serializer is the Java serializer which has been known to be innefficient, consuming large percentage of CPU and RAM. The Kyro Serializer is the better option as it allows flexibility to register custom classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-2f2b0abaa206>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-2f2b0abaa206>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    val conf = new SparkConf()\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Changing the serializer to kyro in the spark conf\n",
    "val conf = new SparkConf()\n",
    "      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "#Controlling size of kyro buffer\n",
    "val conf = new SparkConf()\n",
    "      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .set(\"spark.kryoserializer.buffer.mb\",\"24\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garbage Collection\n",
    "\n",
    "Use Data Structures with fewer objects\n",
    "\n",
    "Persist object in serialized form\n",
    "\n",
    "Clean up cached collections after they are no longer being used\n",
    "\n",
    "https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-6b7845ba6d61>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-6b7845ba6d61>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -verbose:gc - XX:+PrintGCDetails\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Measuring Java GC\n",
    "-verbose:gc - XX:+PrintGCDetails\n",
    "XX:+PrintGCTimeStamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Management\n",
    "\n",
    "Use spark.memory.fraction to set percentage of heap space used\n",
    "\n",
    "Don't use executors with too much memory (it delays garbace collection)\n",
    "\n",
    "Don't use lots of small executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-ef47ccedbacd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-ef47ccedbacd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -XX:+UseCompressedOops\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#set pointers to 4 rather than 8 bytes\n",
    "-XX:+UseCompressedOops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelism\n",
    "\n",
    "Recomended 2-3 tasks per core\n",
    "\n",
    "** Optimize the number of partitions (General Rule 128Mb per partition) **\n",
    "Coursera Course Goes into Great Detail!!!\n",
    "\n",
    "Types of partitioning: https://acadgild.com/blog/partitioning-in-spark/\n",
    "\n",
    "Hash Partitioning\n",
    "\n",
    "Range Partitioning\n",
    "\n",
    "*Partitioning improves operations that shuffle by key*\n",
    "\n",
    "repartition() is expensive as it moves data around, use coalesce() instead to decrease the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations\n",
    "\n",
    "Maximize usage of transfomrations such as map() and filter()\n",
    "\n",
    "Minimize usage of wide transformations like: groupByKey(), reduceByKey(), join()\n",
    "\n",
    "Use cogroup() to join grouped datasets\n",
    "\n",
    "Use repartitionAndSortWithinPartitions()\n",
    "\n",
    "https://stackoverflow.com/questions/37227286/how-to-use-sparks-repartitionandsortwithinpartitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching\n",
    "\n",
    "Cache collections that are to be used multiple times\n",
    "\n",
    "Cache serialized objects intstead of raw data, as it improves garbace collection time\n",
    "\n",
    "\n",
    "Excellent Resources:\n",
    "\n",
    "\n",
    "http://sujee.net/2015/01/22/understanding-spark-caching/#.WfYNNsaZMy4\n",
    "\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with an *OutofMemoryError* try to increase parallelism to more than the number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
