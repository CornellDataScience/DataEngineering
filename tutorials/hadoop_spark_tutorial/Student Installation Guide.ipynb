{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda is a prepackaged Python Ecosystem geared towards Data Science. Anaconda and it's supporting products are supplid by Continuum Analytcis. Anaconda comes pre-built with a wide variety of packages; a full list can be found [here](docs.continuum.io/anaconda/packages/pkg-docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation of Anaconda is fairly simple. First, download the install package supplied by Continuum Analytics and give it execute privileges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/Downloads\n",
    "wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh\n",
    "chmod 733 Anaconda*.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the provided install script; this will walk you through the installation and configuration of Anaconda (for configuration, the defaults will usually be fine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./Anaconda*.sh\n",
    "rm Anaconda*.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environments and Package Installation with Conda and Pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda comes with not one, but two seperate package managers: the traditional python package manager pip, and it's own package manager, Conda. Both can be used to install additional python packages from their respective repositories.\n",
    " \n",
    "It is often quite usefull to be able to manage seperate installations of python/python packages, for example when different versions of the same package are required by different projects. This is accomplished through virtual environments, or seperate, selectable installations of python and it's package. These can be created and managed using conda, or using pip and the virtualenv package.\n",
    "\n",
    "The following cheatsheet should be a suffecient primer on how to use Conda and Pip for package installation and virtual environment management.\n",
    "\n",
    "https://conda.io/docs/_downloads/conda-cheatsheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, before we begin, open your console and run the command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo apt install ssh openssh-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java\n",
    "\n",
    "For hadoop to work properly, you first need to install java. To do so, run the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# First, we want to add the \n",
    "# repository that holds Oracle's Java\n",
    "sudo add-apt-repository ppa:webupd8team/java\n",
    "sudo apt update\n",
    "sudo apt install oracle-java8-installer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the default environment variables and set oracle-java8 as your default java version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt install oracle-java8-set-default\n",
    "sudo update-alternatives --config java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a framework for a distributed filesystem which allows users to store large data sets accross multiple clusters, while maintaining integrity in the face of failure. It includes HDFS which provides access to application data, and YARN, which is a framework for job scheduling and resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by creating a hadoop group, and add your existing user to it. Groups your user belongs to will grant you certain permissions for files belonging to that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo addgroup hadoop \n",
    "sudo usermod -a -G hadoop $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, generate SSH keys to be used for verification with the system. Copy this key to your user on localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh-keygen; ssh-copy-id $USER@localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that setup is done, it's time to actually install Hadoop. First and move into a directory named CDS, located in your home directory. Then, use wget to acquire the tar archive for hadoop. Extract hadoop from the archive and remove the archive. \n",
    "\n",
    "This hadoop directory will have a rather cumbersome name. Create a symbolic link to it, simply named hadoop, in CDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /usr/local\n",
    "sudo wget http://mirror.reverse.net/pub/apache/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz\n",
    "sudo tar xzf *.gz\n",
    "sudo rm *.gz\n",
    "sudo chown -R $USER:hadoop hadoop*\n",
    "sudo ln -s hadoop* hadoop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDS supplies a number of small files of configuration changes to make to your Hadoop and Spark setups. Download the Hadoop Configuration Files that were posted on Slack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd hadoop\n",
    "mv ~/Downloads/CDS-Hadoop-config-files.tar.gz /usr/local/hadoop\n",
    "tar xzf CDS-Hadoop-config-files.tar.gz\n",
    "rm CDS-Hadoop-config-files.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, append the Hadoop-bashrc-snippet file to your bashrc file. .bash rc is a configuration file used by your bash shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Hadoop-bashrc-snippet.txt >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need an actual place to mount the hadoop file system. We will create a hadoop directory in the system /var directory for this purpose. Then, change the ownership of this file to our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo mkdir /var/lib/hadoop\n",
    "sudo chown -R $USER:hadoop /var/lib/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two small changes we want to make manually to the config files. First, change the variable HADOOP opts to disable ipv6.\n",
    "Second, remove the code segment \"${JAVA_HOME}\", and replace it with the location of your chosen jdk directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sed -i s/\"^export HADOOP_OPTS=.*\\$\"/\"export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true\"/ etc/hadoop/hadoop-env.sh\n",
    "sed -i s+\"^export JAVA_HOME=.*\\$\"+'export JAVA_HOME=/usr/lib/jvm/java-8-oracle'+ \"etc/hadoop/hadoop-env.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, append the following files to their respective xml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Hadoop-core-snippet.txt > etc/hadoop/core-site.xml\n",
    "cat Hadoop-hdfs-snippet.txt >> etc/hadoop/dfs-site.xml\n",
    "cat Hadoop-yarn-snippet.txt > etc/hadoop/yarn-site.xml\n",
    "rm *-snippet.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please open a new terminal or reload your bashrc scirpt. Finally, we will format our hdfs directory and start our hadoop file systems. Check your results with the jps command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "hdfs namenode -format\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this should be something like the following:\n",
    "\n",
    "9648 Jps\n",
    "\n",
    "8260 ResourceManager\n",
    "\n",
    "8389 NodeManager\n",
    "\n",
    "9147 DataNode\n",
    "\n",
    "8989 NameNode\n",
    "\n",
    "9342 SecondaryNameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a general-purpose cluser computing system which provides APIs for several high level computing languages, including Python. In addition, it supports higher-level tools including Spark SQL, MLlib, GraphX, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the hadoop installation, we want to install, unpack and remove the provided tar archive. Again, we create a symbolic link to the resulting directory which is simply named spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /usr/local\n",
    "sudo wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
    "sudo tar xzf *.tgz\n",
    "sudo rm *.tgz\n",
    "sudo chown -R $USER spark*\n",
    "sudo ln -s spark* spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Spark Configuration Files posted on Slack. Then, check if the spark-env.sh file exists in the spark/conf directory. If it does not, create a copy of it from the spark-env.sh.template file in spark/conf.\n",
    "\n",
    "Append the contents of Spark-conf-snippet.txt to the spark-env.sh file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd spark\n",
    "mv /home/cai29/Downloads/\n",
    "bash -c 'if [ ! -f conf/spark-env.sh ]; then cp conf/spark-env.sh.template conf/spark-env.sh; fi'\n",
    "tar xzf CDS-Spark-config-files.tar.gz\n",
    "rm CDS-Spark-config-files.tar.gz\n",
    "cat Spark-conf-snippet.txt >> conf/spark-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, append the contents of Spark-bashrc.txt to your .bashrc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Spark-bashrc-snippet.txt >> ~/.bashrc\n",
    "rm *-snippet.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if everything is set up properly by running: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "pyspark --master yarn --deploy-mode client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check if:\n",
    "    \n",
    "A jupyter notebook app pops up\n",
    "\n",
    "Make a new python 3 file, type sc into a cell, and run it. \n",
    "    You should see information about your spark session\n",
    "\n",
    "Go to localhost:50070 using your browser and \n",
    "    confirm that there is an app running\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're done! You've installed Anaconda, Hadoop and Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
