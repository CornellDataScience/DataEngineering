{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark\n",
    "\n",
    "This tutorial is using the python API for spark. The original spark is written in Scala. APIs in Java and R are also available, and should definitely be checked out if it better suits your preferences. R in particular has some pretty good developments going on for both SparkR and Sparklyr packages. The advantage PySpark has over these APIs is the large number of modules and functionalities available, and the compatibility with many python libraries, which allows flexible use of user-defined functions. If you want the best performance and control over your spark program, however, learning Scala and coding Spark in Scala is highly recommended.\n",
    "\n",
    "This tutorial is a piece of working code in PySpark, which should give you the basic functionalities and syntax of the program. The underlying mechanism for Spark and Hadoop will be something that we cover in the meetings and the Data Engineering workshops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes your python can't find the pyspark package because of some path complications.\n",
    "# A handy package in python called findspark makes sure this doesn't happen, and \n",
    "# can be used as shown below.\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have hadoop running in your environment. A handy way to check is the bash command jps. jps should ideally show something similar to the output of this next box, whlch executes the bash code. (in general, putting a ! at the beginning of a line in jupyter notebook means it's executing bash commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8193 NameNode\n",
      "8388 DataNode\n",
      "8887 ResourceManager\n",
      "8647 SecondaryNameNode\n",
      "9241 NodeManager\n",
      "23355 Jps\n",
      "32510 launcher.jar\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your output for the previous one doesn't show the hadoop daemons running, run the following code box, which will start the relevant daemons (assuming you have installed hadoop and spark correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-all.sh\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully now you have the jps output showing the daemons running. If you see some error message or the daemons are still missing (namenode, datanode, secondarynamenode, resourcemanager, nodemanager) then you should make sure that you have correctly installed hadoop and spark.\n",
    "\n",
    "Now that we have the prerequisites done, let's dive into how to start an instance of spark. We will instantiate a SparkSession, and will specify the master as YARN. This means that spark will use YARN as its scheduler. This is especially useful if you are working with the hadoop filesystem as your backend. (our main filesystem in CDS is a hadoop file system.\n",
    "\n",
    "__note: please make sure that the number of executor instances (configuration named \"spark.executor.instances\") does not exceed the number of cores on your computer - 1. Change your spark.executor.memory to 1g and driver memory to 2g to make sure your computer doesn't crash while running this notebook.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .appName(\"tests\") \\\n",
    "        .config(\"spark.executor.instances\", \"50\") \\\n",
    "        .config(\"spark.executor.memory\",\"5g\") \\\n",
    "        .config(\"spark.driver.memory\",\"10g\") \\\n",
    "        .config(\"spark.executor.cores\",'1') \\\n",
    "        .config(\"spark.scheduler.mode\",\"FIFO\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load a file to use. In our case, we'll use the review.json and business.json. These files will be provided separately. But make sure to have it in the hdfs using the hadoop fs -put (local directory of file) (target hdfs directory) command. An example of how you put files into the hdfs directory is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hduser1/Yelp/dataset/business.json\n",
      "/home/hduser1/Yelp/dataset/review.json\n",
      "Found 2 items\n",
      "-rw-r--r--   5 hduser1 supergroup  132272455 2018-03-07 16:18 /first_folder/business.json\n",
      "-rw-r--r--   5 hduser1 supergroup 3819730722 2018-03-07 16:18 /first_folder/review.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: `/first_folder': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# check that your file exists in some local directory. You are free to change this line to any directory\n",
    "# that you have the file stored in (usually ~/Downloads/review.json, but I'll leave it up to you)\n",
    "ls ~/Yelp/dataset/review.json ~/Yelp/dataset/business.json\n",
    "\n",
    "# now create a folder in your hdfs called first_folder\n",
    "hadoop fs -mkdir /first_folder\n",
    "\n",
    "# transfer local files into your hdfs. Again adjust the first directory in the command to the actual \n",
    "# directory of the review.json and business.json files\n",
    "hadoop fs -put ~/Yelp/dataset/review.json /first_folder/\n",
    "hadoop fs -put ~/Yelp/dataset/business.json /first_folder/\n",
    "\n",
    "# display the contents of the /first_folder/ directory in your hdfs\n",
    "hadoop fs -ls /first_folder/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the dataset let's load it to the spark workspace. \n",
    "\n",
    "__please make sure to change the repartition() function argument to 2x the number of CPU cores (which is usually 4) on your laptop. This code was written to be executable on the servers, which has 72 cores currently__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- AcceptsInsurance: boolean (nullable = true)\n",
      " |    |-- AgesAllowed: string (nullable = true)\n",
      " |    |-- Alcohol: string (nullable = true)\n",
      " |    |-- Ambience: struct (nullable = true)\n",
      " |    |    |-- casual: boolean (nullable = true)\n",
      " |    |    |-- classy: boolean (nullable = true)\n",
      " |    |    |-- divey: boolean (nullable = true)\n",
      " |    |    |-- hipster: boolean (nullable = true)\n",
      " |    |    |-- intimate: boolean (nullable = true)\n",
      " |    |    |-- romantic: boolean (nullable = true)\n",
      " |    |    |-- touristy: boolean (nullable = true)\n",
      " |    |    |-- trendy: boolean (nullable = true)\n",
      " |    |    |-- upscale: boolean (nullable = true)\n",
      " |    |-- BYOB: boolean (nullable = true)\n",
      " |    |-- BYOBCorkage: string (nullable = true)\n",
      " |    |-- BestNights: struct (nullable = true)\n",
      " |    |    |-- friday: boolean (nullable = true)\n",
      " |    |    |-- monday: boolean (nullable = true)\n",
      " |    |    |-- saturday: boolean (nullable = true)\n",
      " |    |    |-- sunday: boolean (nullable = true)\n",
      " |    |    |-- thursday: boolean (nullable = true)\n",
      " |    |    |-- tuesday: boolean (nullable = true)\n",
      " |    |    |-- wednesday: boolean (nullable = true)\n",
      " |    |-- BikeParking: boolean (nullable = true)\n",
      " |    |-- BusinessAcceptsBitcoin: boolean (nullable = true)\n",
      " |    |-- BusinessAcceptsCreditCards: boolean (nullable = true)\n",
      " |    |-- BusinessParking: struct (nullable = true)\n",
      " |    |    |-- garage: boolean (nullable = true)\n",
      " |    |    |-- lot: boolean (nullable = true)\n",
      " |    |    |-- street: boolean (nullable = true)\n",
      " |    |    |-- valet: boolean (nullable = true)\n",
      " |    |    |-- validated: boolean (nullable = true)\n",
      " |    |-- ByAppointmentOnly: boolean (nullable = true)\n",
      " |    |-- Caters: boolean (nullable = true)\n",
      " |    |-- CoatCheck: boolean (nullable = true)\n",
      " |    |-- Corkage: boolean (nullable = true)\n",
      " |    |-- DietaryRestrictions: struct (nullable = true)\n",
      " |    |    |-- dairy-free: boolean (nullable = true)\n",
      " |    |    |-- gluten-free: boolean (nullable = true)\n",
      " |    |    |-- halal: boolean (nullable = true)\n",
      " |    |    |-- kosher: boolean (nullable = true)\n",
      " |    |    |-- soy-free: boolean (nullable = true)\n",
      " |    |    |-- vegan: boolean (nullable = true)\n",
      " |    |    |-- vegetarian: boolean (nullable = true)\n",
      " |    |-- DogsAllowed: boolean (nullable = true)\n",
      " |    |-- DriveThru: boolean (nullable = true)\n",
      " |    |-- GoodForDancing: boolean (nullable = true)\n",
      " |    |-- GoodForKids: boolean (nullable = true)\n",
      " |    |-- GoodForMeal: struct (nullable = true)\n",
      " |    |    |-- breakfast: boolean (nullable = true)\n",
      " |    |    |-- brunch: boolean (nullable = true)\n",
      " |    |    |-- dessert: boolean (nullable = true)\n",
      " |    |    |-- dinner: boolean (nullable = true)\n",
      " |    |    |-- latenight: boolean (nullable = true)\n",
      " |    |    |-- lunch: boolean (nullable = true)\n",
      " |    |-- HairSpecializesIn: struct (nullable = true)\n",
      " |    |    |-- africanamerican: boolean (nullable = true)\n",
      " |    |    |-- asian: boolean (nullable = true)\n",
      " |    |    |-- coloring: boolean (nullable = true)\n",
      " |    |    |-- curly: boolean (nullable = true)\n",
      " |    |    |-- extensions: boolean (nullable = true)\n",
      " |    |    |-- kids: boolean (nullable = true)\n",
      " |    |    |-- perms: boolean (nullable = true)\n",
      " |    |    |-- straightperms: boolean (nullable = true)\n",
      " |    |-- HappyHour: boolean (nullable = true)\n",
      " |    |-- HasTV: boolean (nullable = true)\n",
      " |    |-- Music: struct (nullable = true)\n",
      " |    |    |-- background_music: boolean (nullable = true)\n",
      " |    |    |-- dj: boolean (nullable = true)\n",
      " |    |    |-- jukebox: boolean (nullable = true)\n",
      " |    |    |-- karaoke: boolean (nullable = true)\n",
      " |    |    |-- live: boolean (nullable = true)\n",
      " |    |    |-- no_music: boolean (nullable = true)\n",
      " |    |    |-- video: boolean (nullable = true)\n",
      " |    |-- NoiseLevel: string (nullable = true)\n",
      " |    |-- Open24Hours: boolean (nullable = true)\n",
      " |    |-- OutdoorSeating: boolean (nullable = true)\n",
      " |    |-- RestaurantsAttire: string (nullable = true)\n",
      " |    |-- RestaurantsCounterService: boolean (nullable = true)\n",
      " |    |-- RestaurantsDelivery: boolean (nullable = true)\n",
      " |    |-- RestaurantsGoodForGroups: boolean (nullable = true)\n",
      " |    |-- RestaurantsPriceRange2: long (nullable = true)\n",
      " |    |-- RestaurantsReservations: boolean (nullable = true)\n",
      " |    |-- RestaurantsTableService: boolean (nullable = true)\n",
      " |    |-- RestaurantsTakeOut: boolean (nullable = true)\n",
      " |    |-- Smoking: string (nullable = true)\n",
      " |    |-- WheelchairAccessible: boolean (nullable = true)\n",
      " |    |-- WiFi: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- Friday: string (nullable = true)\n",
      " |    |-- Monday: string (nullable = true)\n",
      " |    |-- Saturday: string (nullable = true)\n",
      " |    |-- Sunday: string (nullable = true)\n",
      " |    |-- Thursday: string (nullable = true)\n",
      " |    |-- Tuesday: string (nullable = true)\n",
      " |    |-- Wednesday: string (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data from the hdfs directory, and then repartition the data\n",
    "# the repartitioning chops the dataset (which at the hdfs should be divided\n",
    "# to 29 partitions) to be processed by all your cores. \n",
    "reviews = spark.read.json('/first_folder/review.json').repartition(150)\n",
    "business = spark.read.json('/first_folder/business.json').repartition(150)\n",
    "\n",
    "# let's print out the schema of the dataset to get an understanding of how the data\n",
    "# is structured and the data type.\n",
    "reviews.printSchema()\n",
    "business.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a good idea of what both datasets looks like. In this case, let's say we want to do a simple row count, which will tell us how many reviews there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736897 reviews\n",
      "156639 businesses\n"
     ]
    }
   ],
   "source": [
    "# row count for businesses\n",
    "print(str(reviews.count()) + \" reviews\")\n",
    "\n",
    "# row count for businesses\n",
    "print(str(business.count()) + \" businesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL operations are extremely straightforward in spark, including selecting columns(projection), filtering by column values (selection) and joins, which will all be shown below. This is because the Spark DataFrame API largely revolves around SparkSQL. Most SQL commands can be done in functions that carry the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original review data's schema\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "Schema on review data with projection\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "4334 reviews that have more than 20 votes\n"
     ]
    }
   ],
   "source": [
    "print(\"original review data's schema\")\n",
    "reviews.printSchema()\n",
    "# projection on columns for the review file\n",
    "reviews_text = reviews.select('review_id','text')\n",
    "\n",
    "print(\"Schema on review data with projection\")\n",
    "reviews_text.printSchema()\n",
    "\n",
    "# selection on reviews file\n",
    "reviews_funny = reviews.filter(reviews.funny > 20)\n",
    "print(str(reviews_funny.count()) + \" reviews that have more than 20 votes\")\n",
    "\n",
    "# Join the business_stars and reviews table\n",
    "# on the businessID column\n",
    "reviews_with_business = reviews.join(business, reviews.business_id == business.business_id,how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do summary statistic operations, as well as finding out unique elements and counting how many there are. Please keep in mind that groupBy(), as is the case for most sql engines, are one of the costliest opeations in SparkSQL, and should be used only when they're needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us suppose that we want to predict the stars given the text data. To make this executable in normal laptops, let's do this only for the few thousand reviews that have been received more than 20 funny votes.\n",
    "\n",
    "In order to use the review text to predict star ratings though, I need to first transform the text data into numeric representations. This is done with a word2vec transformation of the review texts. In other words, I would like to convert the string of texts to a vector that can represent each review. I need to several steps to achieve this goal.\n",
    "\n",
    "1. Tokenize the text (convert it to a word - count pairs)\n",
    "2. Remove all the stop words\n",
    "3. Run the final versions into a word2vec model, which will then create a vector representing the \"orientation\" of the words in each review\n",
    "\n",
    "Word2vec has the property that similar words are clumped together using a 2-layer neural network. We train the word2vec on the text data, and then will use a logistic regression model to map the relationship between the text and star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import relevant packages from Spark\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "DTMmatrix = tokenizer.transform(reviews_funny)\n",
    "\n",
    "# Stop word removal\n",
    "stopremove = StopWordsRemover(inputCol='words',outputCol='cleaned')\n",
    "dat = stopremove.transform(DTMmatrix)\n",
    "\n",
    "#fit a word2vec model and transform the data \n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, numPartitions=150, inputCol=\"cleaned\", outputCol=\"word2vec\")\n",
    "model = word2Vec.fit(dat)\n",
    "result = model.transform(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cleaned: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- word2vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a word2vec transformation of the text, let's see if we can predict a star rating of a particular set of funny reviews based on its text. For demonstration purposes, we will use a logistic regression. You should try fitting other models and playing around with the model parameters. Look up the pyspark documentation to see what types of modules there are, in this [link](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cleaned: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- word2vec: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# define my logistic regression model. \n",
    "# Use default values and give it the label and feature column names\n",
    "logit = LogisticRegression(featuresCol = 'word2vec', labelCol='stars')\n",
    "\n",
    "# let's do holdout validation on my model. This means I'll separate some part of the data as my \"test\" set\n",
    "# (or more correctly the validation set) using the randomsplit method.\n",
    "train, test = result.randomSplit([0.8,0.2])\n",
    "logit_model = logit.fit(train)\n",
    "\n",
    "# make predictions using the transform function\n",
    "# a new column 'prediction' will be added to the test dataframe\n",
    "test = logit_model.transform(test)\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate our models using evaluator objects. The default metric is the f1 score, which is a weighted accuracy measure used for multi-class settings (in this case the predicted variable is not a yes/no but rather a star system ranging from 1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33435981133169324"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='stars')\n",
    "evaluator.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a .33 f1 score, which is pretty low. Considering this is just using the text to predict the star ratings, it's expected that the accuracy of this model would not be high. This tutorial should, however, show you the basics of how spark scripts can be written. Please write a script that uses another ML algorithm. Note that for this particular script, I did not need to use a VectorAssembler object. \n",
    "\n",
    "If the inputs to a ML model is distributed over multiple columns (let's say different attributes of a review etc) then you need to use the VectorAssembler to create a new column that contains all these column values as one vector for each row. Please look this up and try to figure it out on your own.\n",
    "\n",
    "For those who want to get ahead, one more thing you can learn is using what is known as a _pipeline_ in spark. These help speed up your operations considerably."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
