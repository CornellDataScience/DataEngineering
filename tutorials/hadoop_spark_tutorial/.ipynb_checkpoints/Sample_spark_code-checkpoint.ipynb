{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = spark.read.csv(\"/user/dk444/train.csv\",header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = spark.read.csv(\"/user/dk444/test.csv\", header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "114321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dat.columns))\n",
    "print(dat.count())\n",
    "dat.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#repartition to 5 blocks\n",
    "dat = dat.repartition(8)\n",
    "test = dat.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- v1: double (nullable = true)\n",
      " |-- v2: double (nullable = true)\n",
      " |-- v3: string (nullable = true)\n",
      " |-- v4: double (nullable = true)\n",
      " |-- v5: double (nullable = true)\n",
      " |-- v6: double (nullable = true)\n",
      " |-- v7: double (nullable = true)\n",
      " |-- v8: double (nullable = true)\n",
      " |-- v9: double (nullable = true)\n",
      " |-- v10: double (nullable = true)\n",
      " |-- v11: double (nullable = true)\n",
      " |-- v12: double (nullable = true)\n",
      " |-- v13: double (nullable = true)\n",
      " |-- v14: double (nullable = true)\n",
      " |-- v15: double (nullable = true)\n",
      " |-- v16: double (nullable = true)\n",
      " |-- v17: double (nullable = true)\n",
      " |-- v18: double (nullable = true)\n",
      " |-- v19: double (nullable = true)\n",
      " |-- v20: double (nullable = true)\n",
      " |-- v21: double (nullable = true)\n",
      " |-- v22: string (nullable = true)\n",
      " |-- v23: double (nullable = true)\n",
      " |-- v24: string (nullable = true)\n",
      " |-- v25: double (nullable = true)\n",
      " |-- v26: double (nullable = true)\n",
      " |-- v27: double (nullable = true)\n",
      " |-- v28: double (nullable = true)\n",
      " |-- v29: double (nullable = true)\n",
      " |-- v30: string (nullable = true)\n",
      " |-- v31: string (nullable = true)\n",
      " |-- v32: double (nullable = true)\n",
      " |-- v33: double (nullable = true)\n",
      " |-- v34: double (nullable = true)\n",
      " |-- v35: double (nullable = true)\n",
      " |-- v36: double (nullable = true)\n",
      " |-- v37: double (nullable = true)\n",
      " |-- v38: integer (nullable = true)\n",
      " |-- v39: double (nullable = true)\n",
      " |-- v40: double (nullable = true)\n",
      " |-- v41: double (nullable = true)\n",
      " |-- v42: double (nullable = true)\n",
      " |-- v43: double (nullable = true)\n",
      " |-- v44: double (nullable = true)\n",
      " |-- v45: double (nullable = true)\n",
      " |-- v46: double (nullable = true)\n",
      " |-- v47: string (nullable = true)\n",
      " |-- v48: double (nullable = true)\n",
      " |-- v49: double (nullable = true)\n",
      " |-- v50: double (nullable = true)\n",
      " |-- v51: double (nullable = true)\n",
      " |-- v52: string (nullable = true)\n",
      " |-- v53: double (nullable = true)\n",
      " |-- v54: double (nullable = true)\n",
      " |-- v55: double (nullable = true)\n",
      " |-- v56: string (nullable = true)\n",
      " |-- v57: double (nullable = true)\n",
      " |-- v58: double (nullable = true)\n",
      " |-- v59: double (nullable = true)\n",
      " |-- v60: double (nullable = true)\n",
      " |-- v61: double (nullable = true)\n",
      " |-- v62: integer (nullable = true)\n",
      " |-- v63: double (nullable = true)\n",
      " |-- v64: double (nullable = true)\n",
      " |-- v65: double (nullable = true)\n",
      " |-- v66: string (nullable = true)\n",
      " |-- v67: double (nullable = true)\n",
      " |-- v68: double (nullable = true)\n",
      " |-- v69: double (nullable = true)\n",
      " |-- v70: double (nullable = true)\n",
      " |-- v71: string (nullable = true)\n",
      " |-- v72: integer (nullable = true)\n",
      " |-- v73: double (nullable = true)\n",
      " |-- v74: string (nullable = true)\n",
      " |-- v75: string (nullable = true)\n",
      " |-- v76: double (nullable = true)\n",
      " |-- v77: double (nullable = true)\n",
      " |-- v78: double (nullable = true)\n",
      " |-- v79: string (nullable = true)\n",
      " |-- v80: double (nullable = true)\n",
      " |-- v81: double (nullable = true)\n",
      " |-- v82: double (nullable = true)\n",
      " |-- v83: double (nullable = true)\n",
      " |-- v84: double (nullable = true)\n",
      " |-- v85: double (nullable = true)\n",
      " |-- v86: double (nullable = true)\n",
      " |-- v87: double (nullable = true)\n",
      " |-- v88: double (nullable = true)\n",
      " |-- v89: double (nullable = true)\n",
      " |-- v90: double (nullable = true)\n",
      " |-- v91: string (nullable = true)\n",
      " |-- v92: double (nullable = true)\n",
      " |-- v93: double (nullable = true)\n",
      " |-- v94: double (nullable = true)\n",
      " |-- v95: double (nullable = true)\n",
      " |-- v96: double (nullable = true)\n",
      " |-- v97: double (nullable = true)\n",
      " |-- v98: double (nullable = true)\n",
      " |-- v99: double (nullable = true)\n",
      " |-- v100: double (nullable = true)\n",
      " |-- v101: double (nullable = true)\n",
      " |-- v102: double (nullable = true)\n",
      " |-- v103: double (nullable = true)\n",
      " |-- v104: double (nullable = true)\n",
      " |-- v105: double (nullable = true)\n",
      " |-- v106: double (nullable = true)\n",
      " |-- v107: string (nullable = true)\n",
      " |-- v108: double (nullable = true)\n",
      " |-- v109: double (nullable = true)\n",
      " |-- v110: string (nullable = true)\n",
      " |-- v111: double (nullable = true)\n",
      " |-- v112: string (nullable = true)\n",
      " |-- v113: string (nullable = true)\n",
      " |-- v114: double (nullable = true)\n",
      " |-- v115: double (nullable = true)\n",
      " |-- v116: double (nullable = true)\n",
      " |-- v117: double (nullable = true)\n",
      " |-- v118: double (nullable = true)\n",
      " |-- v119: double (nullable = true)\n",
      " |-- v120: double (nullable = true)\n",
      " |-- v121: double (nullable = true)\n",
      " |-- v122: double (nullable = true)\n",
      " |-- v123: double (nullable = true)\n",
      " |-- v124: double (nullable = true)\n",
      " |-- v125: string (nullable = true)\n",
      " |-- v126: double (nullable = true)\n",
      " |-- v127: double (nullable = true)\n",
      " |-- v128: double (nullable = true)\n",
      " |-- v129: integer (nullable = true)\n",
      " |-- v130: double (nullable = true)\n",
      " |-- v131: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = dat.drop('ID')\n",
    "test = test.drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target=0, v1=1.89126321222, v2=5.07308380726, v3='C', v4=4.53577352927, v5=10.0828062908, v6=2.93680250997, v7=3.03903435756, v8=0.399666460575, v9=7.81954975863, v10=0.568927407623, v11=14.9999998712, v12=6.24109754675, v13=3.29196682523, v14=8.44685583951, v15=1.5199164593, v16=6.34482710368, v17=3.58590890437, v18=0.613470427774, v19=0.199895316332, v20=17.3551127837, v21=4.35819923434, v22='SWO', v23=-7.32699824992e-07, v24='E', v25=0.614382450625, v26=2.23203214616, v27=3.25278730828, v28=4.59910229022, v29=7.38847542283, v30='C', v31='A', v32=1.44981512376, v33=2.69516755249, v34=6.58163194177, v35=7.52788177704, v36=13.5361298373, v37=0.672783295774, v38=0, v39=0.199415650816, v40=8.86621816591, v41=6.55204512588, v42=13.0018595857, v43=2.78723337437, v44=9.07386356584, v45=10.0510205796, v46=0.609217121791, v47='C', v48=12.5992076123, v49=7.49070586136, v50=0.60156982581, v51=6.1563412669, v52='D', v53=15.4968562468, v54=0.545802944473, v55=2.03531627757, v56='AS', v57=4.59107783697, v58=3.66359371005, v59=10.2712536544, v60=2.04460912635, v61=14.6429828013, v62=3, v63=0.65363616751, v64=6.24884338862, v65=15.0244700737, v66='A', v67=8.73605898676, v68=19.3087563914, v69=10.0689654233, v70=9.5748517769, v71='B', v72=4, v73=2.3049640757, v74='B', v75='B', v76=2.1680667341, v77=6.28252861582, v78=12.2758628034, v79='E', v80=3.45864701681, v81=5.96307042627, v82=3.3050647589, v83=2.48141237496, v84=2.56505584726, v85=3.75384587281, v86=0.808550404658, v87=11.1974343319, v88=2.32342085182, v89=0.654506267333, v90=1.0161668581, v91='G', v92=0.372071586349, v93=5.64955308058, v94=4.51807713529, v95=0.451150868386, v96=5.72490659113, v97=7.33944889983, v98=10.8028182782, v99=1.38011147597, v100=16.3133632889, v101=7.22145534936, v102=2.03456824249, v103=6.26391276598, v104=2.69466423193, v105=0.321742136518, v106=12.1533163259, v107='C', v108=2.70604964022, v109=6.28964132007, v110='B', v111=4.01486951114, v112='H', v113=None, v114=14.7533616288, v115=9.931034196, v116=3.01886844687, v117=13.0045989432, v118=8.0122330018, v119=5.38301878135, v120=0.506912786069, v121=3.31265436881, v122=4.36090273211, v123=4.59791638896, v124=0.127694827089, v125='CC', v126=1.93855204372, v127=2.09107775233, v128=2.510257146, v129=1, v130=1.77370078587, v131=1.51724227269)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the number of rows to display by using the take()\n",
    "dat.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to count unique classes in a column\n",
    "dat.select(col(\"target\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target=1, count=87021), Row(target=0, count=27300)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do an aggregate count to see the distribution of the classes\n",
    "dat.groupBy(col(\"target\")).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=87021), Row(count(1)=27300)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can do the same thing in sql code\n",
    "# we must first create a \"view\" of the data in order to execute SQL functions\n",
    "dat.createOrReplaceTempView(\"dat\")\n",
    "spark.sql(\"SELECT COUNT(*) FROM dat GROUP BY Target\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v3', 'v22', 'v24', 'v30', 'v31', 'v47', 'v52', 'v56', 'v66', 'v71', 'v74', 'v75', 'v79', 'v91', 'v107', 'v110', 'v112', 'v113', 'v125']\n",
      "['target', 'v38', 'v62', 'v72', 'v129']\n"
     ]
    }
   ],
   "source": [
    "#Convert string types(categorical) to integerTypes\n",
    "types = [str(f.dataType) for f in dat.schema.fields]\n",
    "ind_str = [i for i in range(len(types)) if types[i] is 'StringType']\n",
    "ind_int = [i for i in range(len(types)) if types[i] is 'IntegerType']\n",
    "stringcol= [dat.columns[i] for i in ind_str]\n",
    "intcol = [dat.columns[i] for i in ind_int]\n",
    "print(stringcol)\n",
    "print(intcol)\n",
    "del ind_str, ind_int, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all string columns\n",
    "for colname in stringcol:\n",
    "    dat = dat.drop(colname)\n",
    "    test = test.drop(colname)\n",
    "    \n",
    "# remove 'target' from inttype list\n",
    "intcol = intcol[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v38', 'v62', 'v72', 'v129']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imputer requires that all columns it uses be floattype or doubletype, so let's convert ints to floats\n",
    "for colname in intcol:\n",
    "    dat = dat.withColumn( colname+\"cast\", col(colname).cast(\"float\"))\n",
    "    dat = dat.drop(colname)\n",
    "    test = test.withColumn( colname+\"cast\", col(colname).cast(\"float\"))\n",
    "    test = test.drop(colname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Impute missing values for both dat and test\n",
    "#Let's actually impute now!\n",
    "newcolnames = [colname+\"new\" for colname in dat.columns[1:]]\n",
    "imp = Imputer(strategy = 'mean',missingValue=None, inputCols = dat.columns[1:],outputCols=newcolnames )\n",
    "model = imp.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = model.transform(dat)\n",
    "test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v38cast',\n",
       " 'v62cast',\n",
       " 'v72cast',\n",
       " 'v129cast',\n",
       " 'v1new',\n",
       " 'v2new',\n",
       " 'v4new',\n",
       " 'v5new',\n",
       " 'v6new',\n",
       " 'v7new',\n",
       " 'v8new',\n",
       " 'v9new',\n",
       " 'v10new',\n",
       " 'v11new',\n",
       " 'v12new',\n",
       " 'v13new',\n",
       " 'v14new',\n",
       " 'v15new',\n",
       " 'v16new',\n",
       " 'v17new',\n",
       " 'v18new',\n",
       " 'v19new',\n",
       " 'v20new',\n",
       " 'v21new',\n",
       " 'v23new',\n",
       " 'v25new',\n",
       " 'v26new',\n",
       " 'v27new',\n",
       " 'v28new',\n",
       " 'v29new',\n",
       " 'v32new',\n",
       " 'v33new',\n",
       " 'v34new',\n",
       " 'v35new',\n",
       " 'v36new',\n",
       " 'v37new',\n",
       " 'v39new',\n",
       " 'v40new',\n",
       " 'v41new',\n",
       " 'v42new',\n",
       " 'v43new',\n",
       " 'v44new',\n",
       " 'v45new',\n",
       " 'v46new',\n",
       " 'v48new',\n",
       " 'v49new',\n",
       " 'v50new',\n",
       " 'v51new',\n",
       " 'v53new',\n",
       " 'v54new',\n",
       " 'v55new',\n",
       " 'v57new',\n",
       " 'v58new',\n",
       " 'v59new',\n",
       " 'v60new',\n",
       " 'v61new',\n",
       " 'v63new',\n",
       " 'v64new',\n",
       " 'v65new',\n",
       " 'v67new',\n",
       " 'v68new',\n",
       " 'v69new',\n",
       " 'v70new',\n",
       " 'v73new',\n",
       " 'v76new',\n",
       " 'v77new',\n",
       " 'v78new',\n",
       " 'v80new',\n",
       " 'v81new',\n",
       " 'v82new',\n",
       " 'v83new',\n",
       " 'v84new',\n",
       " 'v85new',\n",
       " 'v86new',\n",
       " 'v87new',\n",
       " 'v88new',\n",
       " 'v89new',\n",
       " 'v90new',\n",
       " 'v92new',\n",
       " 'v93new',\n",
       " 'v94new',\n",
       " 'v95new',\n",
       " 'v96new',\n",
       " 'v97new',\n",
       " 'v98new',\n",
       " 'v99new',\n",
       " 'v100new',\n",
       " 'v101new',\n",
       " 'v102new',\n",
       " 'v103new',\n",
       " 'v104new',\n",
       " 'v105new',\n",
       " 'v106new',\n",
       " 'v108new',\n",
       " 'v109new',\n",
       " 'v111new',\n",
       " 'v114new',\n",
       " 'v115new',\n",
       " 'v116new',\n",
       " 'v117new',\n",
       " 'v118new',\n",
       " 'v119new',\n",
       " 'v120new',\n",
       " 'v121new',\n",
       " 'v122new',\n",
       " 'v123new',\n",
       " 'v124new',\n",
       " 'v126new',\n",
       " 'v127new',\n",
       " 'v128new',\n",
       " 'v130new',\n",
       " 'v131new',\n",
       " 'v38castnew',\n",
       " 'v62castnew',\n",
       " 'v72castnew',\n",
       " 'v129castnew']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dat.columns[109:]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the columns into a single feature vector\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in features],\n",
    "    outputCol='features')\n",
    "\n",
    "dat_processed = assembler.transform(dat)\n",
    "test_processed = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = true)\n",
      " |-- v1: double (nullable = true)\n",
      " |-- v2: double (nullable = true)\n",
      " |-- v4: double (nullable = true)\n",
      " |-- v5: double (nullable = true)\n",
      " |-- v6: double (nullable = true)\n",
      " |-- v7: double (nullable = true)\n",
      " |-- v8: double (nullable = true)\n",
      " |-- v9: double (nullable = true)\n",
      " |-- v10: double (nullable = true)\n",
      " |-- v11: double (nullable = true)\n",
      " |-- v12: double (nullable = true)\n",
      " |-- v13: double (nullable = true)\n",
      " |-- v14: double (nullable = true)\n",
      " |-- v15: double (nullable = true)\n",
      " |-- v16: double (nullable = true)\n",
      " |-- v17: double (nullable = true)\n",
      " |-- v18: double (nullable = true)\n",
      " |-- v19: double (nullable = true)\n",
      " |-- v20: double (nullable = true)\n",
      " |-- v21: double (nullable = true)\n",
      " |-- v23: double (nullable = true)\n",
      " |-- v25: double (nullable = true)\n",
      " |-- v26: double (nullable = true)\n",
      " |-- v27: double (nullable = true)\n",
      " |-- v28: double (nullable = true)\n",
      " |-- v29: double (nullable = true)\n",
      " |-- v32: double (nullable = true)\n",
      " |-- v33: double (nullable = true)\n",
      " |-- v34: double (nullable = true)\n",
      " |-- v35: double (nullable = true)\n",
      " |-- v36: double (nullable = true)\n",
      " |-- v37: double (nullable = true)\n",
      " |-- v39: double (nullable = true)\n",
      " |-- v40: double (nullable = true)\n",
      " |-- v41: double (nullable = true)\n",
      " |-- v42: double (nullable = true)\n",
      " |-- v43: double (nullable = true)\n",
      " |-- v44: double (nullable = true)\n",
      " |-- v45: double (nullable = true)\n",
      " |-- v46: double (nullable = true)\n",
      " |-- v48: double (nullable = true)\n",
      " |-- v49: double (nullable = true)\n",
      " |-- v50: double (nullable = true)\n",
      " |-- v51: double (nullable = true)\n",
      " |-- v53: double (nullable = true)\n",
      " |-- v54: double (nullable = true)\n",
      " |-- v55: double (nullable = true)\n",
      " |-- v57: double (nullable = true)\n",
      " |-- v58: double (nullable = true)\n",
      " |-- v59: double (nullable = true)\n",
      " |-- v60: double (nullable = true)\n",
      " |-- v61: double (nullable = true)\n",
      " |-- v63: double (nullable = true)\n",
      " |-- v64: double (nullable = true)\n",
      " |-- v65: double (nullable = true)\n",
      " |-- v67: double (nullable = true)\n",
      " |-- v68: double (nullable = true)\n",
      " |-- v69: double (nullable = true)\n",
      " |-- v70: double (nullable = true)\n",
      " |-- v73: double (nullable = true)\n",
      " |-- v76: double (nullable = true)\n",
      " |-- v77: double (nullable = true)\n",
      " |-- v78: double (nullable = true)\n",
      " |-- v80: double (nullable = true)\n",
      " |-- v81: double (nullable = true)\n",
      " |-- v82: double (nullable = true)\n",
      " |-- v83: double (nullable = true)\n",
      " |-- v84: double (nullable = true)\n",
      " |-- v85: double (nullable = true)\n",
      " |-- v86: double (nullable = true)\n",
      " |-- v87: double (nullable = true)\n",
      " |-- v88: double (nullable = true)\n",
      " |-- v89: double (nullable = true)\n",
      " |-- v90: double (nullable = true)\n",
      " |-- v92: double (nullable = true)\n",
      " |-- v93: double (nullable = true)\n",
      " |-- v94: double (nullable = true)\n",
      " |-- v95: double (nullable = true)\n",
      " |-- v96: double (nullable = true)\n",
      " |-- v97: double (nullable = true)\n",
      " |-- v98: double (nullable = true)\n",
      " |-- v99: double (nullable = true)\n",
      " |-- v100: double (nullable = true)\n",
      " |-- v101: double (nullable = true)\n",
      " |-- v102: double (nullable = true)\n",
      " |-- v103: double (nullable = true)\n",
      " |-- v104: double (nullable = true)\n",
      " |-- v105: double (nullable = true)\n",
      " |-- v106: double (nullable = true)\n",
      " |-- v108: double (nullable = true)\n",
      " |-- v109: double (nullable = true)\n",
      " |-- v111: double (nullable = true)\n",
      " |-- v114: double (nullable = true)\n",
      " |-- v115: double (nullable = true)\n",
      " |-- v116: double (nullable = true)\n",
      " |-- v117: double (nullable = true)\n",
      " |-- v118: double (nullable = true)\n",
      " |-- v119: double (nullable = true)\n",
      " |-- v120: double (nullable = true)\n",
      " |-- v121: double (nullable = true)\n",
      " |-- v122: double (nullable = true)\n",
      " |-- v123: double (nullable = true)\n",
      " |-- v124: double (nullable = true)\n",
      " |-- v126: double (nullable = true)\n",
      " |-- v127: double (nullable = true)\n",
      " |-- v128: double (nullable = true)\n",
      " |-- v130: double (nullable = true)\n",
      " |-- v131: double (nullable = true)\n",
      " |-- v38cast: float (nullable = true)\n",
      " |-- v62cast: float (nullable = true)\n",
      " |-- v72cast: float (nullable = true)\n",
      " |-- v129cast: float (nullable = true)\n",
      " |-- v1new: double (nullable = true)\n",
      " |-- v2new: double (nullable = true)\n",
      " |-- v4new: double (nullable = true)\n",
      " |-- v5new: double (nullable = true)\n",
      " |-- v6new: double (nullable = true)\n",
      " |-- v7new: double (nullable = true)\n",
      " |-- v8new: double (nullable = true)\n",
      " |-- v9new: double (nullable = true)\n",
      " |-- v10new: double (nullable = true)\n",
      " |-- v11new: double (nullable = true)\n",
      " |-- v12new: double (nullable = true)\n",
      " |-- v13new: double (nullable = true)\n",
      " |-- v14new: double (nullable = true)\n",
      " |-- v15new: double (nullable = true)\n",
      " |-- v16new: double (nullable = true)\n",
      " |-- v17new: double (nullable = true)\n",
      " |-- v18new: double (nullable = true)\n",
      " |-- v19new: double (nullable = true)\n",
      " |-- v20new: double (nullable = true)\n",
      " |-- v21new: double (nullable = true)\n",
      " |-- v23new: double (nullable = true)\n",
      " |-- v25new: double (nullable = true)\n",
      " |-- v26new: double (nullable = true)\n",
      " |-- v27new: double (nullable = true)\n",
      " |-- v28new: double (nullable = true)\n",
      " |-- v29new: double (nullable = true)\n",
      " |-- v32new: double (nullable = true)\n",
      " |-- v33new: double (nullable = true)\n",
      " |-- v34new: double (nullable = true)\n",
      " |-- v35new: double (nullable = true)\n",
      " |-- v36new: double (nullable = true)\n",
      " |-- v37new: double (nullable = true)\n",
      " |-- v39new: double (nullable = true)\n",
      " |-- v40new: double (nullable = true)\n",
      " |-- v41new: double (nullable = true)\n",
      " |-- v42new: double (nullable = true)\n",
      " |-- v43new: double (nullable = true)\n",
      " |-- v44new: double (nullable = true)\n",
      " |-- v45new: double (nullable = true)\n",
      " |-- v46new: double (nullable = true)\n",
      " |-- v48new: double (nullable = true)\n",
      " |-- v49new: double (nullable = true)\n",
      " |-- v50new: double (nullable = true)\n",
      " |-- v51new: double (nullable = true)\n",
      " |-- v53new: double (nullable = true)\n",
      " |-- v54new: double (nullable = true)\n",
      " |-- v55new: double (nullable = true)\n",
      " |-- v57new: double (nullable = true)\n",
      " |-- v58new: double (nullable = true)\n",
      " |-- v59new: double (nullable = true)\n",
      " |-- v60new: double (nullable = true)\n",
      " |-- v61new: double (nullable = true)\n",
      " |-- v63new: double (nullable = true)\n",
      " |-- v64new: double (nullable = true)\n",
      " |-- v65new: double (nullable = true)\n",
      " |-- v67new: double (nullable = true)\n",
      " |-- v68new: double (nullable = true)\n",
      " |-- v69new: double (nullable = true)\n",
      " |-- v70new: double (nullable = true)\n",
      " |-- v73new: double (nullable = true)\n",
      " |-- v76new: double (nullable = true)\n",
      " |-- v77new: double (nullable = true)\n",
      " |-- v78new: double (nullable = true)\n",
      " |-- v80new: double (nullable = true)\n",
      " |-- v81new: double (nullable = true)\n",
      " |-- v82new: double (nullable = true)\n",
      " |-- v83new: double (nullable = true)\n",
      " |-- v84new: double (nullable = true)\n",
      " |-- v85new: double (nullable = true)\n",
      " |-- v86new: double (nullable = true)\n",
      " |-- v87new: double (nullable = true)\n",
      " |-- v88new: double (nullable = true)\n",
      " |-- v89new: double (nullable = true)\n",
      " |-- v90new: double (nullable = true)\n",
      " |-- v92new: double (nullable = true)\n",
      " |-- v93new: double (nullable = true)\n",
      " |-- v94new: double (nullable = true)\n",
      " |-- v95new: double (nullable = true)\n",
      " |-- v96new: double (nullable = true)\n",
      " |-- v97new: double (nullable = true)\n",
      " |-- v98new: double (nullable = true)\n",
      " |-- v99new: double (nullable = true)\n",
      " |-- v100new: double (nullable = true)\n",
      " |-- v101new: double (nullable = true)\n",
      " |-- v102new: double (nullable = true)\n",
      " |-- v103new: double (nullable = true)\n",
      " |-- v104new: double (nullable = true)\n",
      " |-- v105new: double (nullable = true)\n",
      " |-- v106new: double (nullable = true)\n",
      " |-- v108new: double (nullable = true)\n",
      " |-- v109new: double (nullable = true)\n",
      " |-- v111new: double (nullable = true)\n",
      " |-- v114new: double (nullable = true)\n",
      " |-- v115new: double (nullable = true)\n",
      " |-- v116new: double (nullable = true)\n",
      " |-- v117new: double (nullable = true)\n",
      " |-- v118new: double (nullable = true)\n",
      " |-- v119new: double (nullable = true)\n",
      " |-- v120new: double (nullable = true)\n",
      " |-- v121new: double (nullable = true)\n",
      " |-- v122new: double (nullable = true)\n",
      " |-- v123new: double (nullable = true)\n",
      " |-- v124new: double (nullable = true)\n",
      " |-- v126new: double (nullable = true)\n",
      " |-- v127new: double (nullable = true)\n",
      " |-- v128new: double (nullable = true)\n",
      " |-- v130new: double (nullable = true)\n",
      " |-- v131new: double (nullable = true)\n",
      " |-- v38castnew: float (nullable = true)\n",
      " |-- v62castnew: float (nullable = true)\n",
      " |-- v72castnew: float (nullable = true)\n",
      " |-- v129castnew: float (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat_processed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a logistic regression model, tree model, and a gradient boosted tree model\n",
    "# Create an evaluator object\n",
    "\n",
    "logit = LogisticRegression(featuresCol=\"features\",labelCol=\"Target\")\n",
    "logit_model = logit.fit(train)\n",
    "\n",
    "cart = DecisionTreeClassifier(maxDepth=10)\n",
    "cart_model = cart.fit(train)\n",
    "\n",
    "gbt = GBTClassifier(maxDepth=10, stepSize=.001,maxIter=10)\n",
    "gbt_model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_logit = logit_model.transform(test)\n",
    "pred_cart = cart_model.transform(test)\n",
    "pred_gbt = gbt_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [evaluator.evaluate(i) for i in [pred_logit, pred_cart, pred_gbt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.985497295510151, 0.8885200864455169, 0.8875588161461015]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981404487483115"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56897/(56897+106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_count = len(dat.columns)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in features],\n",
    "    outputCol='features')\n",
    "\n",
    "dat_processed = assembler.transform(dat)\n",
    "test_processed = assembler.transform(test)\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\",outputCol=\"feature\")\n",
    "\n",
    "dat = normalizer.transform(assembler.transform(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concept of pipeline\n",
    "# Additionally how to cross validate\n",
    "logit = LogisticRegression(featuresCol='feature',labelCol='target')\n",
    "cart = DecisionTreeClassifier(featuresCol='feature',labelCol='target')\n",
    "gbt = GBTClassifier(featuresCol='feature',labelCol='target')\n",
    "\n",
    "\n",
    "paramGrid_logit = ParamGridBuilder() \\\n",
    "    .addGrid(logit.regParam, [0,0.01, 0.1]) \\\n",
    "    .build()\n",
    "    \n",
    "paramGrid_cart = ParamGridBuilder() \\\n",
    "    .addGrid(cart.maxDepth, [10,12,15]) \\\n",
    "    .build()\n",
    "    \n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [10,12,15]) \\\n",
    "    .addGrid(gbt.stepSize,[0.01]) \\\n",
    "    .addGrid(gbt.maxIter,[20]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='target')\n",
    "    \n",
    "\n",
    "cv_logit = CrossValidator(estimator=logit,evaluator=evaluator,estimatorParamMaps=paramGrid_logit,numFolds=10)\n",
    "cv_cart = CrossValidator(estimator=cart,evaluator=evaluator,estimatorParamMaps=paramGrid_cart,numFolds=10)\n",
    "cv_gbt = CrossValidator(estimator=gbt,evaluator=evaluator,estimatorParamMaps=paramGrid_gbt,numFolds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_logit = cv_logit.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_cart = cv_cart.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_gbt = cv_gbt.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7087400464393288, 0.6992343001874801, 0.7080240843897794]\n",
      "[0.5036192095312046, 0.5331364209963431, 0.5430835878468885, 0.5739519392919172]\n",
      "[0.7251348350063479, 0.7448645684366266, 0.7684780371882793, 0.8201436144971819]\n"
     ]
    }
   ],
   "source": [
    "print(cvmodel_logit.avgMetrics)\n",
    "print(cvmodel_cart.avgMetrics)\n",
    "print(cvmodel_gbt.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(cvmodel_gbt.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='feature',labelCol=\"target\")\n",
    "pipeline_rf = Pipeline(stages=[assembler,normalizer, rf])\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [15]) \\\n",
    "    .addGrid(rf.numTrees, [1000]) \\\n",
    "    .build()\n",
    "    \n",
    "cv_rf = CrossValidator(estimator=pipeline_rf, evaluator=evaluator, numFolds=10, estimatorParamMaps=paramGrid_rf)\n",
    "cvmodel_rf = cv_rf.fit(dat)\n",
    "cvmodel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat.rdd.sampleByKey(withReplacement=False,fractions={0:0.0001, 1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 178673.0 failed 4 times, most recent failure: Lost task 0.3 in stage 178673.0 (TID 556659, mylaptop, executor 3): java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d398b7eb3595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 178673.0 failed 4 times, most recent failure: Lost task 0.3 in stage 178673.0 (TID 556659, mylaptop, executor 3): java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "dat.rdd.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
