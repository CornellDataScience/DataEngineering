{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, before we begin, use run the command: _sudo apt install ssh default-java_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Crash Course to Common Shell Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'll be providing a quick introduction of a number of common linux shell commands, just to get you started.\n",
    "\n",
    "For those looking for more depth, I have provided a link to a fairly comprehensive list [here](https://ss64.com/bash/); a book on the topic, which also covers bash scripting [here](http://linuxcommand.org/tlcl.php).\n",
    "\n",
    "Should you ever want to know more about a specific command, one of the best and most immediate resources is to run _man `<command in question`>_, which will open the manual pages for that command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: on to the actual crash course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the following command to download and install the provided example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget CDS-Linux-Example.tar.gz\n",
    "tar xzf CDS-Linux-Example.tar.gz\n",
    "rm CDS-Linux-Example.tar.gz\n",
    "cd example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a small chat about the linux filesystem. In linux, everything in the file system falls into one of two categories: it is either a *file*, or it is a *directory*.\n",
    "\n",
    "A file put simply is just a reserved section of hard drive space with a name, while a directory just has a bunch of pointers to other entities in the filesystem (namely, other files and directories).\n",
    "\n",
    "The linux file system begins with the root node, and it branches out from there. While we can traverse away from there, it may be useful to know where the heck we are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cai29/Cornell/CDS/spark_ws\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# The above causes each line of this cell to be run with bash\n",
    "\n",
    "# First, lets see where we are exactly. Type pwd (Print Working Directory) to get the location\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we know where we are. But what's around us? We're in the dark at the moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The ls (list) command to see what entities are in the current directory\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but The output for that is kinda vague. Lets make it clearer by adding a flag. A flag is an option which may be passed to and used by a program to set certain settings at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The ls -l prints the contents of the current directory with longer, more detailed output\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now we know what's around us. We can see a number of files and directories (including two strange directories: \".\" and \"..\"  We'll get to these later).\n",
    "\n",
    "Let's see if we can't go somewhere. The totally_normal_directory seems a good place to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The cd (change directory) command moves into the specified directory\n",
    "cd totally_normal_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda is a prepackaged Python Ecosystem geared towards Data Science. Anaconda and it's supporting products are supplid by Continuum Analytcis. Anaconda comes pre-built with a wide variety of packages; a full list can be found [here](docs.continuum.io/anaconda/packages/pkg-docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation of Anaconda is fairly simple. First, download the install package supplied by Continuum Analytics and give it execute privileges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~\n",
    "wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh\n",
    "chmod 733 Anaconda*.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the provided install script; this will walk you through the installation and configuration of Anaconda (for configuration, the defaults will usually be fine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "xterm -e bash -c \"Anaconda*.sh\"\n",
    "rm Anaconda*.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installations with Conda and Pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, I think just linking the cheat sheet might be better than anything I could possibly write in a comparable space (I will likely be using this cheat sheet myself, now that I've found it): \n",
    "https://conda.io/docs/_downloads/conda-cheatsheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environments with Conda and Pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above - maybe just a description of what a virtual env _is_? Cause the cheat sheet because the cheat sheet seems sufficient, at least so far as use goes. Especially for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a framework for a distributed filesystem which allows users to store large data sets accross multiple clusters, while maintaining integrity in the face of failure. It includes HDFS which provides access to application data, and YARN, which is a framework for job scheduling and resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by creating a hadoop group, and add your existing user to it. Groups your user belongs to will grant you certain permissions for files belonging to that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "xterm -e bash \\\n",
    "    -c \"sudo addgroup hadoop; sudo usermod -a -G hadoop $USER\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, generate SSH keys to be used for verification with the system. Copy this key to your user on localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "xterm -hold -e bash \\\n",
    "    -c \"ssh-keygen; ssh-copy-id $USER@localhost\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that setup is done, it's time to actually install Hadoop. First and move into a directory named CDS, located in your home directory. Then, use wget to acquire the tar archive for hadoop. Extract hadoop from the archive and remove the archive. \n",
    "\n",
    "This hadoop directory will have a rather cumbersome name. Create a symbolic link to it, simply named hadoop, in CDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir CDS\n",
    "cd CDS\n",
    "wget http://apache.osuosl.org/hadoop/common/hadoop-2.8.0/hadoop-2.8.0.tar.gz\n",
    "tar xzf *.gz\n",
    "rm *.gz\n",
    "ln -s hadoop hadoop*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDS supplies a number of small files of configuration changes to make to your Hadoop and Spark setups. Dowload the tar file with wget and unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget <link>\n",
    "tar xzf CDS-config-files.tar.gz\n",
    "rm CDS-config-files.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, append the Hadoop-bashrc-snippet file to your bashrc file. .bash rc is a configuration file used by your bash shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Hadoop-bashrc-snippet.txt >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need an actual place to mount the hadoop file system. We will create a hadoop directory in the system /var directory for this purpose. Then, change the ownership of this file to our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "xterm -e bash \\\n",
    "    -c \"mkdir /var/lib/hadoop; sudo chown -R $USER:hadoop /var/lib/hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two small changes we want to make manually to the config files. First, change the variable HADOOP opts to disable ipv6.\n",
    "Second, remove the code segment \"${JAVA_HOME}\", and replace it with the location of your chosen jdk directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sed s/\"^.*export HADOOP_OPTS=.*\\$\"/\"export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true\"/ \\\n",
    "        hadoop/etc/hadoop/hadoop-env.sh\n",
    "set s/\"\\${JAVA_HOME}\"/\"/usr/lib/jvm/default-java\"/ \\\n",
    "        hadoop/etc/hadoop/hadoop-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, append the following files to their respective xml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Hadoop-core-snippet.txt >> hadoop/etc/hadoop/core-site.xml\n",
    "cat Hadoop-hdfs-snippet.txt >> hadoop/etc/hadoop/hdfs-site.xml\n",
    "cat Hadoop-yarn-snippet.txt >> hadoop/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will format our hdfs directory and start our hadoop file systems. Check your results with the jps command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs namenode -format\n",
    "start-all.sh\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this should be something like the following:\n",
    "\n",
    "9648 Jps\n",
    "\n",
    "8260 ResourceManager\n",
    "\n",
    "8389 NodeManager\n",
    "\n",
    "9147 DataNode\n",
    "\n",
    "8989 NameNode\n",
    "\n",
    "9342 SecondaryNameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a general-purpose cluser computing system which provides APIs for several high level computing languages, including Python. In addition, it supports higher-level tools including Spark SQL, MLlib, GraphX, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the hadoop installation, we want to install, unpack and remove the provided tar archive. Again, we create a symbolic link to the resulting directory which is simply named spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
    "tar xzf *.tgz\n",
    "rm *.tgz\n",
    "ln -s spark spark*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check if the spark-env.sh file exists in the spark/conf directory. If it does not, create a copy of it from the spark-env.sh.template file in spark/conf.\n",
    "\n",
    "Append the contents of Spark-conf-snippet.txt to the spark-env.sh file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f spark/conf/spark-env.sh] then\n",
    "    cp spark/conf/spark-env.sh.template spark/conf/spark-env.sh\n",
    "fi\n",
    "cat Spark-conf-snippet.txt >> spark/conf/spark-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, append the contents of Spark-bashrc.txt to your .bashrc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Spark-bashrc.txt >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're done! You've installed Anaconda, Hadoop and Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
