{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = spark.read.csv(\"/hduser1/train_approx.csv\",header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = spark.read.csv(\"/user/dk444/test.csv\", header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "114321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dat.columns))\n",
    "print(dat.count())\n",
    "dat.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#repartition to 5 blocks\n",
    "dat = dat.repartition(150)\n",
    "test = dat.repartition(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- v1: double (nullable = true)\n",
      " |-- v2: double (nullable = true)\n",
      " |-- v4: double (nullable = true)\n",
      " |-- v5: double (nullable = true)\n",
      " |-- v6: double (nullable = true)\n",
      " |-- v7: double (nullable = true)\n",
      " |-- v8: double (nullable = true)\n",
      " |-- v9: double (nullable = true)\n",
      " |-- v10: double (nullable = true)\n",
      " |-- v11: double (nullable = true)\n",
      " |-- v12: double (nullable = true)\n",
      " |-- v13: double (nullable = true)\n",
      " |-- v14: double (nullable = true)\n",
      " |-- v15: double (nullable = true)\n",
      " |-- v16: double (nullable = true)\n",
      " |-- v17: double (nullable = true)\n",
      " |-- v18: double (nullable = true)\n",
      " |-- v19: double (nullable = true)\n",
      " |-- v20: double (nullable = true)\n",
      " |-- v21: double (nullable = true)\n",
      " |-- v23: double (nullable = true)\n",
      " |-- v25: double (nullable = true)\n",
      " |-- v26: double (nullable = true)\n",
      " |-- v27: double (nullable = true)\n",
      " |-- v28: double (nullable = true)\n",
      " |-- v29: double (nullable = true)\n",
      " |-- v32: double (nullable = true)\n",
      " |-- v33: double (nullable = true)\n",
      " |-- v34: double (nullable = true)\n",
      " |-- v35: double (nullable = true)\n",
      " |-- v36: double (nullable = true)\n",
      " |-- v37: double (nullable = true)\n",
      " |-- v38: double (nullable = true)\n",
      " |-- v39: double (nullable = true)\n",
      " |-- v40: double (nullable = true)\n",
      " |-- v41: double (nullable = true)\n",
      " |-- v42: double (nullable = true)\n",
      " |-- v43: double (nullable = true)\n",
      " |-- v44: double (nullable = true)\n",
      " |-- v45: double (nullable = true)\n",
      " |-- v46: double (nullable = true)\n",
      " |-- v48: double (nullable = true)\n",
      " |-- v49: double (nullable = true)\n",
      " |-- v50: double (nullable = true)\n",
      " |-- v51: double (nullable = true)\n",
      " |-- v53: double (nullable = true)\n",
      " |-- v54: double (nullable = true)\n",
      " |-- v55: double (nullable = true)\n",
      " |-- v57: double (nullable = true)\n",
      " |-- v58: double (nullable = true)\n",
      " |-- v59: double (nullable = true)\n",
      " |-- v60: double (nullable = true)\n",
      " |-- v61: double (nullable = true)\n",
      " |-- v62: double (nullable = true)\n",
      " |-- v63: double (nullable = true)\n",
      " |-- v64: double (nullable = true)\n",
      " |-- v65: double (nullable = true)\n",
      " |-- v67: double (nullable = true)\n",
      " |-- v68: double (nullable = true)\n",
      " |-- v69: double (nullable = true)\n",
      " |-- v70: double (nullable = true)\n",
      " |-- v72: double (nullable = true)\n",
      " |-- v73: double (nullable = true)\n",
      " |-- v76: double (nullable = true)\n",
      " |-- v77: double (nullable = true)\n",
      " |-- v78: double (nullable = true)\n",
      " |-- v80: double (nullable = true)\n",
      " |-- v81: double (nullable = true)\n",
      " |-- v82: double (nullable = true)\n",
      " |-- v83: double (nullable = true)\n",
      " |-- v84: double (nullable = true)\n",
      " |-- v85: double (nullable = true)\n",
      " |-- v86: double (nullable = true)\n",
      " |-- v87: double (nullable = true)\n",
      " |-- v88: double (nullable = true)\n",
      " |-- v89: double (nullable = true)\n",
      " |-- v90: double (nullable = true)\n",
      " |-- v92: double (nullable = true)\n",
      " |-- v93: double (nullable = true)\n",
      " |-- v94: double (nullable = true)\n",
      " |-- v95: double (nullable = true)\n",
      " |-- v96: double (nullable = true)\n",
      " |-- v97: double (nullable = true)\n",
      " |-- v98: double (nullable = true)\n",
      " |-- v99: double (nullable = true)\n",
      " |-- v100: double (nullable = true)\n",
      " |-- v101: double (nullable = true)\n",
      " |-- v102: double (nullable = true)\n",
      " |-- v103: double (nullable = true)\n",
      " |-- v104: double (nullable = true)\n",
      " |-- v105: double (nullable = true)\n",
      " |-- v106: double (nullable = true)\n",
      " |-- v108: double (nullable = true)\n",
      " |-- v109: double (nullable = true)\n",
      " |-- v111: double (nullable = true)\n",
      " |-- v114: double (nullable = true)\n",
      " |-- v115: double (nullable = true)\n",
      " |-- v116: double (nullable = true)\n",
      " |-- v117: double (nullable = true)\n",
      " |-- v118: double (nullable = true)\n",
      " |-- v119: double (nullable = true)\n",
      " |-- v120: double (nullable = true)\n",
      " |-- v121: double (nullable = true)\n",
      " |-- v122: double (nullable = true)\n",
      " |-- v123: double (nullable = true)\n",
      " |-- v124: double (nullable = true)\n",
      " |-- v126: double (nullable = true)\n",
      " |-- v127: double (nullable = true)\n",
      " |-- v128: double (nullable = true)\n",
      " |-- v129: double (nullable = true)\n",
      " |-- v130: double (nullable = true)\n",
      " |-- v131: double (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = dat.drop('ID')\n",
    "test = test.drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=12110, v1=-1.22841523304125, v2=-0.357811779916537, v4=0.817177479661627, v5=-0.811954765800889, v6=0.720647219646289, v7=-0.437880521986937, v8=-0.361606808454984, v9=0.636475765116843, v10=0.537825481827579, v11=1.79311200685637, v12=0.223156706032837, v13=-0.773794685356874, v14=-1.75694427989216, v15=-0.34562127873204, v16=-1.67263704309952, v17=-0.0195593544443429, v18=-0.876003309234192, v19=-0.107737693757444, v20=0.513535945819183, v21=-2.72304289942787, v23=-0.172529846682884, v25=-0.47995994607189, v26=-0.880319869674916, v27=-0.0313546142109312, v28=-1.15732553902148, v29=0.861397333454323, v32=-0.513475215282834, v33=-0.642982696732912, v34=2.03217103437292, v35=1.50211996142938, v36=-1.14937284152847, v37=-0.977432379413093, v38=-0.158065130020606, v39=-0.0253085080043668, v40=-1.81535143589921, v41=1.24511216910305, v42=-0.0433709071269032, v43=-1.17362253424068, v44=-0.511586986818077, v45=0.340964432465869, v46=-0.425951316058441, v48=-0.220252808885165, v49=1.24994058410757, v50=0.820099705098666, v51=0.00604842858104646, v53=1.26680671689274, v54=-0.463016935593837, v55=-0.6713046909471, v57=-0.216060154340775, v58=-0.948395922084216, v59=0.941816524447457, v60=-0.655223833087605, v61=0.725418353907522, v62=7.14015074310081, v63=-0.377369758666245, v64=0.0994949852182556, v65=0.48040362216179, v67=0.695892880645136, v68=-0.418807371284671, v69=-0.303018755658158, v70=0.675275698072672, v72=4.95035204406019, v73=-0.833281338675938, v76=-0.0821377340600681, v77=0.392079256709914, v78=2.28995862936641, v80=-1.04903551477558, v81=0.187274327273923, v82=1.14125236414573, v83=-0.0751659241461625, v84=-1.39138313950557, v85=0.876379395143241, v86=0.0967674102165002, v87=0.0885908067816423, v88=1.20991298317196, v89=-0.481187508454242, v90=0.219384854949453, v92=0.732979761827474, v93=0.168416933433321, v94=-1.00426358526587, v95=0.566849267990194, v96=1.14449356976873, v97=-0.371475404734787, v98=-0.6408109612705, v99=0.947465856918251, v100=0.950171530579658, v101=0.832865435650809, v102=-0.398375539463608, v103=-1.00903450938936, v104=-1.09083119666276, v105=-0.379976775302327, v106=0.372533050590808, v108=0.0394269381133163, v109=-0.0328606117866934, v111=-0.529223239151755, v114=1.94988716501906, v115=0.293715019999874, v116=-1.13222284143143, v117=-0.854877769383112, v118=-0.622796435127063, v119=-0.478889897311886, v120=1.4275816036327, v121=-0.316672514642609, v122=1.32782254550443, v123=-0.217048448252573, v124=-0.374916175277802, v126=-0.684850339166704, v127=-0.197059990207056, v128=-0.097471333978023, v129=-0.452227449803975, v130=-0.367742836203885, v131=-1.07620152013709, target=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the number of rows to display by using the take()\n",
    "dat.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to count unique classes in a column\n",
    "dat.select(col(\"target\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target=1, count=87021), Row(target=0, count=27300)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do an aggregate count to see the distribution of the classes\n",
    "dat.groupBy(col(\"target\")).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=87021), Row(count(1)=27300)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can do the same thing in sql code\n",
    "# we must first create a \"view\" of the data in order to execute SQL functions\n",
    "dat.createOrReplaceTempView(\"dat\")\n",
    "spark.sql(\"SELECT COUNT(*) FROM dat GROUP BY Target\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v3', 'v22', 'v24', 'v30', 'v31', 'v47', 'v52', 'v56', 'v66', 'v71', 'v74', 'v75', 'v79', 'v91', 'v107', 'v110', 'v112', 'v113', 'v125']\n",
      "['target', 'v38', 'v62', 'v72', 'v129']\n"
     ]
    }
   ],
   "source": [
    "#Convert string types(categorical) to integerTypes\n",
    "types = [str(f.dataType) for f in dat.schema.fields]\n",
    "ind_str = [i for i in range(len(types)) if types[i] is 'StringType']\n",
    "ind_int = [i for i in range(len(types)) if types[i] is 'IntegerType']\n",
    "stringcol= [dat.columns[i] for i in ind_str]\n",
    "intcol = [dat.columns[i] for i in ind_int]\n",
    "print(stringcol)\n",
    "print(intcol)\n",
    "del ind_str, ind_int, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all string columns\n",
    "for colname in stringcol:\n",
    "    dat = dat.drop(colname)\n",
    "    test = test.drop(colname)\n",
    "    \n",
    "# remove 'target' from inttype list\n",
    "intcol = intcol[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v38', 'v62', 'v72', 'v129']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imputer requires that all columns it uses be floattype or doubletype, so let's convert ints to floats\n",
    "for colname in intcol:\n",
    "    dat = dat.withColumn( colname+\"cast\", col(colname).cast(\"float\"))\n",
    "    dat = dat.drop(colname)\n",
    "    test = test.withColumn( colname+\"cast\", col(colname).cast(\"float\"))\n",
    "    test = test.drop(colname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Impute missing values for both dat and test\n",
    "#Let's actually impute now!\n",
    "newcolnames = [colname+\"new\" for colname in dat.columns[1:]]\n",
    "imp = Imputer(strategy = 'mean',missingValue=None, inputCols = dat.columns[1:],outputCols=newcolnames )\n",
    "model = imp.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = model.transform(dat)\n",
    "test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v38cast',\n",
       " 'v62cast',\n",
       " 'v72cast',\n",
       " 'v129cast',\n",
       " 'v1new',\n",
       " 'v2new',\n",
       " 'v4new',\n",
       " 'v5new',\n",
       " 'v6new',\n",
       " 'v7new',\n",
       " 'v8new',\n",
       " 'v9new',\n",
       " 'v10new',\n",
       " 'v11new',\n",
       " 'v12new',\n",
       " 'v13new',\n",
       " 'v14new',\n",
       " 'v15new',\n",
       " 'v16new',\n",
       " 'v17new',\n",
       " 'v18new',\n",
       " 'v19new',\n",
       " 'v20new',\n",
       " 'v21new',\n",
       " 'v23new',\n",
       " 'v25new',\n",
       " 'v26new',\n",
       " 'v27new',\n",
       " 'v28new',\n",
       " 'v29new',\n",
       " 'v32new',\n",
       " 'v33new',\n",
       " 'v34new',\n",
       " 'v35new',\n",
       " 'v36new',\n",
       " 'v37new',\n",
       " 'v39new',\n",
       " 'v40new',\n",
       " 'v41new',\n",
       " 'v42new',\n",
       " 'v43new',\n",
       " 'v44new',\n",
       " 'v45new',\n",
       " 'v46new',\n",
       " 'v48new',\n",
       " 'v49new',\n",
       " 'v50new',\n",
       " 'v51new',\n",
       " 'v53new',\n",
       " 'v54new',\n",
       " 'v55new',\n",
       " 'v57new',\n",
       " 'v58new',\n",
       " 'v59new',\n",
       " 'v60new',\n",
       " 'v61new',\n",
       " 'v63new',\n",
       " 'v64new',\n",
       " 'v65new',\n",
       " 'v67new',\n",
       " 'v68new',\n",
       " 'v69new',\n",
       " 'v70new',\n",
       " 'v73new',\n",
       " 'v76new',\n",
       " 'v77new',\n",
       " 'v78new',\n",
       " 'v80new',\n",
       " 'v81new',\n",
       " 'v82new',\n",
       " 'v83new',\n",
       " 'v84new',\n",
       " 'v85new',\n",
       " 'v86new',\n",
       " 'v87new',\n",
       " 'v88new',\n",
       " 'v89new',\n",
       " 'v90new',\n",
       " 'v92new',\n",
       " 'v93new',\n",
       " 'v94new',\n",
       " 'v95new',\n",
       " 'v96new',\n",
       " 'v97new',\n",
       " 'v98new',\n",
       " 'v99new',\n",
       " 'v100new',\n",
       " 'v101new',\n",
       " 'v102new',\n",
       " 'v103new',\n",
       " 'v104new',\n",
       " 'v105new',\n",
       " 'v106new',\n",
       " 'v108new',\n",
       " 'v109new',\n",
       " 'v111new',\n",
       " 'v114new',\n",
       " 'v115new',\n",
       " 'v116new',\n",
       " 'v117new',\n",
       " 'v118new',\n",
       " 'v119new',\n",
       " 'v120new',\n",
       " 'v121new',\n",
       " 'v122new',\n",
       " 'v123new',\n",
       " 'v124new',\n",
       " 'v126new',\n",
       " 'v127new',\n",
       " 'v128new',\n",
       " 'v130new',\n",
       " 'v131new',\n",
       " 'v38castnew',\n",
       " 'v62castnew',\n",
       " 'v72castnew',\n",
       " 'v129castnew']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dat.columns[109:]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the columns into a single feature vector\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in features],\n",
    "    outputCol='features')\n",
    "\n",
    "dat_processed = assembler.transform(dat)\n",
    "test_processed = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = true)\n",
      " |-- v1: double (nullable = true)\n",
      " |-- v2: double (nullable = true)\n",
      " |-- v4: double (nullable = true)\n",
      " |-- v5: double (nullable = true)\n",
      " |-- v6: double (nullable = true)\n",
      " |-- v7: double (nullable = true)\n",
      " |-- v8: double (nullable = true)\n",
      " |-- v9: double (nullable = true)\n",
      " |-- v10: double (nullable = true)\n",
      " |-- v11: double (nullable = true)\n",
      " |-- v12: double (nullable = true)\n",
      " |-- v13: double (nullable = true)\n",
      " |-- v14: double (nullable = true)\n",
      " |-- v15: double (nullable = true)\n",
      " |-- v16: double (nullable = true)\n",
      " |-- v17: double (nullable = true)\n",
      " |-- v18: double (nullable = true)\n",
      " |-- v19: double (nullable = true)\n",
      " |-- v20: double (nullable = true)\n",
      " |-- v21: double (nullable = true)\n",
      " |-- v23: double (nullable = true)\n",
      " |-- v25: double (nullable = true)\n",
      " |-- v26: double (nullable = true)\n",
      " |-- v27: double (nullable = true)\n",
      " |-- v28: double (nullable = true)\n",
      " |-- v29: double (nullable = true)\n",
      " |-- v32: double (nullable = true)\n",
      " |-- v33: double (nullable = true)\n",
      " |-- v34: double (nullable = true)\n",
      " |-- v35: double (nullable = true)\n",
      " |-- v36: double (nullable = true)\n",
      " |-- v37: double (nullable = true)\n",
      " |-- v39: double (nullable = true)\n",
      " |-- v40: double (nullable = true)\n",
      " |-- v41: double (nullable = true)\n",
      " |-- v42: double (nullable = true)\n",
      " |-- v43: double (nullable = true)\n",
      " |-- v44: double (nullable = true)\n",
      " |-- v45: double (nullable = true)\n",
      " |-- v46: double (nullable = true)\n",
      " |-- v48: double (nullable = true)\n",
      " |-- v49: double (nullable = true)\n",
      " |-- v50: double (nullable = true)\n",
      " |-- v51: double (nullable = true)\n",
      " |-- v53: double (nullable = true)\n",
      " |-- v54: double (nullable = true)\n",
      " |-- v55: double (nullable = true)\n",
      " |-- v57: double (nullable = true)\n",
      " |-- v58: double (nullable = true)\n",
      " |-- v59: double (nullable = true)\n",
      " |-- v60: double (nullable = true)\n",
      " |-- v61: double (nullable = true)\n",
      " |-- v63: double (nullable = true)\n",
      " |-- v64: double (nullable = true)\n",
      " |-- v65: double (nullable = true)\n",
      " |-- v67: double (nullable = true)\n",
      " |-- v68: double (nullable = true)\n",
      " |-- v69: double (nullable = true)\n",
      " |-- v70: double (nullable = true)\n",
      " |-- v73: double (nullable = true)\n",
      " |-- v76: double (nullable = true)\n",
      " |-- v77: double (nullable = true)\n",
      " |-- v78: double (nullable = true)\n",
      " |-- v80: double (nullable = true)\n",
      " |-- v81: double (nullable = true)\n",
      " |-- v82: double (nullable = true)\n",
      " |-- v83: double (nullable = true)\n",
      " |-- v84: double (nullable = true)\n",
      " |-- v85: double (nullable = true)\n",
      " |-- v86: double (nullable = true)\n",
      " |-- v87: double (nullable = true)\n",
      " |-- v88: double (nullable = true)\n",
      " |-- v89: double (nullable = true)\n",
      " |-- v90: double (nullable = true)\n",
      " |-- v92: double (nullable = true)\n",
      " |-- v93: double (nullable = true)\n",
      " |-- v94: double (nullable = true)\n",
      " |-- v95: double (nullable = true)\n",
      " |-- v96: double (nullable = true)\n",
      " |-- v97: double (nullable = true)\n",
      " |-- v98: double (nullable = true)\n",
      " |-- v99: double (nullable = true)\n",
      " |-- v100: double (nullable = true)\n",
      " |-- v101: double (nullable = true)\n",
      " |-- v102: double (nullable = true)\n",
      " |-- v103: double (nullable = true)\n",
      " |-- v104: double (nullable = true)\n",
      " |-- v105: double (nullable = true)\n",
      " |-- v106: double (nullable = true)\n",
      " |-- v108: double (nullable = true)\n",
      " |-- v109: double (nullable = true)\n",
      " |-- v111: double (nullable = true)\n",
      " |-- v114: double (nullable = true)\n",
      " |-- v115: double (nullable = true)\n",
      " |-- v116: double (nullable = true)\n",
      " |-- v117: double (nullable = true)\n",
      " |-- v118: double (nullable = true)\n",
      " |-- v119: double (nullable = true)\n",
      " |-- v120: double (nullable = true)\n",
      " |-- v121: double (nullable = true)\n",
      " |-- v122: double (nullable = true)\n",
      " |-- v123: double (nullable = true)\n",
      " |-- v124: double (nullable = true)\n",
      " |-- v126: double (nullable = true)\n",
      " |-- v127: double (nullable = true)\n",
      " |-- v128: double (nullable = true)\n",
      " |-- v130: double (nullable = true)\n",
      " |-- v131: double (nullable = true)\n",
      " |-- v38cast: float (nullable = true)\n",
      " |-- v62cast: float (nullable = true)\n",
      " |-- v72cast: float (nullable = true)\n",
      " |-- v129cast: float (nullable = true)\n",
      " |-- v1new: double (nullable = true)\n",
      " |-- v2new: double (nullable = true)\n",
      " |-- v4new: double (nullable = true)\n",
      " |-- v5new: double (nullable = true)\n",
      " |-- v6new: double (nullable = true)\n",
      " |-- v7new: double (nullable = true)\n",
      " |-- v8new: double (nullable = true)\n",
      " |-- v9new: double (nullable = true)\n",
      " |-- v10new: double (nullable = true)\n",
      " |-- v11new: double (nullable = true)\n",
      " |-- v12new: double (nullable = true)\n",
      " |-- v13new: double (nullable = true)\n",
      " |-- v14new: double (nullable = true)\n",
      " |-- v15new: double (nullable = true)\n",
      " |-- v16new: double (nullable = true)\n",
      " |-- v17new: double (nullable = true)\n",
      " |-- v18new: double (nullable = true)\n",
      " |-- v19new: double (nullable = true)\n",
      " |-- v20new: double (nullable = true)\n",
      " |-- v21new: double (nullable = true)\n",
      " |-- v23new: double (nullable = true)\n",
      " |-- v25new: double (nullable = true)\n",
      " |-- v26new: double (nullable = true)\n",
      " |-- v27new: double (nullable = true)\n",
      " |-- v28new: double (nullable = true)\n",
      " |-- v29new: double (nullable = true)\n",
      " |-- v32new: double (nullable = true)\n",
      " |-- v33new: double (nullable = true)\n",
      " |-- v34new: double (nullable = true)\n",
      " |-- v35new: double (nullable = true)\n",
      " |-- v36new: double (nullable = true)\n",
      " |-- v37new: double (nullable = true)\n",
      " |-- v39new: double (nullable = true)\n",
      " |-- v40new: double (nullable = true)\n",
      " |-- v41new: double (nullable = true)\n",
      " |-- v42new: double (nullable = true)\n",
      " |-- v43new: double (nullable = true)\n",
      " |-- v44new: double (nullable = true)\n",
      " |-- v45new: double (nullable = true)\n",
      " |-- v46new: double (nullable = true)\n",
      " |-- v48new: double (nullable = true)\n",
      " |-- v49new: double (nullable = true)\n",
      " |-- v50new: double (nullable = true)\n",
      " |-- v51new: double (nullable = true)\n",
      " |-- v53new: double (nullable = true)\n",
      " |-- v54new: double (nullable = true)\n",
      " |-- v55new: double (nullable = true)\n",
      " |-- v57new: double (nullable = true)\n",
      " |-- v58new: double (nullable = true)\n",
      " |-- v59new: double (nullable = true)\n",
      " |-- v60new: double (nullable = true)\n",
      " |-- v61new: double (nullable = true)\n",
      " |-- v63new: double (nullable = true)\n",
      " |-- v64new: double (nullable = true)\n",
      " |-- v65new: double (nullable = true)\n",
      " |-- v67new: double (nullable = true)\n",
      " |-- v68new: double (nullable = true)\n",
      " |-- v69new: double (nullable = true)\n",
      " |-- v70new: double (nullable = true)\n",
      " |-- v73new: double (nullable = true)\n",
      " |-- v76new: double (nullable = true)\n",
      " |-- v77new: double (nullable = true)\n",
      " |-- v78new: double (nullable = true)\n",
      " |-- v80new: double (nullable = true)\n",
      " |-- v81new: double (nullable = true)\n",
      " |-- v82new: double (nullable = true)\n",
      " |-- v83new: double (nullable = true)\n",
      " |-- v84new: double (nullable = true)\n",
      " |-- v85new: double (nullable = true)\n",
      " |-- v86new: double (nullable = true)\n",
      " |-- v87new: double (nullable = true)\n",
      " |-- v88new: double (nullable = true)\n",
      " |-- v89new: double (nullable = true)\n",
      " |-- v90new: double (nullable = true)\n",
      " |-- v92new: double (nullable = true)\n",
      " |-- v93new: double (nullable = true)\n",
      " |-- v94new: double (nullable = true)\n",
      " |-- v95new: double (nullable = true)\n",
      " |-- v96new: double (nullable = true)\n",
      " |-- v97new: double (nullable = true)\n",
      " |-- v98new: double (nullable = true)\n",
      " |-- v99new: double (nullable = true)\n",
      " |-- v100new: double (nullable = true)\n",
      " |-- v101new: double (nullable = true)\n",
      " |-- v102new: double (nullable = true)\n",
      " |-- v103new: double (nullable = true)\n",
      " |-- v104new: double (nullable = true)\n",
      " |-- v105new: double (nullable = true)\n",
      " |-- v106new: double (nullable = true)\n",
      " |-- v108new: double (nullable = true)\n",
      " |-- v109new: double (nullable = true)\n",
      " |-- v111new: double (nullable = true)\n",
      " |-- v114new: double (nullable = true)\n",
      " |-- v115new: double (nullable = true)\n",
      " |-- v116new: double (nullable = true)\n",
      " |-- v117new: double (nullable = true)\n",
      " |-- v118new: double (nullable = true)\n",
      " |-- v119new: double (nullable = true)\n",
      " |-- v120new: double (nullable = true)\n",
      " |-- v121new: double (nullable = true)\n",
      " |-- v122new: double (nullable = true)\n",
      " |-- v123new: double (nullable = true)\n",
      " |-- v124new: double (nullable = true)\n",
      " |-- v126new: double (nullable = true)\n",
      " |-- v127new: double (nullable = true)\n",
      " |-- v128new: double (nullable = true)\n",
      " |-- v130new: double (nullable = true)\n",
      " |-- v131new: double (nullable = true)\n",
      " |-- v38castnew: float (nullable = true)\n",
      " |-- v62castnew: float (nullable = true)\n",
      " |-- v72castnew: float (nullable = true)\n",
      " |-- v129castnew: float (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat_processed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a logistic regression model, tree model, and a gradient boosted tree model\n",
    "# Create an evaluator object\n",
    "\n",
    "logit = LogisticRegression(featuresCol=\"features\",labelCol=\"Target\")\n",
    "logit_model = logit.fit(train)\n",
    "\n",
    "cart = DecisionTreeClassifier(maxDepth=10)\n",
    "cart_model = cart.fit(train)\n",
    "\n",
    "gbt = GBTClassifier(maxDepth=10, stepSize=.001,maxIter=10)\n",
    "gbt_model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_logit = logit_model.transform(test)\n",
    "pred_cart = cart_model.transform(test)\n",
    "pred_gbt = gbt_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [evaluator.evaluate(i) for i in [pred_logit, pred_cart, pred_gbt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.985497295510151, 0.8885200864455169, 0.8875588161461015]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981404487483115"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56897/(56897+106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_count = len(dat.columns)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in features],\n",
    "    outputCol='features')\n",
    "\n",
    "dat_processed = assembler.transform(dat)\n",
    "test_processed = assembler.transform(test)\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\",outputCol=\"feature\")\n",
    "\n",
    "dat = normalizer.transform(assembler.transform(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concept of pipeline\n",
    "# Additionally how to cross validate\n",
    "logit = LogisticRegression(featuresCol='feature',labelCol='target')\n",
    "cart = DecisionTreeClassifier(featuresCol='feature',labelCol='target')\n",
    "gbt = GBTClassifier(featuresCol='feature',labelCol='target')\n",
    "\n",
    "\n",
    "paramGrid_logit = ParamGridBuilder() \\\n",
    "    .addGrid(logit.regParam, [0,0.01, 0.1]) \\\n",
    "    .build()\n",
    "    \n",
    "paramGrid_cart = ParamGridBuilder() \\\n",
    "    .addGrid(cart.maxDepth, [10,12,15]) \\\n",
    "    .build()\n",
    "    \n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [10,12,15]) \\\n",
    "    .addGrid(gbt.stepSize,[0.01]) \\\n",
    "    .addGrid(gbt.maxIter,[20]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='target')\n",
    "    \n",
    "\n",
    "cv_logit = CrossValidator(estimator=logit,evaluator=evaluator,estimatorParamMaps=paramGrid_logit,numFolds=10)\n",
    "cv_cart = CrossValidator(estimator=cart,evaluator=evaluator,estimatorParamMaps=paramGrid_cart,numFolds=10)\n",
    "cv_gbt = CrossValidator(estimator=gbt,evaluator=evaluator,estimatorParamMaps=paramGrid_gbt,numFolds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_logit = cv_logit.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_cart = cv_cart.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_gbt = cv_gbt.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7087400464393288, 0.6992343001874801, 0.7080240843897794]\n",
      "[0.5036192095312046, 0.5331364209963431, 0.5430835878468885, 0.5739519392919172]\n",
      "[0.7251348350063479, 0.7448645684366266, 0.7684780371882793, 0.8201436144971819]\n"
     ]
    }
   ],
   "source": [
    "print(cvmodel_logit.avgMetrics)\n",
    "print(cvmodel_cart.avgMetrics)\n",
    "print(cvmodel_gbt.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(cvmodel_gbt.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='feature',labelCol=\"target\")\n",
    "pipeline_rf = Pipeline(stages=[assembler,normalizer, rf])\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [15]) \\\n",
    "    .addGrid(rf.numTrees, [1000]) \\\n",
    "    .build()\n",
    "    \n",
    "cv_rf = CrossValidator(estimator=pipeline_rf, evaluator=evaluator, numFolds=10, estimatorParamMaps=paramGrid_rf)\n",
    "cvmodel_rf = cv_rf.fit(dat)\n",
    "cvmodel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat.rdd.sampleByKey(withReplacement=False,fractions={0:0.0001, 1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 178673.0 failed 4 times, most recent failure: Lost task 0.3 in stage 178673.0 (TID 556659, mylaptop, executor 3): java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d398b7eb3595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 178673.0 failed 4 times, most recent failure: Lost task 0.3 in stage 178673.0 (TID 556659, mylaptop, executor 3): java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"/home/hduser1/anaconda3/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "dat.rdd.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
